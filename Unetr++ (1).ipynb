{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/training\" \"/content/training\"\n",
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/test\" \"/content/test\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfaFhO_Pg8XB",
        "outputId": "77836fed-b100-4fdf-9f18-cb182a3b12e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##install"
      ],
      "metadata": {
        "id": "m6Suxvc2mZfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade batchgenerators\n",
        "!pip install monai\n",
        "!pip install timm\n",
        "!pip install thop\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rm_LS4u4mYJY",
        "outputId": "89713149-0953-4519-a413-47604ab14b14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting batchgenerators\n",
            "  Downloading batchgenerators-0.25.tar.gz (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from batchgenerators) (9.4.0)\n",
            "Requirement already satisfied: numpy>=1.10.2 in /usr/local/lib/python3.10/dist-packages (from batchgenerators) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from batchgenerators) (1.13.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from batchgenerators) (0.23.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from batchgenerators) (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from batchgenerators) (1.0.0)\n",
            "Collecting unittest2 (from batchgenerators)\n",
            "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from batchgenerators) (3.5.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->batchgenerators) (3.3)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->batchgenerators) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->batchgenerators) (2024.7.21)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->batchgenerators) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->batchgenerators) (0.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->batchgenerators) (1.4.2)\n",
            "Collecting argparse (from unittest2->batchgenerators)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: six>=1.4 in /usr/local/lib/python3.10/dist-packages (from unittest2->batchgenerators) (1.16.0)\n",
            "Collecting traceback2 (from unittest2->batchgenerators)\n",
            "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting linecache2 (from traceback2->unittest2->batchgenerators)\n",
            "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
            "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: batchgenerators\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.25-py3-none-any.whl size=89008 sha256=5fddd5f636f385aef098bc5558c6b984b1def6c30e9d26e4287891d58d20db19\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/b0/1b/40912fb58eb167b86cbc444ddb2e6ba382b248215295f932e2\n",
            "Successfully built batchgenerators\n",
            "Installing collected packages: linecache2, argparse, traceback2, unittest2, batchgenerators\n",
            "Successfully installed argparse-1.4.0 batchgenerators-0.25 linecache2-1.0.0 traceback2-1.4.0 unittest2-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "530db510ac3242fd930498308416c277"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.9->monai)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->monai)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
            "Downloading monai-1.3.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n",
            "Successfully installed monai-1.3.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting timm\n",
            "  Downloading timm-1.0.7-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.7\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->thop) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n",
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##load train data\n",
        "\n"
      ],
      "metadata": {
        "id": "PCa6yJdMKm4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageTk\n",
        "import torch\n",
        "import PIL\n",
        "from torchvision import transforms\n",
        "#import config.config as cfg\n",
        "\n",
        "def get_files(file_dir, file_list, type_str):\n",
        "\n",
        "    for file_ in os.listdir(file_dir):\n",
        "        path = os.path.join(file_dir, file_)\n",
        "        if os.path.isdir(path):\n",
        "            get_files(file_dir, file_list, type_str)\n",
        "        else:\n",
        "            if file_.rfind(type_str) !=-1:\n",
        "                file_list.append(path)\n",
        "\n",
        "def AddGaussNoise(src,sigma):\n",
        "    mean = 0\n",
        "    # 获取图片的高度和宽度\n",
        "    height, width, channels = src.shape[0:3]\n",
        "    gauss = np.random.normal(mean,sigma,(height,width,channels))\n",
        "\n",
        "\n",
        "    noisy_img = src + gauss\n",
        "    noisy_img = np.clip(noisy_img, 0, 1)\n",
        "\n",
        "\n",
        "    # cv2.namedWindow(\"data2D\", cv2.WINDOW_NORMAL)\n",
        "    # cv2.imshow(\"data2D\", noisy_img)\n",
        "    #\n",
        "    # cv2.waitKey(0)\n",
        "    return noisy_img\n",
        "\n",
        "class TrainData():\n",
        "    def __init__(self, file_root, label_path, train_flag = False):\n",
        "        self.data_dir = file_root\n",
        "        self.label_path = label_path\n",
        "        self.train_flag = train_flag\n",
        "\n",
        "        self.train_list = None\n",
        "        self.prepare(self.data_dir)\n",
        "\n",
        "\n",
        "        self.transformsRotate  = transforms.RandomRotation(degrees=[-45, 45], interpolation=PIL.Image.BILINEAR)\n",
        "        self.transformGray = transforms.RandomGrayscale(p=0.5)\n",
        "\n",
        "        self.transformVFlip = transforms.RandomVerticalFlip(p=0.5)\n",
        "        self.transformHFlip = transforms.RandomHorizontalFlip(p=0.5)\n",
        "        # self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "        # cl1 = clahe.apply(img)\n",
        "\n",
        "    def prepare(self, file_path):\n",
        "\n",
        "        file_list = []\n",
        "        get_files(file_path, file_list, \"tif\") #drive\n",
        "        # get_files(file_path, file_list, \"jpg\") #CHASEDB1\n",
        "        # get_files(file_path, file_list, \"ppm\") #STAREdatabase\n",
        "        # get_files(file_path, file_list, \"im.npy\")#HRFdatas\n",
        "\n",
        "        self.train_list = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        file_path = self.train_list[item]\n",
        "\n",
        "        label_path = self.label_path + os.path.split(file_path)[-1].replace(\"_training.tif\", \"_manual1.gif\")\n",
        "        # label_path = self.label_path + os.path.split(file_path)[-1].replace(\".jpg\", \"_1stHO.png\")\n",
        "        #label_path = self.label_path + os.path.split(file_path)[-1].replace(\".ppm\", \".ah.ppm\")\n",
        "        # label_path = self.label_path + os.path.split(file_path)[-1].replace(\"im.npy\", \"label.npy\")\n",
        "\n",
        "\n",
        "        img = cv2.imread(file_path)\n",
        "        label = Image.open(label_path)#cv2.imread(label_path, 0)#\n",
        "        label = np.array(label)\n",
        "        # img = np.load(file_path)\n",
        "        # label = np.load(label_path)\n",
        "\n",
        "        data2D = img\n",
        "        label2D = label\n",
        "\n",
        "        data2D = (data2D - np.min(data2D))/(np.max(data2D) - np.min(data2D))\n",
        "        label2D = label2D / np.max(label2D)\n",
        "\n",
        "\n",
        "        data2D = torch.tensor(data2D).permute(2, 0, 1)\n",
        "        label2D = torch.tensor(label2D)\n",
        "\n",
        "        label2D = torch.unsqueeze(label2D, dim=0)\n",
        "        mdata = torch.cat([label2D, data2D], dim=0)\n",
        "\n",
        "\n",
        "        mdata = self.transformVFlip(mdata)\n",
        "        mdata = self.transformHFlip(mdata)\n",
        "        #random roatte\n",
        "        flag = np.random.randint(0, 2, 1)[0]\n",
        "        # if flag==1:\n",
        "        mdata = self.transformsRotate(mdata)\n",
        "\n",
        "        label2D = mdata[0,:,:]#.numpy()\n",
        "        data2D = mdata[1:,:,:]#.permute(1, 2, 0).numpy()\n",
        "\n",
        "\n",
        "\n",
        "        scale = np.random.randint(8, 14, 1)[0] / 10\n",
        "        #\n",
        "        data2D = torch.pow(data2D, scale)\n",
        "\n",
        "        weight_ = cv2.GaussianBlur(label2D.numpy(), (5, 5), 1)\n",
        "\n",
        "        #to Gray\n",
        "        # data2D = self.transformGray(data2D)\n",
        "        # data2D_ = data2D.permute(1, 2, 0).numpy()\n",
        "        # label2D_ = label2D.numpy()\n",
        "        # cv2.namedWindow(\"data2D\", cv2.WINDOW_NORMAL)\n",
        "        # cv2.imshow(\"data2D\", data2D_)\n",
        "        # cv2.namedWindow(\"label2D\", cv2.WINDOW_NORMAL)\n",
        "        # cv2.imshow(\"label2D\", weight_)\n",
        "        #\n",
        "        # cv2.waitKey(0)\n",
        "\n",
        "\n",
        "\n",
        "        return data2D, label2D, weight_\n",
        "\n",
        "\n",
        "def data_crop(train_data, train_label, weight_):\n",
        "\n",
        "    bn, c, height, width = train_data.shape\n",
        "    CROP_SIZE = 192\n",
        "    rangh = height - CROP_SIZE-1\n",
        "    rangw = width - CROP_SIZE-1\n",
        "\n",
        "\n",
        "    nums = 4\n",
        "    train_data_ = torch.zeros((nums*bn, 3,CROP_SIZE, CROP_SIZE))\n",
        "    train_label_ = torch.zeros((nums*bn,CROP_SIZE, CROP_SIZE))\n",
        "    train_weight_ = torch.zeros((nums*bn,CROP_SIZE, CROP_SIZE))\n",
        "\n",
        "\n",
        "    for b in range(bn):\n",
        "        for i in range(nums):\n",
        "            offh = np.random.randint(0, rangh, 1)[0]\n",
        "            offw = np.random.randint(0, rangw, 1)[0]\n",
        "            # print(height, \" \", width, \" \",  offh+cfg.CROP_SIZE, \"   \", offw+cfg.CROP_SIZE)\n",
        "            train_data_[nums*b+i,:, :, :] = train_data[b, :, offh:offh+CROP_SIZE, offw:offw+CROP_SIZE]\n",
        "            train_label_[nums*b+i, :, :] = train_label[b, offh:offh+CROP_SIZE, offw:offw+CROP_SIZE]\n",
        "            train_weight_[nums*b+i, :, :] = weight_[b, offh:offh+CROP_SIZE, offw:offw+CROP_SIZE]\n",
        "            # data2D = train_data[0, :, offh:offh+cfg.CROP_SIZE, offw:offw+cfg.CROP_SIZE].permute(1, 2, 0).numpy()\n",
        "            # label2D = train_label[0, offh:offh+cfg.CROP_SIZE, offw:offw+cfg.CROP_SIZE].numpy()\n",
        "            # cv2.namedWindow(\"data2D\", cv2.WINDOW_NORMAL)\n",
        "            # cv2.imshow(\"data2D\", data2D)\n",
        "            # cv2.namedWindow(\"label2D\", cv2.WINDOW_NORMAL)\n",
        "            # cv2.imshow(\"label2D\", label2D)\n",
        "            #\n",
        "        # cv2.waitKey(0)\n",
        "\n",
        "    return train_data_, train_label_, train_weight_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-6tITvPQEEEe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##load test data"
      ],
      "metadata": {
        "id": "D603EameKy3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageTk\n",
        "import torch\n",
        "import PIL\n",
        "from torchvision import transforms\n",
        "#import config.config as cfg\n",
        "\n",
        "def get_files(file_dir, file_list, type_str):\n",
        "\n",
        "    for file_ in os.listdir(file_dir):\n",
        "        path = os.path.join(file_dir, file_)\n",
        "        if os.path.isdir(path):\n",
        "            get_files(file_dir, file_list, type_str)\n",
        "        else:\n",
        "            if file_.rfind(type_str) !=-1:\n",
        "                file_list.append(path)\n",
        "\n",
        "\n",
        "class TestData():\n",
        "    def __init__(self, file_root, label_path, train_flag = False):\n",
        "        self.data_dir = file_root\n",
        "        self.label_path = label_path\n",
        "        self.train_flag = train_flag\n",
        "\n",
        "        self.train_list = None\n",
        "        self.prepare(self.data_dir)\n",
        "        # self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    def prepare(self, file_path):\n",
        "\n",
        "        file_list = []\n",
        "        get_files(file_path, file_list, \"tif\") #drive\n",
        "        # get_files(file_path, file_list, \"jpg\") #CHASEDB1\n",
        "        # get_files(file_path, file_list, \"ppm\") #STAREdatabase\n",
        "        # get_files(file_path, file_list, \"im.npy\")#HRFdatas\n",
        "\n",
        "        self.train_list = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        file_path = self.train_list[item]\n",
        "\n",
        "        label_path = self.label_path + os.path.split(file_path)[-1].replace(\"_test.tif\", \"_manual1.gif\")\n",
        "        # label_path = self.label_path + os.path.split(file_path)[-1].replace(\".jpg\", \"_1stHO.png\")\n",
        "        #label_path = self.label_path + os.path.split(file_path)[-1].replace(\".ppm\", \".ah.ppm\")\n",
        "        # label_path = self.label_path + os.path.split(file_path)[-1].replace(\"im.npy\", \"label.npy\")\n",
        "\n",
        "\n",
        "        img = cv2.imread(file_path)\n",
        "        label = Image.open(label_path)#cv2.imread(label_path, 0)#\n",
        "        label = np.array(label)\n",
        "        # img = np.load(file_path)\n",
        "        # label = np.load(label_path)\n",
        "\n",
        "\n",
        "        data2D = img\n",
        "        label2D = label\n",
        "\n",
        "\n",
        "        data2D = (data2D - np.min(data2D)) / (np.max(data2D) - np.min(data2D))\n",
        "        label2D = label2D / np.max(label2D)\n",
        "\n",
        "\n",
        "        data2D = torch.tensor(data2D).permute(2, 0, 1)\n",
        "        label2D = torch.tensor(label2D)\n",
        "\n",
        "\n",
        "        # cv2.namedWindow(\"data2D\", cv2.WINDOW_NORMAL)\n",
        "        # cv2.imshow(\"data2D\", data2D[0, :, :].numpy())\n",
        "        # cv2.namedWindow(\"label2D\", cv2.WINDOW_NORMAL)\n",
        "        # cv2.imshow(\"label2D\", label2D)\n",
        "        #\n",
        "        # cv2.waitKey(0)\n",
        "\n",
        "\n",
        "\n",
        "        return data2D, label2D\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IDuIkBo9EFpQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##network_architecture"
      ],
      "metadata": {
        "id": "Rina1rN8LZ8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####neural network"
      ],
      "metadata": {
        "id": "V_f_Xn9IMRoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "\n",
        "class no_op(object):\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        pass"
      ],
      "metadata": {
        "id": "CPUYACifIMPh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def maybe_to_torch(d):\n",
        "    if isinstance(d, list):\n",
        "        d = [maybe_to_torch(i) if not isinstance(i, torch.Tensor) else i for i in d]\n",
        "    elif not isinstance(d, torch.Tensor):\n",
        "        d = torch.from_numpy(d).float()\n",
        "    return d\n",
        "\n",
        "\n",
        "def to_cuda(data, non_blocking=True, gpu_id=0):\n",
        "    if isinstance(data, list):\n",
        "        data = [i.cuda(gpu_id, non_blocking=non_blocking) for i in data]\n",
        "    else:\n",
        "        data = data.cuda(gpu_id, non_blocking=non_blocking)\n",
        "    return data"
      ],
      "metadata": {
        "id": "J5XdQrAaIdnW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#    Copyright 2020 Division of Medical Image Computing, German Cancer Research Center (DKFZ), Heidelberg, Germany\n",
        "#\n",
        "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#    you may not use this file except in compliance with the License.\n",
        "#    You may obtain a copy of the License at\n",
        "#\n",
        "#        http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#    Unless required by applicable law or agreed to in writing, software\n",
        "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#    See the License for the specific language governing permissions and\n",
        "#    limitations under the License.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from batchgenerators.augmentations.utils import pad_nd_image\n",
        "#from utilities.random_stuff import no_op\n",
        "#from utilities.to_torch import to_cuda, maybe_to_torch\n",
        "from torch import nn\n",
        "import torch\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from typing import Union, Tuple, List\n",
        "\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "    def get_device(self):\n",
        "        if next(self.parameters()).device == \"cpu\":\n",
        "            return \"cpu\"\n",
        "        else:\n",
        "            return next(self.parameters()).device.index\n",
        "\n",
        "    def set_device(self, device):\n",
        "        if device == \"cpu\":\n",
        "            self.cpu()\n",
        "        else:\n",
        "            self.cuda(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class SegmentationNetwork(NeuralNetwork):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        # if we have 5 pooling then our patch size must be divisible by 2**5\n",
        "        self.input_shape_must_be_divisible_by = None  # for example in a 2d network that does 5 pool in x and 6 pool\n",
        "        # in y this would be (32, 64)\n",
        "\n",
        "        # we need to know this because we need to know if we are a 2d or a 3d netowrk\n",
        "        self.conv_op = None  # nn.Conv2d or nn.Conv3d\n",
        "\n",
        "        # this tells us how many channely we have in the output. Important for preallocation in inference\n",
        "        self.num_classes = None  # number of channels in the output\n",
        "\n",
        "        # depending on the loss, we do not hard code a nonlinearity into the architecture. To aggregate predictions\n",
        "        # during inference, we need to apply the nonlinearity, however. So it is important to let the newtork know what\n",
        "        # to apply in inference. For the most part this will be softmax\n",
        "        self.inference_apply_nonlin = lambda x: x  # softmax_helper\n",
        "\n",
        "        # This is for saving a gaussian importance map for inference. It weights voxels higher that are closer to the\n",
        "        # center. Prediction at the borders are often less accurate and are thus downweighted. Creating these Gaussians\n",
        "        # can be expensive, so it makes sense to save and reuse them.\n",
        "        self._gaussian_3d = self._patch_size_for_gaussian_3d = None\n",
        "        self._gaussian_2d = self._patch_size_for_gaussian_2d = None\n",
        "\n",
        "    def predict_3D(self, x: np.ndarray, do_mirroring: bool, mirror_axes: Tuple[int, ...] = (0, 1, 2),\n",
        "                   use_sliding_window: bool = False,\n",
        "                   step_size: float = 0.5, patch_size: Tuple[int, ...] = None, regions_class_order: Tuple[int, ...] = None,\n",
        "                   use_gaussian: bool = False, pad_border_mode: str = \"constant\",\n",
        "                   pad_kwargs: dict = None, all_in_gpu: bool = False,\n",
        "                   verbose: bool = True, mixed_precision: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Use this function to predict a 3D image. It does not matter whether the network is a 2D or 3D U-Net, it will\n",
        "        detect that automatically and run the appropriate code.\n",
        "\n",
        "        When running predictions, you need to specify whether you want to run fully convolutional of sliding window\n",
        "        based inference. We very strongly recommend you use sliding window with the default settings.\n",
        "\n",
        "        It is the responsibility of the user to make sure the network is in the proper mode (eval for inference!). If\n",
        "        the network is not in eval mode it will print a warning.\n",
        "\n",
        "        :param x: Your input data. Must be a nd.ndarray of shape (c, x, y, z).\n",
        "        :param do_mirroring: If True, use test time data augmentation in the form of mirroring\n",
        "        :param mirror_axes: Determines which axes to use for mirroing. Per default, mirroring is done along all three\n",
        "        axes\n",
        "        :param use_sliding_window: if True, run sliding window prediction. Heavily recommended! This is also the default\n",
        "        :param step_size: When running sliding window prediction, the step size determines the distance between adjacent\n",
        "        predictions. The smaller the step size, the denser the predictions (and the longer it takes!). Step size is given\n",
        "        as a fraction of the patch_size. 0.5 is the default and means that wen advance by patch_size * 0.5 between\n",
        "        predictions. step_size cannot be larger than 1!\n",
        "        :param patch_size: The patch size that was used for training the network. Do not use different patch sizes here,\n",
        "        this will either crash or give potentially less accurate segmentations\n",
        "        :param regions_class_order: Fabian only\n",
        "        :param use_gaussian: (Only applies to sliding window prediction) If True, uses a Gaussian importance weighting\n",
        "         to weigh predictions closer to the center of the current patch higher than those at the borders. The reason\n",
        "         behind this is that the segmentation accuracy decreases towards the borders. Default (and recommended): True\n",
        "        :param pad_border_mode: leave this alone\n",
        "        :param pad_kwargs: leave this alone\n",
        "        :param all_in_gpu: experimental. You probably want to leave this as is it\n",
        "        :param verbose: Do you want a wall of text? If yes then set this to True\n",
        "        :param mixed_precision: if True, will run inference in mixed precision with autocast()\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        assert step_size <= 1, 'step_size must be smaller than 1. Otherwise there will be a gap between consecutive ' \\\n",
        "                               'predictions'\n",
        "\n",
        "        if verbose: print(\"debug: mirroring\", do_mirroring, \"mirror_axes\", mirror_axes)\n",
        "\n",
        "        assert self.get_device() != \"cpu\", \"CPU not implemented\"\n",
        "\n",
        "        if pad_kwargs is None:\n",
        "            pad_kwargs = {'constant_values': 0}\n",
        "\n",
        "        # A very long time ago the mirror axes were (2, 3, 4) for a 3d network. This is just to intercept any old\n",
        "        # code that uses this convention\n",
        "        self.conv_op = nn.Conv3d\n",
        "        if len(mirror_axes):\n",
        "            if self.conv_op == nn.Conv2d:\n",
        "                if max(mirror_axes) > 1:\n",
        "                    raise ValueError(\"mirror axes. duh\")\n",
        "            if self.conv_op == nn.Conv3d:\n",
        "                if max(mirror_axes) > 2:\n",
        "                    raise ValueError(\"mirror axes. duh\")\n",
        "\n",
        "        if self.training:\n",
        "            print('WARNING! Network is in train mode during inference. This may be intended, or not...')\n",
        "\n",
        "        assert len(x.shape) == 4, \"data must have shape (c,x,y,z)\"\n",
        "\n",
        "        if mixed_precision:\n",
        "            context = autocast\n",
        "        else:\n",
        "            context = no_op\n",
        "\n",
        "        with context():\n",
        "            with torch.no_grad():\n",
        "                if self.conv_op == nn.Conv3d:\n",
        "                    if use_sliding_window:\n",
        "                        res = self._internal_predict_3D_3Dconv_tiled(x, step_size, do_mirroring, mirror_axes, patch_size,\n",
        "                                                                     regions_class_order, use_gaussian, pad_border_mode,\n",
        "                                                                     pad_kwargs=pad_kwargs, all_in_gpu=all_in_gpu,\n",
        "                                                                     verbose=verbose)\n",
        "                    else:\n",
        "                        res = self._internal_predict_3D_3Dconv(x, patch_size, do_mirroring, mirror_axes, regions_class_order,\n",
        "                                                               pad_border_mode, pad_kwargs=pad_kwargs, verbose=verbose)\n",
        "                elif self.conv_op == nn.Conv2d:\n",
        "                    if use_sliding_window:\n",
        "                        res = self._internal_predict_3D_2Dconv_tiled(x, patch_size, do_mirroring, mirror_axes, step_size,\n",
        "                                                                     regions_class_order, use_gaussian, pad_border_mode,\n",
        "                                                                     pad_kwargs, all_in_gpu, False)\n",
        "                    else:\n",
        "                        res = self._internal_predict_3D_2Dconv(x, patch_size, do_mirroring, mirror_axes, regions_class_order,\n",
        "                                                               pad_border_mode, pad_kwargs, all_in_gpu, False)\n",
        "                else:\n",
        "                    raise RuntimeError(\"Invalid conv op, cannot determine what dimensionality (2d/3d) the network is\")\n",
        "\n",
        "        return res\n",
        "\n",
        "    def predict_2D(self, x, do_mirroring: bool, mirror_axes: tuple = (0, 1, 2), use_sliding_window: bool = False,\n",
        "                   step_size: float = 0.5, patch_size: tuple = None, regions_class_order: tuple = None,\n",
        "                   use_gaussian: bool = False, pad_border_mode: str = \"constant\",\n",
        "                   pad_kwargs: dict = None, all_in_gpu: bool = False,\n",
        "                   verbose: bool = True, mixed_precision: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Use this function to predict a 2D image. If this is a 3D U-Net it will crash because you cannot predict a 2D\n",
        "        image with that (you dummy).\n",
        "\n",
        "        When running predictions, you need to specify whether you want to run fully convolutional of sliding window\n",
        "        based inference. We very strongly recommend you use sliding window with the default settings.\n",
        "\n",
        "        It is the responsibility of the user to make sure the network is in the proper mode (eval for inference!). If\n",
        "        the network is not in eval mode it will print a warning.\n",
        "\n",
        "        :param x: Your input data. Must be a nd.ndarray of shape (c, x, y).\n",
        "        :param do_mirroring: If True, use test time data augmentation in the form of mirroring\n",
        "        :param mirror_axes: Determines which axes to use for mirroing. Per default, mirroring is done along all three\n",
        "        axes\n",
        "        :param use_sliding_window: if True, run sliding window prediction. Heavily recommended! This is also the default\n",
        "        :param step_size: When running sliding window prediction, the step size determines the distance between adjacent\n",
        "        predictions. The smaller the step size, the denser the predictions (and the longer it takes!). Step size is given\n",
        "        as a fraction of the patch_size. 0.5 is the default and means that wen advance by patch_size * 0.5 between\n",
        "        predictions. step_size cannot be larger than 1!\n",
        "        :param patch_size: The patch size that was used for training the network. Do not use different patch sizes here,\n",
        "        this will either crash or give potentially less accurate segmentations\n",
        "        :param regions_class_order: Fabian only\n",
        "        :param use_gaussian: (Only applies to sliding window prediction) If True, uses a Gaussian importance weighting\n",
        "         to weigh predictions closer to the center of the current patch higher than those at the borders. The reason\n",
        "         behind this is that the segmentation accuracy decreases towards the borders. Default (and recommended): True\n",
        "        :param pad_border_mode: leave this alone\n",
        "        :param pad_kwargs: leave this alone\n",
        "        :param all_in_gpu: experimental. You probably want to leave this as is it\n",
        "        :param verbose: Do you want a wall of text? If yes then set this to True\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        assert step_size <= 1, 'step_size must be smaler than 1. Otherwise there will be a gap between consecutive ' \\\n",
        "                               'predictions'\n",
        "\n",
        "        if self.conv_op == nn.Conv3d:\n",
        "            raise RuntimeError(\"Cannot predict 2d if the network is 3d. Dummy.\")\n",
        "\n",
        "        if verbose: print(\"debug: mirroring\", do_mirroring, \"mirror_axes\", mirror_axes)\n",
        "\n",
        "        assert self.get_device() != \"cpu\", \"CPU not implemented\"\n",
        "\n",
        "        if pad_kwargs is None:\n",
        "            pad_kwargs = {'constant_values': 0}\n",
        "\n",
        "        # A very long time ago the mirror axes were (2, 3) for a 2d network. This is just to intercept any old\n",
        "        # code that uses this convention\n",
        "        if len(mirror_axes):\n",
        "            if max(mirror_axes) > 1:\n",
        "                raise ValueError(\"mirror axes. duh\")\n",
        "\n",
        "        if self.training:\n",
        "            print('WARNING! Network is in train mode during inference. This may be intended, or not...')\n",
        "\n",
        "        assert len(x.shape) == 3, \"data must have shape (c,x,y)\"\n",
        "\n",
        "        if mixed_precision:\n",
        "            context = autocast\n",
        "        else:\n",
        "            context = no_op\n",
        "\n",
        "        with context():\n",
        "            with torch.no_grad():\n",
        "                if self.conv_op == nn.Conv2d:\n",
        "                    if use_sliding_window:\n",
        "                        res = self._internal_predict_2D_2Dconv_tiled(x, step_size, do_mirroring, mirror_axes, patch_size,\n",
        "                                                                     regions_class_order, use_gaussian, pad_border_mode,\n",
        "                                                                     pad_kwargs, all_in_gpu, verbose)\n",
        "                    else:\n",
        "                        res = self._internal_predict_2D_2Dconv(x, patch_size, do_mirroring, mirror_axes, regions_class_order,\n",
        "                                                               pad_border_mode, pad_kwargs, verbose)\n",
        "                else:\n",
        "                    raise RuntimeError(\"Invalid conv op, cannot determine what dimensionality (2d/3d) the network is\")\n",
        "\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_gaussian(patch_size, sigma_scale=1. / 8) -> np.ndarray:\n",
        "        tmp = np.zeros(patch_size)\n",
        "        center_coords = [i // 2 for i in patch_size]\n",
        "        sigmas = [i * sigma_scale for i in patch_size]\n",
        "        tmp[tuple(center_coords)] = 1\n",
        "        gaussian_importance_map = gaussian_filter(tmp, sigmas, 0, mode='constant', cval=0)\n",
        "        gaussian_importance_map = gaussian_importance_map / np.max(gaussian_importance_map) * 1\n",
        "        gaussian_importance_map = gaussian_importance_map.astype(np.float32)\n",
        "\n",
        "        # gaussian_importance_map cannot be 0, otherwise we may end up with nans!\n",
        "        gaussian_importance_map[gaussian_importance_map == 0] = np.min(\n",
        "            gaussian_importance_map[gaussian_importance_map != 0])\n",
        "\n",
        "        return gaussian_importance_map\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_steps_for_sliding_window(patch_size: Tuple[int, ...], image_size: Tuple[int, ...], step_size: float) -> List[List[int]]:\n",
        "        assert [i >= j for i, j in zip(image_size, patch_size)], \"image size must be as large or larger than patch_size\"\n",
        "        assert 0 < step_size <= 1, 'step_size must be larger than 0 and smaller or equal to 1'\n",
        "\n",
        "        # our step width is patch_size*step_size at most, but can be narrower. For example if we have image size of\n",
        "        # 110, patch size of 64 and step_size of 0.5, then we want to make 3 steps starting at coordinate 0, 23, 46\n",
        "        target_step_sizes_in_voxels = [i * step_size for i in patch_size]\n",
        "\n",
        "        num_steps = [int(np.ceil((i - k) / j)) + 1 for i, j, k in zip(image_size, target_step_sizes_in_voxels, patch_size)]\n",
        "\n",
        "        steps = []\n",
        "        for dim in range(len(patch_size)):\n",
        "            # the highest step value for this dimension is\n",
        "            max_step_value = image_size[dim] - patch_size[dim]\n",
        "            if num_steps[dim] > 1:\n",
        "                actual_step_size = max_step_value / (num_steps[dim] - 1)\n",
        "            else:\n",
        "                actual_step_size = 99999999999  # does not matter because there is only one step at 0\n",
        "\n",
        "            steps_here = [int(np.round(actual_step_size * i)) for i in range(num_steps[dim])]\n",
        "\n",
        "            steps.append(steps_here)\n",
        "\n",
        "        return steps\n",
        "\n",
        "    def _internal_predict_3D_3Dconv_tiled(self, x: np.ndarray, step_size: float, do_mirroring: bool, mirror_axes: tuple,\n",
        "                                          patch_size: tuple, regions_class_order: tuple, use_gaussian: bool,\n",
        "                                          pad_border_mode: str, pad_kwargs: dict, all_in_gpu: bool,\n",
        "                                          verbose: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "        # better safe than sorry\n",
        "        assert len(x.shape) == 4, \"x must be (c, x, y, z)\"\n",
        "        assert self.get_device() != \"cpu\"\n",
        "        if verbose: print(\"step_size:\", step_size)\n",
        "        if verbose: print(\"do mirror:\", do_mirroring)\n",
        "\n",
        "        assert patch_size is not None, \"patch_size cannot be None for tiled prediction\"\n",
        "\n",
        "        # for sliding window inference the image must at least be as large as the patch size. It does not matter\n",
        "        # whether the shape is divisible by 2**num_pool as long as the patch size is\n",
        "        data, slicer = pad_nd_image(x, patch_size, pad_border_mode, pad_kwargs, True, None)\n",
        "        data_shape = data.shape  # still c, x, y, z\n",
        "\n",
        "        # compute the steps for sliding window\n",
        "        steps = self._compute_steps_for_sliding_window(patch_size, data_shape[1:], step_size)\n",
        "        num_tiles = len(steps[0]) * len(steps[1]) * len(steps[2])\n",
        "\n",
        "        if verbose:\n",
        "            print(\"data shape:\", data_shape)\n",
        "            print(\"patch size:\", patch_size)\n",
        "            print(\"steps (x, y, and z):\", steps)\n",
        "            print(\"number of tiles:\", num_tiles)\n",
        "\n",
        "        # we only need to compute that once. It can take a while to compute this due to the large sigma in\n",
        "        # gaussian_filter\n",
        "        if use_gaussian and num_tiles > 1:\n",
        "            if self._gaussian_3d is None or not all(\n",
        "                    [i == j for i, j in zip(patch_size, self._patch_size_for_gaussian_3d)]):\n",
        "                if verbose: print('computing Gaussian')\n",
        "                gaussian_importance_map = self._get_gaussian(patch_size, sigma_scale=1. / 8)\n",
        "\n",
        "                self._gaussian_3d = gaussian_importance_map\n",
        "                self._patch_size_for_gaussian_3d = patch_size\n",
        "            else:\n",
        "                if verbose: print(\"using precomputed Gaussian\")\n",
        "                gaussian_importance_map = self._gaussian_3d\n",
        "\n",
        "            gaussian_importance_map = torch.from_numpy(gaussian_importance_map).cuda(self.get_device(),\n",
        "                                                                                     non_blocking=True)\n",
        "\n",
        "        else:\n",
        "            gaussian_importance_map = None\n",
        "\n",
        "        if all_in_gpu:\n",
        "            # If we run the inference in GPU only (meaning all tensors are allocated on the GPU, this reduces\n",
        "            # CPU-GPU communication but required more GPU memory) we need to preallocate a few things on GPU\n",
        "\n",
        "            if use_gaussian and num_tiles > 1:\n",
        "                # half precision for the outputs should be good enough. If the outputs here are half, the\n",
        "                # gaussian_importance_map should be as well\n",
        "                gaussian_importance_map = gaussian_importance_map.half()\n",
        "\n",
        "                # make sure we did not round anything to 0\n",
        "                gaussian_importance_map[gaussian_importance_map == 0] = gaussian_importance_map[\n",
        "                    gaussian_importance_map != 0].min()\n",
        "\n",
        "                add_for_nb_of_preds = gaussian_importance_map\n",
        "            else:\n",
        "                add_for_nb_of_preds = torch.ones(data.shape[1:], device=self.get_device())\n",
        "\n",
        "            if verbose: print(\"initializing result array (on GPU)\")\n",
        "            aggregated_results = torch.zeros([self.num_classes] + list(data.shape[1:]), dtype=torch.half,\n",
        "                                             device=self.get_device())\n",
        "\n",
        "            if verbose: print(\"moving data to GPU\")\n",
        "            data = torch.from_numpy(data).cuda(self.get_device(), non_blocking=True)\n",
        "\n",
        "            if verbose: print(\"initializing result_numsamples (on GPU)\")\n",
        "            aggregated_nb_of_predictions = torch.zeros([self.num_classes] + list(data.shape[1:]), dtype=torch.half,\n",
        "                                                       device=self.get_device())\n",
        "        else:\n",
        "            if use_gaussian and num_tiles > 1:\n",
        "                add_for_nb_of_preds = self._gaussian_3d\n",
        "            else:\n",
        "                add_for_nb_of_preds = np.ones(data.shape[1:], dtype=np.float32)\n",
        "\n",
        "            aggregated_results = np.zeros([self.num_classes] + list(data.shape[1:]), dtype=np.float32)\n",
        "            aggregated_nb_of_predictions = np.zeros([self.num_classes] + list(data.shape[1:]), dtype=np.float32)\n",
        "\n",
        "        for x in steps[0]:\n",
        "            lb_x = x\n",
        "            ub_x = x + patch_size[0]\n",
        "            for y in steps[1]:\n",
        "                lb_y = y\n",
        "                ub_y = y + patch_size[1]\n",
        "                for z in steps[2]:\n",
        "                    lb_z = z\n",
        "                    ub_z = z + patch_size[2]\n",
        "\n",
        "                    predicted_patch = self._internal_maybe_mirror_and_pred_3D(\n",
        "                        data[None, :, lb_x:ub_x, lb_y:ub_y, lb_z:ub_z], mirror_axes, do_mirroring,\n",
        "                        gaussian_importance_map)[0]\n",
        "\n",
        "                    if all_in_gpu:\n",
        "                        predicted_patch = predicted_patch.half()\n",
        "                    else:\n",
        "                        predicted_patch = predicted_patch.cpu().numpy()\n",
        "\n",
        "                    aggregated_results[:, lb_x:ub_x, lb_y:ub_y, lb_z:ub_z] += predicted_patch\n",
        "                    aggregated_nb_of_predictions[:, lb_x:ub_x, lb_y:ub_y, lb_z:ub_z] += add_for_nb_of_preds\n",
        "\n",
        "        # we reverse the padding here (remeber that we padded the input to be at least as large as the patch size\n",
        "        slicer = tuple(\n",
        "            [slice(0, aggregated_results.shape[i]) for i in\n",
        "             range(len(aggregated_results.shape) - (len(slicer) - 1))] + slicer[1:])\n",
        "        aggregated_results = aggregated_results[slicer]\n",
        "        aggregated_nb_of_predictions = aggregated_nb_of_predictions[slicer]\n",
        "\n",
        "        # computing the class_probabilities by dividing the aggregated result with result_numsamples\n",
        "        class_probabilities = aggregated_results / aggregated_nb_of_predictions\n",
        "\n",
        "        if regions_class_order is None:\n",
        "            predicted_segmentation = class_probabilities.argmax(0)\n",
        "        else:\n",
        "            if all_in_gpu:\n",
        "                class_probabilities_here = class_probabilities.detach().cpu().numpy()\n",
        "            else:\n",
        "                class_probabilities_here = class_probabilities\n",
        "            predicted_segmentation = np.zeros(class_probabilities_here.shape[1:], dtype=np.float32)\n",
        "            for i, c in enumerate(regions_class_order):\n",
        "                predicted_segmentation[class_probabilities_here[i] > 0.5] = c\n",
        "\n",
        "        if all_in_gpu:\n",
        "            if verbose: print(\"copying results to CPU\")\n",
        "\n",
        "            if regions_class_order is None:\n",
        "                predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
        "\n",
        "            class_probabilities = class_probabilities.detach().cpu().numpy()\n",
        "\n",
        "        if verbose: print(\"prediction done\")\n",
        "        return predicted_segmentation, class_probabilities\n",
        "\n",
        "    def _internal_predict_2D_2Dconv(self, x: np.ndarray, min_size: Tuple[int, int], do_mirroring: bool,\n",
        "                                    mirror_axes: tuple = (0, 1, 2), regions_class_order: tuple = None,\n",
        "                                    pad_border_mode: str = \"constant\", pad_kwargs: dict = None,\n",
        "                                    verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        This one does fully convolutional inference. No sliding window\n",
        "        \"\"\"\n",
        "        assert len(x.shape) == 3, \"x must be (c, x, y)\"\n",
        "        assert self.get_device() != \"cpu\"\n",
        "        assert self.input_shape_must_be_divisible_by is not None, 'input_shape_must_be_divisible_by must be set to ' \\\n",
        "                                                                  'run _internal_predict_2D_2Dconv'\n",
        "        if verbose: print(\"do mirror:\", do_mirroring)\n",
        "\n",
        "        data, slicer = pad_nd_image(x, min_size, pad_border_mode, pad_kwargs, True,\n",
        "                                    self.input_shape_must_be_divisible_by)\n",
        "\n",
        "        predicted_probabilities = self._internal_maybe_mirror_and_pred_2D(data[None], mirror_axes, do_mirroring,\n",
        "                                                                          None)[0]\n",
        "\n",
        "        slicer = tuple(\n",
        "            [slice(0, predicted_probabilities.shape[i]) for i in range(len(predicted_probabilities.shape) -\n",
        "                                                                       (len(slicer) - 1))] + slicer[1:])\n",
        "        predicted_probabilities = predicted_probabilities[slicer]\n",
        "\n",
        "        if regions_class_order is None:\n",
        "            predicted_segmentation = predicted_probabilities.argmax(0)\n",
        "            predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
        "            predicted_probabilities = predicted_probabilities.detach().cpu().numpy()\n",
        "        else:\n",
        "            predicted_probabilities = predicted_probabilities.detach().cpu().numpy()\n",
        "            predicted_segmentation = np.zeros(predicted_probabilities.shape[1:], dtype=np.float32)\n",
        "            for i, c in enumerate(regions_class_order):\n",
        "                predicted_segmentation[predicted_probabilities[i] > 0.5] = c\n",
        "\n",
        "        return predicted_segmentation, predicted_probabilities\n",
        "\n",
        "    def _internal_predict_3D_3Dconv(self, x: np.ndarray, min_size: Tuple[int, ...], do_mirroring: bool,\n",
        "                                    mirror_axes: tuple = (0, 1, 2), regions_class_order: tuple = None,\n",
        "                                    pad_border_mode: str = \"constant\", pad_kwargs: dict = None,\n",
        "                                    verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        This one does fully convolutional inference. No sliding window\n",
        "        \"\"\"\n",
        "        assert len(x.shape) == 4, \"x must be (c, x, y, z)\"\n",
        "        assert self.get_device() != \"cpu\"\n",
        "        assert self.input_shape_must_be_divisible_by is not None, 'input_shape_must_be_divisible_by must be set to ' \\\n",
        "                                                                  'run _internal_predict_3D_3Dconv'\n",
        "        if verbose: print(\"do mirror:\", do_mirroring)\n",
        "\n",
        "        data, slicer = pad_nd_image(x, min_size, pad_border_mode, pad_kwargs, True,\n",
        "                                    self.input_shape_must_be_divisible_by)\n",
        "\n",
        "        predicted_probabilities = self._internal_maybe_mirror_and_pred_3D(data[None], mirror_axes, do_mirroring,\n",
        "                                                                          None)[0]\n",
        "\n",
        "        slicer = tuple(\n",
        "            [slice(0, predicted_probabilities.shape[i]) for i in range(len(predicted_probabilities.shape) -\n",
        "                                                                       (len(slicer) - 1))] + slicer[1:])\n",
        "        predicted_probabilities = predicted_probabilities[slicer]\n",
        "\n",
        "        if regions_class_order is None:\n",
        "            predicted_segmentation = predicted_probabilities.argmax(0)\n",
        "            predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
        "            predicted_probabilities = predicted_probabilities.detach().cpu().numpy()\n",
        "        else:\n",
        "            predicted_probabilities = predicted_probabilities.detach().cpu().numpy()\n",
        "            predicted_segmentation = np.zeros(predicted_probabilities.shape[1:], dtype=np.float32)\n",
        "            for i, c in enumerate(regions_class_order):\n",
        "                predicted_segmentation[predicted_probabilities[i] > 0.5] = c\n",
        "\n",
        "        return predicted_segmentation, predicted_probabilities\n",
        "\n",
        "    def _internal_maybe_mirror_and_pred_3D(self, x: Union[np.ndarray, torch.tensor], mirror_axes: tuple,\n",
        "                                           do_mirroring: bool = True,\n",
        "                                           mult: np.ndarray or torch.tensor = None) -> torch.tensor:\n",
        "        assert len(x.shape) == 5, 'x must be (b, c, x, y, z)'\n",
        "        # everything in here takes place on the GPU. If x and mult are not yet on GPU this will be taken care of here\n",
        "        # we now return a cuda tensor! Not numpy array!\n",
        "\n",
        "        x = to_cuda(maybe_to_torch(x), gpu_id=self.get_device())\n",
        "        result_torch = torch.zeros([1, self.num_classes] + list(x.shape[2:]),\n",
        "                                   dtype=torch.float).cuda(self.get_device(), non_blocking=True)\n",
        "\n",
        "        if mult is not None:\n",
        "            mult = to_cuda(maybe_to_torch(mult), gpu_id=self.get_device())\n",
        "\n",
        "        if do_mirroring:\n",
        "            mirror_idx = 8\n",
        "            num_results = 2 ** len(mirror_axes)\n",
        "        else:\n",
        "            mirror_idx = 1\n",
        "            num_results = 1\n",
        "\n",
        "        for m in range(mirror_idx):\n",
        "            if m == 0:\n",
        "                pred = self.inference_apply_nonlin(self(x))\n",
        "                result_torch += 1 / num_results * pred\n",
        "\n",
        "            if m == 1 and (2 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (4, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (4,))\n",
        "\n",
        "            if m == 2 and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (3, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (3,))\n",
        "\n",
        "            if m == 3 and (2 in mirror_axes) and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (4, 3))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (4, 3))\n",
        "\n",
        "            if m == 4 and (0 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (2, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (2,))\n",
        "\n",
        "            if m == 5 and (0 in mirror_axes) and (2 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (4, 2))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (4, 2))\n",
        "\n",
        "            if m == 6 and (0 in mirror_axes) and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (3, 2))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (3, 2))\n",
        "\n",
        "            if m == 7 and (0 in mirror_axes) and (1 in mirror_axes) and (2 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (4, 3, 2))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (4, 3, 2))\n",
        "\n",
        "        if mult is not None:\n",
        "            result_torch[:, :] *= mult\n",
        "\n",
        "        return result_torch\n",
        "\n",
        "    def _internal_maybe_mirror_and_pred_2D(self, x: Union[np.ndarray, torch.tensor], mirror_axes: tuple,\n",
        "                                           do_mirroring: bool = True,\n",
        "                                           mult: np.ndarray or torch.tensor = None) -> torch.tensor:\n",
        "        # everything in here takes place on the GPU. If x and mult are not yet on GPU this will be taken care of here\n",
        "        # we now return a cuda tensor! Not numpy array!\n",
        "        assert len(x.shape) == 4, 'x must be (b, c, x, y)'\n",
        "\n",
        "        x = to_cuda(maybe_to_torch(x), gpu_id=self.get_device())\n",
        "        result_torch = torch.zeros([x.shape[0], self.num_classes] + list(x.shape[2:]),\n",
        "                                   dtype=torch.float).cuda(self.get_device(), non_blocking=True)\n",
        "\n",
        "        if mult is not None:\n",
        "            mult = to_cuda(maybe_to_torch(mult), gpu_id=self.get_device())\n",
        "\n",
        "        if do_mirroring:\n",
        "            mirror_idx = 4\n",
        "            num_results = 2 ** len(mirror_axes)\n",
        "        else:\n",
        "            mirror_idx = 1\n",
        "            num_results = 1\n",
        "\n",
        "        for m in range(mirror_idx):\n",
        "            if m == 0:\n",
        "                pred = self.inference_apply_nonlin(self(x))\n",
        "                result_torch += 1 / num_results * pred\n",
        "\n",
        "            if m == 1 and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (3, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (3, ))\n",
        "\n",
        "            if m == 2 and (0 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (2, ))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (2, ))\n",
        "\n",
        "            if m == 3 and (0 in mirror_axes) and (1 in mirror_axes):\n",
        "                pred = self.inference_apply_nonlin(self(torch.flip(x, (3, 2))))\n",
        "                result_torch += 1 / num_results * torch.flip(pred, (3, 2))\n",
        "\n",
        "        if mult is not None:\n",
        "            result_torch[:, :] *= mult\n",
        "\n",
        "        return result_torch\n",
        "\n",
        "    def _internal_predict_2D_2Dconv_tiled(self, x: np.ndarray, step_size: float, do_mirroring: bool, mirror_axes: tuple,\n",
        "                                          patch_size: tuple, regions_class_order: tuple, use_gaussian: bool,\n",
        "                                          pad_border_mode: str, pad_kwargs: dict, all_in_gpu: bool,\n",
        "                                          verbose: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        # better safe than sorry\n",
        "        assert len(x.shape) == 3, \"x must be (c, x, y)\"\n",
        "        assert self.get_device() != \"cpu\"\n",
        "        if verbose: print(\"step_size:\", step_size)\n",
        "        if verbose: print(\"do mirror:\", do_mirroring)\n",
        "\n",
        "        assert patch_size is not None, \"patch_size cannot be None for tiled prediction\"\n",
        "\n",
        "        # for sliding window inference the image must at least be as large as the patch size. It does not matter\n",
        "        # whether the shape is divisible by 2**num_pool as long as the patch size is\n",
        "        data, slicer = pad_nd_image(x, patch_size, pad_border_mode, pad_kwargs, True, None)\n",
        "        data_shape = data.shape  # still c, x, y\n",
        "\n",
        "        # compute the steps for sliding window\n",
        "        steps = self._compute_steps_for_sliding_window(patch_size, data_shape[1:], step_size)\n",
        "        num_tiles = len(steps[0]) * len(steps[1])\n",
        "\n",
        "        if verbose:\n",
        "            print(\"data shape:\", data_shape)\n",
        "            print(\"patch size:\", patch_size)\n",
        "            print(\"steps (x, y, and z):\", steps)\n",
        "            print(\"number of tiles:\", num_tiles)\n",
        "\n",
        "        # we only need to compute that once. It can take a while to compute this due to the large sigma in\n",
        "        # gaussian_filter\n",
        "        if use_gaussian and num_tiles > 1:\n",
        "            if self._gaussian_2d is None or not all(\n",
        "                    [i == j for i, j in zip(patch_size, self._patch_size_for_gaussian_2d)]):\n",
        "                if verbose: print('computing Gaussian')\n",
        "                gaussian_importance_map = self._get_gaussian(patch_size, sigma_scale=1. / 8)\n",
        "\n",
        "                self._gaussian_2d = gaussian_importance_map\n",
        "                self._patch_size_for_gaussian_2d = patch_size\n",
        "            else:\n",
        "                if verbose: print(\"using precomputed Gaussian\")\n",
        "                gaussian_importance_map = self._gaussian_2d\n",
        "\n",
        "            gaussian_importance_map = torch.from_numpy(gaussian_importance_map).cuda(self.get_device(),\n",
        "                                                                                     non_blocking=True)\n",
        "        else:\n",
        "            gaussian_importance_map = None\n",
        "\n",
        "        if all_in_gpu:\n",
        "            # If we run the inference in GPU only (meaning all tensors are allocated on the GPU, this reduces\n",
        "            # CPU-GPU communication but required more GPU memory) we need to preallocate a few things on GPU\n",
        "\n",
        "            if use_gaussian and num_tiles > 1:\n",
        "                # half precision for the outputs should be good enough. If the outputs here are half, the\n",
        "                # gaussian_importance_map should be as well\n",
        "                gaussian_importance_map = gaussian_importance_map.half()\n",
        "\n",
        "                # make sure we did not round anything to 0\n",
        "                gaussian_importance_map[gaussian_importance_map == 0] = gaussian_importance_map[\n",
        "                    gaussian_importance_map != 0].min()\n",
        "\n",
        "                add_for_nb_of_preds = gaussian_importance_map\n",
        "            else:\n",
        "                add_for_nb_of_preds = torch.ones(data.shape[1:], device=self.get_device())\n",
        "\n",
        "            if verbose: print(\"initializing result array (on GPU)\")\n",
        "            aggregated_results = torch.zeros([self.num_classes] + list(data.shape[1:]), dtype=torch.half,\n",
        "                                             device=self.get_device())\n",
        "\n",
        "            if verbose: print(\"moving data to GPU\")\n",
        "            data = torch.from_numpy(data).cuda(self.get_device(), non_blocking=True)\n",
        "\n",
        "            if verbose: print(\"initializing result_numsamples (on GPU)\")\n",
        "            aggregated_nb_of_predictions = torch.zeros([self.num_classes] + list(data.shape[1:]), dtype=torch.half,\n",
        "                                                       device=self.get_device())\n",
        "        else:\n",
        "            if use_gaussian and num_tiles > 1:\n",
        "                add_for_nb_of_preds = self._gaussian_2d\n",
        "            else:\n",
        "                add_for_nb_of_preds = np.ones(data.shape[1:], dtype=np.float32)\n",
        "            aggregated_results = np.zeros([self.num_classes] + list(data.shape[1:]), dtype=np.float32)\n",
        "            aggregated_nb_of_predictions = np.zeros([self.num_classes] + list(data.shape[1:]), dtype=np.float32)\n",
        "\n",
        "        for x in steps[0]:\n",
        "            lb_x = x\n",
        "            ub_x = x + patch_size[0]\n",
        "            for y in steps[1]:\n",
        "                lb_y = y\n",
        "                ub_y = y + patch_size[1]\n",
        "\n",
        "                predicted_patch = self._internal_maybe_mirror_and_pred_2D(\n",
        "                    data[None, :, lb_x:ub_x, lb_y:ub_y], mirror_axes, do_mirroring,\n",
        "                    gaussian_importance_map)[0]\n",
        "\n",
        "                if all_in_gpu:\n",
        "                    predicted_patch = predicted_patch.half()\n",
        "                else:\n",
        "                    predicted_patch = predicted_patch.cpu().numpy()\n",
        "\n",
        "                aggregated_results[:, lb_x:ub_x, lb_y:ub_y] += predicted_patch\n",
        "                aggregated_nb_of_predictions[:, lb_x:ub_x, lb_y:ub_y] += add_for_nb_of_preds\n",
        "\n",
        "        # we reverse the padding here (remeber that we padded the input to be at least as large as the patch size\n",
        "        slicer = tuple(\n",
        "            [slice(0, aggregated_results.shape[i]) for i in\n",
        "             range(len(aggregated_results.shape) - (len(slicer) - 1))] + slicer[1:])\n",
        "        aggregated_results = aggregated_results[slicer]\n",
        "        aggregated_nb_of_predictions = aggregated_nb_of_predictions[slicer]\n",
        "\n",
        "        # computing the class_probabilities by dividing the aggregated result with result_numsamples\n",
        "        class_probabilities = aggregated_results / aggregated_nb_of_predictions\n",
        "\n",
        "        if regions_class_order is None:\n",
        "            predicted_segmentation = class_probabilities.argmax(0)\n",
        "        else:\n",
        "            if all_in_gpu:\n",
        "                class_probabilities_here = class_probabilities.detach().cpu().numpy()\n",
        "            else:\n",
        "                class_probabilities_here = class_probabilities\n",
        "            predicted_segmentation = np.zeros(class_probabilities_here.shape[1:], dtype=np.float32)\n",
        "            for i, c in enumerate(regions_class_order):\n",
        "                predicted_segmentation[class_probabilities_here[i] > 0.5] = c\n",
        "\n",
        "        if all_in_gpu:\n",
        "            if verbose: print(\"copying results to CPU\")\n",
        "\n",
        "            if regions_class_order is None:\n",
        "                predicted_segmentation = predicted_segmentation.detach().cpu().numpy()\n",
        "\n",
        "            class_probabilities = class_probabilities.detach().cpu().numpy()\n",
        "\n",
        "        if verbose: print(\"prediction done\")\n",
        "        return predicted_segmentation, class_probabilities\n",
        "\n",
        "    def _internal_predict_3D_2Dconv(self, x: np.ndarray, min_size: Tuple[int, int], do_mirroring: bool,\n",
        "                                    mirror_axes: tuple = (0, 1), regions_class_order: tuple = None,\n",
        "                                    pad_border_mode: str = \"constant\", pad_kwargs: dict = None,\n",
        "                                    all_in_gpu: bool = False, verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if all_in_gpu:\n",
        "            raise NotImplementedError\n",
        "        assert len(x.shape) == 4, \"data must be c, x, y, z\"\n",
        "        predicted_segmentation = []\n",
        "        softmax_pred = []\n",
        "        for s in range(x.shape[1]):\n",
        "            pred_seg, softmax_pres = self._internal_predict_2D_2Dconv(\n",
        "                x[:, s], min_size, do_mirroring, mirror_axes, regions_class_order, pad_border_mode, pad_kwargs, verbose)\n",
        "            predicted_segmentation.append(pred_seg[None])\n",
        "            softmax_pred.append(softmax_pres[None])\n",
        "        predicted_segmentation = np.vstack(predicted_segmentation)\n",
        "        softmax_pred = np.vstack(softmax_pred).transpose((1, 0, 2, 3))\n",
        "        return predicted_segmentation, softmax_pred\n",
        "\n",
        "    def predict_3D_pseudo3D_2Dconv(self, x: np.ndarray, min_size: Tuple[int, int], do_mirroring: bool,\n",
        "                                   mirror_axes: tuple = (0, 1), regions_class_order: tuple = None,\n",
        "                                   pseudo3D_slices: int = 5, all_in_gpu: bool = False,\n",
        "                                   pad_border_mode: str = \"constant\", pad_kwargs: dict = None,\n",
        "                                   verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if all_in_gpu:\n",
        "            raise NotImplementedError\n",
        "        assert len(x.shape) == 4, \"data must be c, x, y, z\"\n",
        "        assert pseudo3D_slices % 2 == 1, \"pseudo3D_slices must be odd\"\n",
        "        extra_slices = (pseudo3D_slices - 1) // 2\n",
        "\n",
        "        shp_for_pad = np.array(x.shape)\n",
        "        shp_for_pad[1] = extra_slices\n",
        "\n",
        "        pad = np.zeros(shp_for_pad, dtype=np.float32)\n",
        "        data = np.concatenate((pad, x, pad), 1)\n",
        "\n",
        "        predicted_segmentation = []\n",
        "        softmax_pred = []\n",
        "        for s in range(extra_slices, data.shape[1] - extra_slices):\n",
        "            d = data[:, (s - extra_slices):(s + extra_slices + 1)]\n",
        "            d = d.reshape((-1, d.shape[-2], d.shape[-1]))\n",
        "            pred_seg, softmax_pres = \\\n",
        "                self._internal_predict_2D_2Dconv(d, min_size, do_mirroring, mirror_axes,\n",
        "                                                 regions_class_order, pad_border_mode, pad_kwargs, verbose)\n",
        "            predicted_segmentation.append(pred_seg[None])\n",
        "            softmax_pred.append(softmax_pres[None])\n",
        "        predicted_segmentation = np.vstack(predicted_segmentation)\n",
        "        softmax_pred = np.vstack(softmax_pred).transpose((1, 0, 2, 3))\n",
        "\n",
        "        return predicted_segmentation, softmax_pred\n",
        "\n",
        "    def _internal_predict_3D_2Dconv_tiled(self, x: np.ndarray, patch_size: Tuple[int, int], do_mirroring: bool,\n",
        "                                          mirror_axes: tuple = (0, 1), step_size: float = 0.5,\n",
        "                                          regions_class_order: tuple = None, use_gaussian: bool = False,\n",
        "                                          pad_border_mode: str = \"edge\", pad_kwargs: dict =None,\n",
        "                                          all_in_gpu: bool = False,\n",
        "                                          verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        if all_in_gpu:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        assert len(x.shape) == 4, \"data must be c, x, y, z\"\n",
        "\n",
        "        predicted_segmentation = []\n",
        "        softmax_pred = []\n",
        "\n",
        "        for s in range(x.shape[1]):\n",
        "            pred_seg, softmax_pres = self._internal_predict_2D_2Dconv_tiled(\n",
        "                x[:, s], step_size, do_mirroring, mirror_axes, patch_size, regions_class_order, use_gaussian,\n",
        "                pad_border_mode, pad_kwargs, all_in_gpu, verbose)\n",
        "\n",
        "            predicted_segmentation.append(pred_seg[None])\n",
        "            softmax_pred.append(softmax_pres[None])\n",
        "\n",
        "        predicted_segmentation = np.vstack(predicted_segmentation)\n",
        "        softmax_pred = np.vstack(softmax_pred).transpose((1, 0, 2, 3))\n",
        "\n",
        "        return predicted_segmentation, softmax_pred\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(SegmentationNetwork._compute_steps_for_sliding_window((30, 224, 224), (162, 529, 529), 0.5))\n",
        "    print(SegmentationNetwork._compute_steps_for_sliding_window((30, 224, 224), (162, 529, 529), 1))\n",
        "    print(SegmentationNetwork._compute_steps_for_sliding_window((30, 224, 224), (162, 529, 529), 0.1))\n",
        "\n",
        "    print(SegmentationNetwork._compute_steps_for_sliding_window((30, 224, 224), (60, 448, 224), 1))\n",
        "    print(SegmentationNetwork._compute_steps_for_sliding_window((30, 224, 224), (60, 448, 224), 0.5))\n",
        "\n",
        "    print(SegmentationNetwork._compute_steps_for_sliding_window((30, 224, 224), (30, 224, 224), 1))\n",
        "    print(SegmentationNetwork._compute_steps_for_sliding_window((30, 224, 224), (30, 224, 224), 0.125))\n",
        "\n",
        "\n",
        "    print(SegmentationNetwork._compute_steps_for_sliding_window((123, 54, 123), (246, 162, 369), 0.25))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ILWMyY2BFp8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229d8a96-be51-4aa5-ce5a-121e4b19f439"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 15, 29, 44, 59, 73, 88, 103, 117, 132], [0, 102, 203, 305], [0, 102, 203, 305]]\n",
            "[[0, 26, 53, 79, 106, 132], [0, 152, 305], [0, 152, 305]]\n",
            "[[0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99, 102, 105, 108, 111, 114, 117, 120, 123, 126, 129, 132], [0, 22, 44, 65, 87, 109, 131, 152, 174, 196, 218, 240, 261, 283, 305], [0, 22, 44, 65, 87, 109, 131, 152, 174, 196, 218, 240, 261, 283, 305]]\n",
            "[[0, 30], [0, 224], [0]]\n",
            "[[0, 15, 30], [0, 112, 224], [0]]\n",
            "[[0], [0], [0]]\n",
            "[[0], [0], [0]]\n",
            "[[0, 31, 62, 92, 123], [0, 14, 27, 40, 54, 68, 81, 94, 108], [0, 31, 62, 92, 123, 154, 184, 215, 246]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-99e559c65698>:22: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####dynunet_block"
      ],
      "metadata": {
        "id": "0rKuVQ_HMoP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Sequence, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from monai.networks.blocks.convolutions import Convolution\n",
        "from monai.networks.layers.factories import Act, Norm\n",
        "from monai.networks.layers.utils import get_act_layer, get_norm_layer\n",
        "\n",
        "\n",
        "class UnetResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A skip-connection based module that can be used for DynUNet, based on:\n",
        "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
        "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
        "\n",
        "    Args:\n",
        "        spatial_dims: number of spatial dimensions.\n",
        "        in_channels: number of input channels.\n",
        "        out_channels: number of output channels.\n",
        "        kernel_size: convolution kernel size.\n",
        "        stride: convolution stride.\n",
        "        norm_name: feature normalization type and arguments.\n",
        "        act_name: activation layer type and arguments.\n",
        "        dropout: dropout probability.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: Union[Sequence[int], int],\n",
        "        stride: Union[Sequence[int], int],\n",
        "        norm_name: Union[Tuple, str],\n",
        "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
        "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = get_conv_layer(\n",
        "            spatial_dims,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            dropout=dropout,\n",
        "            conv_only=True,\n",
        "        )\n",
        "        self.conv2 = get_conv_layer(\n",
        "            spatial_dims, out_channels, out_channels, kernel_size=kernel_size, stride=1, dropout=dropout, conv_only=True\n",
        "        )\n",
        "        self.lrelu = get_act_layer(name=act_name)\n",
        "        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
        "        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
        "        self.downsample = in_channels != out_channels\n",
        "        stride_np = np.atleast_1d(stride)\n",
        "        if not np.all(stride_np == 1):\n",
        "            self.downsample = True\n",
        "        if self.downsample:\n",
        "            self.conv3 = get_conv_layer(\n",
        "                spatial_dims, in_channels, out_channels, kernel_size=1, stride=stride, dropout=dropout, conv_only=True\n",
        "            )\n",
        "            self.norm3 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        residual = inp\n",
        "        out = self.conv1(inp)\n",
        "        out = self.norm1(out)\n",
        "        out = self.lrelu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.norm2(out)\n",
        "        if hasattr(self, \"conv3\"):\n",
        "            residual = self.conv3(residual)\n",
        "        if hasattr(self, \"norm3\"):\n",
        "            residual = self.norm3(residual)\n",
        "        out += residual\n",
        "        out = self.lrelu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UnetBasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A CNN module module that can be used for DynUNet, based on:\n",
        "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
        "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
        "\n",
        "    Args:\n",
        "        spatial_dims: number of spatial dimensions.\n",
        "        in_channels: number of input channels.\n",
        "        out_channels: number of output channels.\n",
        "        kernel_size: convolution kernel size.\n",
        "        stride: convolution stride.\n",
        "        norm_name: feature normalization type and arguments.\n",
        "        act_name: activation layer type and arguments.\n",
        "        dropout: dropout probability.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: Union[Sequence[int], int],\n",
        "        stride: Union[Sequence[int], int],\n",
        "        norm_name: Union[Tuple, str],\n",
        "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
        "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = get_conv_layer(\n",
        "            spatial_dims,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            dropout=dropout,\n",
        "            conv_only=True,\n",
        "        )\n",
        "        self.conv2 = get_conv_layer(\n",
        "            spatial_dims, out_channels, out_channels, kernel_size=kernel_size, stride=1, dropout=dropout, conv_only=True\n",
        "        )\n",
        "        self.lrelu = get_act_layer(name=act_name)\n",
        "        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
        "        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        out = self.conv1(inp)\n",
        "        out = self.norm1(out)\n",
        "        out = self.lrelu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.lrelu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UnetUpBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    An upsampling module that can be used for DynUNet, based on:\n",
        "    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.\n",
        "    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.\n",
        "\n",
        "    Args:\n",
        "        spatial_dims: number of spatial dimensions.\n",
        "        in_channels: number of input channels.\n",
        "        out_channels: number of output channels.\n",
        "        kernel_size: convolution kernel size.\n",
        "        stride: convolution stride.\n",
        "        upsample_kernel_size: convolution kernel size for transposed convolution layers.\n",
        "        norm_name: feature normalization type and arguments.\n",
        "        act_name: activation layer type and arguments.\n",
        "        dropout: dropout probability.\n",
        "        trans_bias: transposed convolution bias.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        spatial_dims: int,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: Union[Sequence[int], int],\n",
        "        stride: Union[Sequence[int], int],\n",
        "        upsample_kernel_size: Union[Sequence[int], int],\n",
        "        norm_name: Union[Tuple, str],\n",
        "        act_name: Union[Tuple, str] = (\"leakyrelu\", {\"inplace\": True, \"negative_slope\": 0.01}),\n",
        "        dropout: Optional[Union[Tuple, str, float]] = None,\n",
        "        trans_bias: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        upsample_stride = upsample_kernel_size\n",
        "        self.transp_conv = get_conv_layer(\n",
        "            spatial_dims,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=upsample_kernel_size,\n",
        "            stride=upsample_stride,\n",
        "            dropout=dropout,\n",
        "            bias=trans_bias,\n",
        "            conv_only=True,\n",
        "            is_transposed=True,\n",
        "        )\n",
        "        self.conv_block = UnetBasicBlock(\n",
        "            spatial_dims,\n",
        "            out_channels + out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=1,\n",
        "            dropout=dropout,\n",
        "            norm_name=norm_name,\n",
        "            act_name=act_name,\n",
        "        )\n",
        "\n",
        "    def forward(self, inp, skip):\n",
        "        # number of channels for skip should equals to out_channels\n",
        "        out = self.transp_conv(inp)\n",
        "        out = torch.cat((out, skip), dim=1)\n",
        "        out = self.conv_block(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UnetOutBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, spatial_dims: int, in_channels: int, out_channels: int, dropout: Optional[Union[Tuple, str, float]] = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv = get_conv_layer(\n",
        "            spatial_dims, in_channels, out_channels, kernel_size=1, stride=1, dropout=dropout, bias=True, conv_only=True\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        return self.conv(inp)\n",
        "\n",
        "\n",
        "def get_conv_layer(\n",
        "    spatial_dims: int,\n",
        "    in_channels: int,\n",
        "    out_channels: int,\n",
        "    kernel_size: Union[Sequence[int], int] = 3,\n",
        "    stride: Union[Sequence[int], int] = 1,\n",
        "    act: Optional[Union[Tuple, str]] = Act.PRELU,\n",
        "    norm: Union[Tuple, str] = Norm.INSTANCE,\n",
        "    dropout: Optional[Union[Tuple, str, float]] = None,\n",
        "    bias: bool = False,\n",
        "    conv_only: bool = True,\n",
        "    is_transposed: bool = False,\n",
        "):\n",
        "    padding = get_padding(kernel_size, stride)\n",
        "    output_padding = None\n",
        "    if is_transposed:\n",
        "        output_padding = get_output_padding(kernel_size, stride, padding)\n",
        "    return Convolution(\n",
        "        spatial_dims,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        strides=stride,\n",
        "        kernel_size=kernel_size,\n",
        "        act=act,\n",
        "        norm=norm,\n",
        "        dropout=dropout,\n",
        "        bias=bias,\n",
        "        conv_only=conv_only,\n",
        "        is_transposed=is_transposed,\n",
        "        padding=padding,\n",
        "        output_padding=output_padding,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_padding(\n",
        "    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int]\n",
        ") -> Union[Tuple[int, ...], int]:\n",
        "\n",
        "    kernel_size_np = np.atleast_1d(kernel_size)\n",
        "    stride_np = np.atleast_1d(stride)\n",
        "    padding_np = (kernel_size_np - stride_np + 1) / 2\n",
        "    if np.min(padding_np) < 0:\n",
        "        raise AssertionError(\"padding value should not be negative, please change the kernel size and/or stride.\")\n",
        "    padding = tuple(int(p) for p in padding_np)\n",
        "\n",
        "    return padding if len(padding) > 1 else padding[0]\n",
        "\n",
        "\n",
        "def get_output_padding(\n",
        "    kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], padding: Union[Sequence[int], int]\n",
        ") -> Union[Tuple[int, ...], int]:\n",
        "    kernel_size_np = np.atleast_1d(kernel_size)\n",
        "    stride_np = np.atleast_1d(stride)\n",
        "    padding_np = np.atleast_1d(padding)\n",
        "\n",
        "    out_padding_np = 2 * padding_np + stride_np - kernel_size_np\n",
        "    if np.min(out_padding_np) < 0:\n",
        "        raise AssertionError(\"out_padding value should not be negative, please change the kernel size and/or stride.\")\n",
        "    out_padding = tuple(int(p) for p in out_padding_np)\n",
        "\n",
        "    return out_padding if len(out_padding) > 1 else out_padding[0]"
      ],
      "metadata": {
        "id": "ZRiGEf_gJG7c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####synapse.transformerblock"
      ],
      "metadata": {
        "id": "X50-49TvNRRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "#from network_architecture.dynunet_block import UnetResBlock\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A transformer block, based on: \"Shaker et al.,\n",
        "    UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            spatial_dims: int,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            proj_size: int,\n",
        "            num_heads: int,\n",
        "            dropout_rate: float = 0.0,\n",
        "            pos_embed=False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size: the size of the input for each stage.\n",
        "            hidden_size: dimension of hidden layer.\n",
        "            proj_size: projection size for keys and values in the spatial attention module.\n",
        "            num_heads: number of attention heads.\n",
        "            dropout_rate: faction of the input units to drop.\n",
        "            pos_embed: bool argument to determine if positional embedding is used.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if not (0 <= dropout_rate <= 1):\n",
        "            raise ValueError(\"dropout_rate should be between 0 and 1.\")\n",
        "\n",
        "        if hidden_size % num_heads != 0:\n",
        "            print(\"Hidden size is \", hidden_size)\n",
        "            print(\"Num heads is \", num_heads)\n",
        "            raise ValueError(\"hidden_size should be divisible by num_heads.\")\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        self.gamma = nn.Parameter(1e-6 * torch.ones(hidden_size), requires_grad=True)\n",
        "        self.epa_block = EPA(input_size=input_size, hidden_size=hidden_size, proj_size=proj_size, num_heads=num_heads, channel_attn_drop=dropout_rate,spatial_attn_drop=dropout_rate)\n",
        "        self.conv51 = UnetResBlock(spatial_dims, hidden_size, hidden_size, kernel_size=3, stride=1, norm_name=\"batch\")\n",
        "        self.conv8 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(hidden_size, hidden_size, 1))\n",
        "\n",
        "        self.pos_embed = None\n",
        "        if pos_embed:\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, input_size, hidden_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        x = x.reshape(B, C, H * W).permute(0, 2, 1)\n",
        "\n",
        "        if self.pos_embed is not None:\n",
        "            x = x + self.pos_embed\n",
        "        attn = x + self.gamma * self.epa_block(self.norm(x))\n",
        "\n",
        "        attn_skip = attn.reshape(B, H, W, C).permute(0, 3, 1, 2)  # (B, C, H, W)\n",
        "        attn = self.conv51(attn_skip)\n",
        "        x = attn_skip + self.conv8(attn)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class EPA(nn.Module):\n",
        "    \"\"\"\n",
        "        Efficient Paired Attention Block, based on: \"Shaker et al.,\n",
        "        UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation\"\n",
        "        \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, proj_size, num_heads=4, qkv_bias=False,\n",
        "                 channel_attn_drop=0.1, spatial_attn_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "        self.temperature2 = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "        # qkvv are 4 linear layers (query_shared, key_shared, value_spatial, value_channel)\n",
        "        self.qkvv = nn.Linear(hidden_size, hidden_size * 4, bias=qkv_bias)\n",
        "\n",
        "        # E and F are projection matrices with shared weights used in spatial attention module to project\n",
        "        # keys and values from HWD-dimension to P-dimension\n",
        "        self.E = self.F = nn.Linear(input_size, proj_size)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(channel_attn_drop)\n",
        "        self.attn_drop_2 = nn.Dropout(spatial_attn_drop)\n",
        "\n",
        "        self.out_proj = nn.Linear(hidden_size, int(hidden_size // 2))\n",
        "        self.out_proj2 = nn.Linear(hidden_size, int(hidden_size // 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        qkvv = self.qkvv(x)\n",
        "        qkvv = qkvv.reshape(B, N, 4, self.num_heads, C // self.num_heads)\n",
        "\n",
        "        qkvv = qkvv.permute(2, 0, 3, 1, 4)\n",
        "\n",
        "        q_shared, k_shared, v_CA, v_SA = qkvv[0], qkvv[1], qkvv[2], qkvv[3]\n",
        "\n",
        "        q_shared = q_shared.transpose(-2, -1)\n",
        "        k_shared = k_shared.transpose(-2, -1)\n",
        "        v_CA = v_CA.transpose(-2, -1)\n",
        "        v_SA = v_SA.transpose(-2, -1)\n",
        "\n",
        "        k_shared_projected = self.E(k_shared)\n",
        "\n",
        "        v_SA_projected = self.F(v_SA)\n",
        "\n",
        "        q_shared = torch.nn.functional.normalize(q_shared, dim=-1)\n",
        "        k_shared = torch.nn.functional.normalize(k_shared, dim=-1)\n",
        "\n",
        "        attn_CA = (q_shared @ k_shared.transpose(-2, -1)) * self.temperature\n",
        "\n",
        "        attn_CA = attn_CA.softmax(dim=-1)\n",
        "        attn_CA = self.attn_drop(attn_CA)\n",
        "\n",
        "        x_CA = (attn_CA @ v_CA).permute(0, 3, 1, 2).reshape(B, N, C)\n",
        "\n",
        "        attn_SA = (q_shared.permute(0, 1, 3, 2) @ k_shared_projected) * self.temperature2\n",
        "\n",
        "        attn_SA = attn_SA.softmax(dim=-1)\n",
        "        attn_SA = self.attn_drop_2(attn_SA)\n",
        "\n",
        "        x_SA = (attn_SA @ v_SA_projected.transpose(-2, -1)).permute(0, 3, 1, 2).reshape(B, N, C)\n",
        "\n",
        "        # Concat fusion\n",
        "        x_SA = self.out_proj(x_SA)\n",
        "        x_CA = self.out_proj2(x_CA)\n",
        "        x = torch.cat((x_SA, x_CA), dim=-1)\n",
        "        return x\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'temperature', 'temperature2'}"
      ],
      "metadata": {
        "id": "K51YfJLz3sCv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####layers"
      ],
      "metadata": {
        "id": "-kdfrHq9NcY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError\n",
        "        self.normalized_shape = (normalized_shape,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "\n",
        "class PositionalEncodingFourier(nn.Module):\n",
        "    def __init__(self, hidden_dim=32, dim=768, temperature=10000):\n",
        "        super().__init__()\n",
        "        self.token_projection = nn.Conv2d(hidden_dim * 2, dim, kernel_size=1)\n",
        "        self.scale = 2 * math.pi\n",
        "        self.temperature = temperature\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, B, H, W):\n",
        "        mask = torch.zeros(B, H, W).bool().to(self.token_projection.weight.device)\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        eps = 1e-6\n",
        "        y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "        x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.hidden_dim, dtype=torch.float32, device=mask.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.hidden_dim)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(),\n",
        "                             pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(),\n",
        "                             pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        pos = self.token_projection(pos)\n",
        "\n",
        "        return pos"
      ],
      "metadata": {
        "id": "AXA8m4lg32kB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####synapse.model_components"
      ],
      "metadata": {
        "id": "w80l270qNhnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from timm.models.layers import trunc_normal_\n",
        "from typing import Sequence, Tuple, Union\n",
        "from monai.networks.layers.utils import get_norm_layer\n",
        "from monai.utils import optional_import\n",
        "#from network_architecture.layers import LayerNorm\n",
        "#from network_architecture.synapse.transformerblock import TransformerBlock\n",
        "#from network_architecture.dynunet_block import get_conv_layer, UnetResBlock\n",
        "\n",
        "\n",
        "einops, _ = optional_import(\"einops\")\n",
        "class GroupNorm(nn.Module):\n",
        "    def __init__(self, num_groups, in_channels):\n",
        "        super(GroupNorm, self).__init__()\n",
        "        self.GN = nn.GroupNorm(num_groups=num_groups, in_channels=in_channels)\n",
        "        self.GN.weight.data.fill_(1)\n",
        "        self.GN.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        x = self.GN(x)\n",
        "        return x\n",
        "\n",
        "class Conv3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0, bias=False):\n",
        "        super(Conv3D, self).__init__()\n",
        "        padding = padding\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "\n",
        "        nn.init.xavier_normal_(self.conv.weight.data)\n",
        "        if bias !=False:\n",
        "            nn.init.constant_(self.conv.bias.data, 0.0)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "class DConvGNLeaky(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,  return_module=False):\n",
        "        super(DConvGNLeaky, self).__init__()\n",
        "        self.deconv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, bias=True)\n",
        "        self.bn1 = GroupNorm(out_channels)\n",
        "        self.relu = nn.LeakyReLU(0.1, inplace=True)\n",
        "        nn.init.xavier_normal_(self.deconv.weight.data)\n",
        "        nn.init.constant_(self.deconv.bias.data, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.deconv(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class UnetrPPEncoder(nn.Module):\n",
        "    def __init__(self, input_size, patch_size, dims=[32, 64, 128, 256],\n",
        "                 proj_size =[128,128,64,32], depths=[3, 3, 3, 3],  num_heads=4, spatial_dims=2, in_channels=1, dropout=0.0, transformer_dropout_rate=0.15 ,**kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n",
        "        stem_layer = nn.Sequential(\n",
        "            get_conv_layer(spatial_dims, in_channels, dims[0], kernel_size=patch_size, stride=patch_size,\n",
        "                           dropout=dropout, conv_only=True, ),\n",
        "            get_norm_layer(name=(\"group\", {\"num_groups\": 1}), channels=dims[0]),\n",
        "        )\n",
        "        self.downsample_layers.append(stem_layer)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                get_conv_layer(spatial_dims, dims[i], dims[i + 1], kernel_size=(2, 2), stride=(2, 2),\n",
        "                               dropout=dropout, conv_only=True, ),\n",
        "                get_norm_layer(name=(\"group\", {\"num_groups\": dims[i]}), channels=dims[i + 1]),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple Transformer blocks\n",
        "        for i in range(4):\n",
        "            stage_blocks = []\n",
        "            for j in range(depths[i]):\n",
        "                stage_blocks.append(TransformerBlock(spatial_dims=spatial_dims, input_size=input_size[i], hidden_size=dims[i],  proj_size=proj_size[i], num_heads=num_heads,\n",
        "                                     dropout_rate=transformer_dropout_rate, pos_embed=True))\n",
        "            self.stages.append(nn.Sequential(*stage_blocks))\n",
        "        self.hidden_states = []\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, (LayerNorm, nn.LayerNorm)):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        hidden_states = []\n",
        "\n",
        "        x = self.downsample_layers[0](x)\n",
        "        x = self.stages[0](x)\n",
        "\n",
        "        hidden_states.append(x)\n",
        "\n",
        "        for i in range(1, 4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "            if i == 3:  # Reshape the output of the last stage\n",
        "                x = einops.rearrange(x, \"b c h w -> b (h w) c\")\n",
        "            hidden_states.append(x)\n",
        "        return x, hidden_states\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, hidden_states = self.forward_features(x)\n",
        "        return x, hidden_states\n",
        "\n",
        "\n",
        "class UnetrUpBlock(nn.Module):\n",
        "    def     __init__(\n",
        "            self,\n",
        "            spatial_dims: int,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            kernel_size: Union[Sequence[int], int],\n",
        "            upsample_kernel_size: Union[Sequence[int], int],\n",
        "            norm_name: Union[Tuple, str],\n",
        "            proj_size: int = 64,\n",
        "            num_heads: int = 4,\n",
        "            out_size: int = 0,\n",
        "            depth: int = 3,\n",
        "            conv_decoder: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            spatial_dims: number of spatial dimensions.\n",
        "            in_channels: number of input channels.\n",
        "            out_channels: number of output channels.\n",
        "            kernel_size: convolution kernel size.\n",
        "            upsample_kernel_size: convolution kernel size for transposed convolution layers.\n",
        "            norm_name: feature normalization type and arguments.\n",
        "            proj_size: projection size for keys and values in the spatial attention module.\n",
        "            num_heads: number of heads inside each EPA module.\n",
        "            out_size: spatial size for each decoder.\n",
        "            depth: number of blocks for the current decoder stage.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        upsample_stride = upsample_kernel_size\n",
        "        self.transp_conv = get_conv_layer(\n",
        "            spatial_dims,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=upsample_kernel_size,\n",
        "            stride=upsample_stride,\n",
        "            conv_only=True,\n",
        "            is_transposed=True,\n",
        "        )\n",
        "\n",
        "        # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        self.decoder_block = nn.ModuleList()\n",
        "        self.conv_decoder = conv_decoder\n",
        "        # If this is the last decoder, use ConvBlock(UnetResBlock) instead of EPA_Block (see suppl. material in the paper)\n",
        "        if conv_decoder == True:\n",
        "            self.decoder_block.append(\n",
        "                UnetResBlock(spatial_dims, out_channels, out_channels, kernel_size=kernel_size, stride=1,\n",
        "                             norm_name=norm_name, ))\n",
        "        else:\n",
        "            stage_blocks = []\n",
        "            for j in range(depth):\n",
        "                stage_blocks.append(TransformerBlock(spatial_dims, input_size=out_size, hidden_size= out_channels, proj_size=proj_size, num_heads=num_heads,\n",
        "                                                     dropout_rate=0.15, pos_embed=True))\n",
        "            self.decoder_block.append(nn.Sequential(*stage_blocks))\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, (nn.LayerNorm)):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, inp, skip):\n",
        "\n",
        "        out = self.transp_conv(inp)\n",
        "        # if self.conv_decoder ==True:\n",
        "        #     out = torch.cat([out,skip], dim=1)\n",
        "        # else:\n",
        "        out = out + skip\n",
        "        out = self.decoder_block[0](out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "LHPewwvk2IoD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##net"
      ],
      "metadata": {
        "id": "SA5ho5GQ47JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####retinal_vasuclar_net"
      ],
      "metadata": {
        "id": "SBbwW_BQOrLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import Tuple, Union\n",
        "#from network_architecture.neural_network import SegmentationNetwork\n",
        "#from network_architecture.dynunet_block import UnetOutBlock, UnetResBlock\n",
        "#from network_architecture.synapse.model_components import UnetrPPEncoder, UnetrUpBlock\n",
        "\n",
        "\n",
        "class RetinalVasularSeg(SegmentationNetwork):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            img_size: [640, 640],\n",
        "            feature_size: int = 16,\n",
        "            hidden_size: int = 256,\n",
        "            num_heads: int = 4,\n",
        "            pos_embed: str = \"perceptron\",  # TODO: Remove the argument\n",
        "            norm_name: Union[Tuple, str] = \"instance\",\n",
        "            dropout_rate: float = 0.0,\n",
        "            depths=None,\n",
        "            dims=None,\n",
        "            conv_op=nn.Conv3d,\n",
        "            do_ds=True,\n",
        "\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if depths is None:\n",
        "            depths = [3, 3, 3, 3]\n",
        "        self.do_ds = do_ds\n",
        "        self.conv_op = conv_op\n",
        "        self.num_classes = out_channels\n",
        "        if not (0 <= dropout_rate <= 1):\n",
        "            raise AssertionError(\"dropout_rate should be between 0 and 1.\")\n",
        "\n",
        "        if pos_embed not in [\"conv\", \"perceptron\"]:\n",
        "            raise KeyError(f\"Position embedding layer of type {pos_embed} is not supported.\")\n",
        "\n",
        "        self.patch_size = (3, 3)\n",
        "        self.feat_size = (\n",
        "            img_size[0] // self.patch_size[0] // 8,  # 8 is the downsampling happened through the four encoders stages\n",
        "            img_size[1] // self.patch_size[1] // 8,  # 8 is the downsampling happened through the four encoders stages\n",
        "        )\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = [self.feat_size[0]*8*self.feat_size[1]*8, self.feat_size[0]*4*self.feat_size[1]*4,\n",
        "                           self.feat_size[0]*2*self.feat_size[1]*2, self.feat_size[0]*1*self.feat_size[1]*1]\n",
        "        self.unetr_pp_encoder = UnetrPPEncoder(input_size=self.input_size, patch_size=self.patch_size, dims=dims, depths=depths, num_heads=num_heads, in_channels=in_channels)\n",
        "\n",
        "        self.prarm = nn.Parameter(torch.tensor([0.2, 0.6, 0.2]),requires_grad=True).cuda().float()\n",
        "        norm_name ='batch'\n",
        "\n",
        "        self.encoder1 = UnetResBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=feature_size*2,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "        )\n",
        "        self.encoder11 = UnetResBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size*2,\n",
        "            out_channels=feature_size,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "        )\n",
        "        self.decoder5 = UnetrUpBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size * 16,\n",
        "            out_channels=feature_size * 8,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            out_size=img_size[0]//(self.patch_size[0]*4) * img_size[1]//(self.patch_size[1]*4),\n",
        "        )\n",
        "        self.decoder4 = UnetrUpBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size * 8,\n",
        "            out_channels=feature_size * 4,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            out_size=img_size[0]//(self.patch_size[0]*2) * img_size[1]//(self.patch_size[1]*2),\n",
        "        )\n",
        "        self.decoder3 = UnetrUpBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size * 4,\n",
        "            out_channels=feature_size * 2,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            out_size=img_size[0]//self.patch_size[0] * img_size[1]//self.patch_size[1],\n",
        "        )\n",
        "        self.decoder2 = UnetrUpBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size * 2,\n",
        "            out_channels=feature_size,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=self.patch_size,\n",
        "            norm_name=norm_name,\n",
        "            out_size=img_size[0] * img_size[1],\n",
        "            conv_decoder=True,\n",
        "        )\n",
        "        self.out1 = UnetOutBlock(spatial_dims=2, in_channels=feature_size, out_channels=out_channels)\n",
        "        if self.do_ds:\n",
        "            self.out2 = UnetOutBlock(spatial_dims=2, in_channels=feature_size * 2, out_channels=out_channels)\n",
        "            self.out3 = UnetOutBlock(spatial_dims=2, in_channels=feature_size * 4, out_channels=out_channels)\n",
        "\n",
        "    def proj_feat(self, x, hidden_size, feat_size):\n",
        "        x = x.view(x.size(0), feat_size[0], feat_size[1], hidden_size)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        return x\n",
        "\n",
        "    def forward(self, x_in):\n",
        "\n",
        "        x_output, hidden_states = self.unetr_pp_encoder(x_in)\n",
        "\n",
        "        convBlock = self.encoder1(x_in)\n",
        "        convBlock = self.encoder11(convBlock)\n",
        "\n",
        "\n",
        "        # Four encoders\n",
        "        enc1 = hidden_states[0]\n",
        "        enc2 = hidden_states[1]\n",
        "        enc3 = hidden_states[2]\n",
        "        enc4 = hidden_states[3]\n",
        "\n",
        "        # Four decoders\n",
        "        dec4 = self.proj_feat(enc4, self.hidden_size, self.feat_size)\n",
        "        dec3 = self.decoder5(dec4, enc3)\n",
        "        dec2 = self.decoder4(dec3, enc2)\n",
        "        dec1 = self.decoder3(dec2, enc1)\n",
        "\n",
        "        out = self.decoder2(dec1, convBlock)\n",
        "        if self.do_ds:\n",
        "            logits = [self.out1(out), self.out2(dec1), self.out3(dec2)]\n",
        "        else:\n",
        "            logits = self.out1(out)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "LFBS3svoFm5x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####pp_lite_seg"
      ],
      "metadata": {
        "id": "iJhgX7FOO_cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class ConvBN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 bias = False,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self._conv = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size//2 if padding else 0,\n",
        "            bias = bias, **kwargs)\n",
        "        self._batch_norm = nn.BatchNorm2d(out_channels, momentum=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._conv(x)\n",
        "        x = self._batch_norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=3,\n",
        "                 stride = 1,\n",
        "                 padding=1,\n",
        "                 bias = False,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self._conv = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size//2 if padding else 0, bias = bias,**kwargs)\n",
        "\n",
        "        self._batch_norm = nn.BatchNorm2d(out_channels, momentum=0.1)\n",
        "        self._relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._conv(x)\n",
        "        x = self._batch_norm(x)\n",
        "        x = self._relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBNRelu(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=3,\n",
        "                 stride = 1,\n",
        "                 padding=1,\n",
        "                 bias = False,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size//2 if padding else 0, bias = bias, **kwargs)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels, momentum=0.1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def avg_max_reduce_channel_helper(x, use_concat=True):\n",
        "    # Reduce hw by avg and max, only support single input\n",
        "    assert not isinstance(x, (list, tuple))\n",
        "    # print(\"x before mean and max:\", x.shape)\n",
        "    mean_value = torch.mean(x, dim=1, keepdim=True)\n",
        "    max_value = torch.max(x, dim=1, keepdim=True)[0]\n",
        "    # mean_value = mean_value.unsqueeze(0)\n",
        "    # print(\"mean max:\", mean_value.shape, max_value.shape)\n",
        "\n",
        "    if use_concat:\n",
        "        res = torch.at([mean_value, max_value], dim=1)\n",
        "    else:\n",
        "        res = [mean_value, max_value]\n",
        "    return res\n",
        "\n",
        "\n",
        "def avg_max_reduce_channel(x):\n",
        "    # Reduce hw by avg and max\n",
        "    # Return cat([avg_ch_0, max_ch_0, avg_ch_1, max_ch_1, ...])\n",
        "    if not isinstance(x, (list, tuple)):\n",
        "        return avg_max_reduce_channel_helper(x)\n",
        "    elif len(x) == 1:\n",
        "        return avg_max_reduce_channel_helper(x[0])\n",
        "    else:\n",
        "        res = []\n",
        "        for xi in x:\n",
        "            # print(xi.shape)\n",
        "            res.extend(avg_max_reduce_channel_helper(xi, False))\n",
        "        # print(\"res:\\n\",)\n",
        "        # for it in res:\n",
        "        #     print(it.shape)\n",
        "        return torch.cat(res, dim=1)\n",
        "\n",
        "\n",
        "class UAFM(nn.Module):\n",
        "    \"\"\"\n",
        "    The base of Unified Attention Fusion Module.\n",
        "    Args:\n",
        "        x_ch (int): The channel of x tensor, which is the low level feature.\n",
        "        y_ch (int): The channel of y tensor, which is the high level feature.\n",
        "        out_ch (int): The channel of output tensor.\n",
        "        ksize (int, optional): The kernel size of the conv for x tensor. Default: 3.\n",
        "        resize_mode (str, optional): The resize model in unsampling y tensor. Default: bilinear.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_ch, y_ch, out_ch, ukernel_size=2, ksize=3, resize_mode='nearest'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_x = ConvBNReLU(\n",
        "            x_ch, y_ch, kernel_size=ksize, padding=ksize // 2, bias=False)\n",
        "        self.conv_out = ConvBNReLU(\n",
        "            y_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
        "        self.resize_mode = resize_mode\n",
        "        self.transp = nn.ConvTranspose2d(y_ch, y_ch, kernel_size=ukernel_size, stride=ukernel_size, bias=False)\n",
        "\n",
        "    def check(self, x, y):\n",
        "        # print(\"x dim:\",x.ndim)\n",
        "        assert x.ndim == 4 and y.ndim == 4\n",
        "        x_h, x_w = x.shape[2:]\n",
        "        y_h, y_w = y.shape[2:]\n",
        "        assert x_h >= y_h and x_w >= y_w\n",
        "\n",
        "    def prepare(self, x, y):\n",
        "        x = self.prepare_x(x, y)\n",
        "        y = self.prepare_y(x, y)\n",
        "        return x, y\n",
        "\n",
        "    def prepare_x(self, x, y):\n",
        "        x = self.conv_x(x)\n",
        "        return x\n",
        "\n",
        "    def prepare_y(self, x, y):\n",
        "        y_up = self.transp(y)#F.interpolate(y, x.shape[2:], mode=self.resize_mode)\n",
        "        return y_up\n",
        "\n",
        "    def fuse(self, x, y):\n",
        "        out = x + y\n",
        "        out = self.conv_out(out)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): The low level feature.\n",
        "            y (Tensor): The high level feature.\n",
        "        \"\"\"\n",
        "        # print(\"x,y shape:\",x.shape, y.shape)\n",
        "        self.check(x, y)\n",
        "        x, y = self.prepare(x, y)\n",
        "        out = self.fuse(x, y)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UAFM_SpAtten(UAFM):\n",
        "    \"\"\"\n",
        "    The UAFM with spatial attention, which uses mean and max values.\n",
        "    Args:\n",
        "        x_ch (int): The channel of x tensor, which is the low level feature.\n",
        "        y_ch (int): The channel of y tensor, which is the high level feature.\n",
        "        out_ch (int): The channel of output tensor.\n",
        "        ksize (int, optional): The kernel size of the conv for x tensor. Default: 3.\n",
        "        resize_mode (str, optional): The resize model in unsampling y tensor. Default: bilinear.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_ch, y_ch, out_ch, ukernel_size=2, ksize=3, resize_mode='bilinear'):\n",
        "        super().__init__(x_ch, y_ch, out_ch, ukernel_size, ksize, resize_mode)\n",
        "\n",
        "        self.conv_xy_atten = nn.Sequential(\n",
        "            ConvBNReLU(\n",
        "                4, 2, kernel_size=3, padding=1, bias=False),\n",
        "            ConvBN(\n",
        "                2, 1, kernel_size=3, padding=1, bias=False))\n",
        "\n",
        "    def fuse(self, x, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): The low level feature.\n",
        "            y (Tensor): The high level feature.\n",
        "        \"\"\"\n",
        "        # print(\"x, y shape:\",x.shape, y.shape)\n",
        "        atten = avg_max_reduce_channel([x, y])\n",
        "        atten = F.sigmoid(self.conv_xy_atten(atten))\n",
        "\n",
        "        out = x * atten + y * (1 - atten)\n",
        "        # out = self.conv_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CatBottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
        "        super(CatBottleneck, self).__init__()\n",
        "        assert block_num > 1, \"block number should be larger than 1.\"\n",
        "        self.conv_list = nn.ModuleList()\n",
        "        self.stride = stride\n",
        "        if stride == 2:\n",
        "            self.avd_layer = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    out_planes // 2,\n",
        "                    out_planes // 2,\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    groups=out_planes // 2,\n",
        "                    bias=False),\n",
        "                nn.BatchNorm2d(out_planes // 2, momentum=0.1), )\n",
        "            self.skip = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        for idx in range(block_num):\n",
        "            if idx == 0:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(\n",
        "                        in_planes, out_planes // 2, kernel_size=1))\n",
        "            elif idx == 1 and block_num == 2:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(\n",
        "                        out_planes // 2, out_planes // 2, stride=stride))\n",
        "            elif idx == 1 and block_num > 2:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(\n",
        "                        out_planes // 2, out_planes // 4, stride=stride))\n",
        "            elif idx < block_num - 1:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(out_planes // int(math.pow(2, idx)), out_planes\n",
        "                               // int(math.pow(2, idx + 1))))\n",
        "            else:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(out_planes // int(math.pow(2, idx)), out_planes\n",
        "                               // int(math.pow(2, idx))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_list = []\n",
        "        out1 = self.conv_list[0](x)\n",
        "        for idx, conv in enumerate(self.conv_list[1:]):\n",
        "            if idx == 0:\n",
        "                if self.stride == 2:\n",
        "                    out = conv(self.avd_layer(out1))\n",
        "                else:\n",
        "                    out = conv(out1)\n",
        "            else:\n",
        "                out = conv(out)\n",
        "            out_list.append(out)\n",
        "\n",
        "        if self.stride == 2:\n",
        "            out1 = self.skip(out1)\n",
        "        out_list.insert(0, out1)\n",
        "        out = torch.cat(out_list, dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class AddBottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
        "        super(AddBottleneck, self).__init__()\n",
        "        assert block_num > 1, \"block number should be larger than 1.\"\n",
        "        self.conv_list = nn.ModuleList()\n",
        "        self.stride = stride\n",
        "        if stride == 2:\n",
        "            self.avd_layer = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    out_planes // 2,\n",
        "                    out_planes // 2,\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    groups=out_planes // 2,\n",
        "                    bias=False),\n",
        "                nn.BatchNorm2D(out_planes // 2, momentum=0.1), )\n",
        "            self.skip = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_planes,\n",
        "                    in_planes,\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    groups=in_planes,\n",
        "                    bias_attr=False),\n",
        "                nn.BatchNorm2d(in_planes, momentum=0.1),\n",
        "                nn.Conv2d(\n",
        "                    in_planes, out_planes, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_planes, momentum=0.1), )\n",
        "            stride = 1\n",
        "\n",
        "        for idx in range(block_num):\n",
        "            if idx == 0:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(\n",
        "                        in_planes, out_planes // 2, kernel=1))\n",
        "            elif idx == 1 and block_num == 2:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(\n",
        "                        out_planes // 2, out_planes // 2, stride=stride))\n",
        "            elif idx == 1 and block_num > 2:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(\n",
        "                        out_planes // 2, out_planes // 4, stride=stride))\n",
        "            elif idx < block_num - 1:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(out_planes // int(math.pow(2, idx)), out_planes\n",
        "                               // int(math.pow(2, idx + 1))))\n",
        "            else:\n",
        "                self.conv_list.append(\n",
        "                    ConvBNRelu(out_planes // int(math.pow(2, idx)), out_planes\n",
        "                               // int(math.pow(2, idx))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_list = []\n",
        "        out = x\n",
        "        for idx, conv in enumerate(self.conv_list):\n",
        "            if idx == 0 and self.stride == 2:\n",
        "                out = self.avd_layer(conv(out))\n",
        "            else:\n",
        "                out = conv(out)\n",
        "            out_list.append(out)\n",
        "        if self.stride == 2:\n",
        "            x = self.skip(x)\n",
        "        return torch.cat(out_list, dim=1) + x\n",
        "\n",
        "\n",
        "class STDCNet(nn.Module):\n",
        "    \"\"\"\n",
        "    The STDCNet implementation based on Pytorch.\n",
        "\n",
        "    The original article refers to Meituan\n",
        "    Fan, Mingyuan, et al. \"Rethinking BiSeNet For Real-time Semantic Segmentation.\"\n",
        "    (https://arxiv.org/abs/2104.13188)\n",
        "\n",
        "    Args:\n",
        "        base(int, optional): base channels. Default: 64.\n",
        "        layers(list, optional): layers numbers list. It determines STDC block numbers of STDCNet's stage3\\4\\5. Defualt: [4, 5, 3].\n",
        "        block_num(int,optional): block_num of features block. Default: 4.\n",
        "        type(str,optional): feature fusion method \"cat\"/\"add\". Default: \"cat\".\n",
        "        num_classes(int, optional): class number for image classification. Default: 1000.\n",
        "        dropout(float,optional): dropout ratio. if >0,use dropout ratio.  Default: 0.20.\n",
        "        use_conv_last(bool,optional): whether to use the last ConvBNReLU layer . Default: False.\n",
        "        pretrained(str, optional): the path of pretrained model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 base=64,\n",
        "                 layers=[4, 5, 3],\n",
        "                 block_num=4,\n",
        "                 type=\"cat\",\n",
        "                 num_classes=1000,\n",
        "                 dropout=0.20,\n",
        "                 use_conv_last=False,\n",
        "                 pretrained=None):\n",
        "        super(STDCNet, self).__init__()\n",
        "        if type == \"cat\":\n",
        "            block = CatBottleneck\n",
        "        elif type == \"add\":\n",
        "            block = AddBottleneck\n",
        "        self.use_conv_last = use_conv_last\n",
        "        self.feat_channels = [base // 2, base, base * 4, base * 8, base * 16]\n",
        "        self.features = self._make_layers(base, layers, block_num, block)\n",
        "        self.conv_last = ConvBNRelu(base * 16, max(1024, base * 16), 1, 1)\n",
        "\n",
        "        if (layers == [4, 5, 3]):  # stdc1446\n",
        "            self.x2 = nn.Sequential(self.features[:1])\n",
        "            self.x4 = nn.Sequential(self.features[1:2])\n",
        "            self.x8 = nn.Sequential(self.features[2:6])\n",
        "            self.x16 = nn.Sequential(self.features[6:11])\n",
        "            self.x32 = nn.Sequential(self.features[11:])\n",
        "        elif (layers == [2, 2, 2]):  # stdc813\n",
        "            self.x2 = nn.Sequential(self.features[:1])\n",
        "            self.x4 = nn.Sequential(self.features[1:2])\n",
        "            self.x8 = nn.Sequential(self.features[2:4])\n",
        "            self.x16 = nn.Sequential(self.features[4:6])\n",
        "            self.x32 = nn.Sequential(self.features[6:])\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"model with layers:{} is not implemented!\".format(layers))\n",
        "\n",
        "        self.pretrained = pretrained\n",
        "        # self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function for feature extract.\n",
        "        \"\"\"\n",
        "        feat2 = self.x2(x)\n",
        "        feat4 = self.x4(feat2)\n",
        "        feat8 = self.x8(feat4)\n",
        "        feat16 = self.x16(feat8)\n",
        "        feat32 = self.x32(feat16)\n",
        "        if self.use_conv_last:\n",
        "            feat32 = self.conv_last(feat32)\n",
        "        return feat2, feat4, feat8, feat16, feat32\n",
        "\n",
        "    def _make_layers(self, base, layers, block_num, block):\n",
        "        features = []\n",
        "        features += [ConvBNRelu(3, base // 2, 3, 2)]\n",
        "        features += [ConvBNRelu(base // 2, base, 3, 2)]\n",
        "\n",
        "        for i, layer in enumerate(layers):\n",
        "            for j in range(layer):\n",
        "                if i == 0 and j == 0:\n",
        "                    features.append(block(base, base * 4, block_num, 2))\n",
        "                elif j == 0:\n",
        "                    features.append(\n",
        "                        block(base * int(math.pow(2, i + 1)), base * int(\n",
        "                            math.pow(2, i + 2)), block_num, 2))\n",
        "                else:\n",
        "                    features.append(\n",
        "                        block(base * int(math.pow(2, i + 2)), base * int(\n",
        "                            math.pow(2, i + 2)), block_num, 1))\n",
        "\n",
        "        return nn.Sequential(*features)\n",
        "\n",
        "    # def init_weight(self):\n",
        "    #     for layer in self.sublayers():\n",
        "    #         if isinstance(layer, nn.Conv2D):\n",
        "    #             param_init.normal_init(layer.weight, std=0.001)\n",
        "    #         elif isinstance(layer, (nn.BatchNorm, nn.SyncBatchNorm)):\n",
        "    #             param_init.constant_init(layer.weight, value=1.0)\n",
        "    #             param_init.constant_init(layer.bias, value=0.0)\n",
        "    #     if self.pretrained is not None:\n",
        "    #         utils.load_pretrained_model(self, self.pretrained)\n",
        "\n",
        "\n",
        "def STDC2(**kwargs):\n",
        "    model = STDCNet(base=64, layers=[4, 5, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "class PPLiteSeg(nn.Module):\n",
        "    \"\"\"\n",
        "    The PP_LiteSeg implementation based on Pytorch.\n",
        "\n",
        "    The original article refers to \"Juncai Peng, Yi Liu, Shiyu Tang, Yuying Hao, Lutao Chu,\n",
        "    Guowei Chen, Zewu Wu, Zeyu Chen, Zhiliang Yu, Yuning Du, Qingqing Dang,Baohua Lai,\n",
        "    Qiwen Liu, Xiaoguang Hu, Dianhai Yu, Yanjun Ma. PP-LiteSeg: A Superior Real-Time Semantic\n",
        "    Segmentation Model. https://arxiv.org/abs/2204.02681\".\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): The number of target classes.\n",
        "        backbone(nn.Layer): Backbone network, such as stdc1net and resnet18. The backbone must\n",
        "            has feat_channels, of which the length is 5.\n",
        "        backbone_indices (List(int), optional): The values indicate the indices of output of backbone.\n",
        "            Default: [2, 3, 4].\n",
        "        arm_type (str, optional): The type of attention refinement module. Default: ARM_Add_SpAttenAdd3.\n",
        "        cm_bin_sizes (List(int), optional): The bin size of context module. Default: [1,2,4].\n",
        "        cm_out_ch (int, optional): The output channel of the last context module. Default: 128.\n",
        "        arm_out_chs (List(int), optional): The out channels of each arm module. Default: [64, 96, 128].\n",
        "        seg_head_inter_chs (List(int), optional): The intermediate channels of segmentation head.\n",
        "            Default: [64, 64, 64].\n",
        "        resize_mode (str, optional): The resize mode for the upsampling operation in decoder.\n",
        "            Default: bilinear.\n",
        "        pretrained (str, optional): The path or url of pretrained model. Default: None.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes = 19,\n",
        "                 backbone = STDC2(),\n",
        "                 backbone_indices=[2, 3, 4],\n",
        "                 arm_type='UAFM_SpAtten',\n",
        "                 cm_bin_sizes=[1, 2, 4],\n",
        "                 cm_out_ch=128,\n",
        "                 arm_out_chs=[64, 96, 128],\n",
        "                 seg_head_inter_chs=[64, 64, 64],\n",
        "                 resize_mode='nearest',\n",
        "                 pretrained=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # backbone\n",
        "        assert hasattr(backbone, 'feat_channels'), \\\n",
        "            \"The backbone should has feat_channels.\"\n",
        "        assert len(backbone.feat_channels) >= len(backbone_indices), \\\n",
        "            f\"The length of input backbone_indices ({len(backbone_indices)}) should not be\" \\\n",
        "            f\"greater than the length of feat_channels ({len(backbone.feat_channels)}).\"\n",
        "        assert len(backbone.feat_channels) > max(backbone_indices), \\\n",
        "            f\"The max value ({max(backbone_indices)}) of backbone_indices should be \" \\\n",
        "            f\"less than the length of feat_channels ({len(backbone.feat_channels)}).\"\n",
        "        self.backbone = backbone\n",
        "\n",
        "        assert len(backbone_indices) > 1, \"The lenght of backbone_indices \" \\\n",
        "                                          \"should be greater than 1\"\n",
        "        self.backbone_indices = backbone_indices  # [..., x16_id, x32_id]\n",
        "        backbone_out_chs = [backbone.feat_channels[i] for i in backbone_indices]\n",
        "\n",
        "        # head\n",
        "        if len(arm_out_chs) == 1:\n",
        "            arm_out_chs = arm_out_chs * len(backbone_indices)\n",
        "        assert len(arm_out_chs) == len(backbone_indices), \"The length of \" \\\n",
        "                                                          \"arm_out_chs and backbone_indices should be equal\"\n",
        "\n",
        "        self.ppseg_head = PPLiteSegHead(backbone_out_chs, arm_out_chs,\n",
        "                                        cm_bin_sizes, cm_out_ch, arm_type,\n",
        "                                        resize_mode)\n",
        "\n",
        "        if len(seg_head_inter_chs) == 1:\n",
        "            seg_head_inter_chs = seg_head_inter_chs * len(backbone_indices)\n",
        "        assert len(seg_head_inter_chs) == len(backbone_indices), \"The length of \" \\\n",
        "                                                                 \"seg_head_inter_chs and backbone_indices should be equal\"\n",
        "        self.seg_heads = nn.ModuleList()  # [..., head_16, head32]\n",
        "        print(\"arm_out_chs:\",arm_out_chs, \" ; seg_head_inter_chs:\",seg_head_inter_chs)\n",
        "        for in_ch, mid_ch in zip(arm_out_chs, seg_head_inter_chs):\n",
        "            self.seg_heads.append(SegHead(in_ch, mid_ch, num_classes))\n",
        "\n",
        "        # pretrained\n",
        "        self.pretrained = pretrained\n",
        "        # self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_hw = x.shape[2:]\n",
        "        # print(\"x_hw:\",x_hw)\n",
        "\n",
        "        feats_backbone = self.backbone(x)  # [x2, x4, x8, x16, x32]\n",
        "        # print(type(feats_backbone))\n",
        "        assert len(feats_backbone) >= len(self.backbone_indices), \\\n",
        "            f\"The nums of backbone feats ({len(feats_backbone)}) should be greater or \" \\\n",
        "            f\"equal than the nums of backbone_indices ({len(self.backbone_indices)})\"\n",
        "\n",
        "        feats_selected = [feats_backbone[i] for i in self.backbone_indices]\n",
        "\n",
        "        feats_head = self.ppseg_head(feats_selected)  # [..., x8, x16, x32]\n",
        "\n",
        "        if self.training:\n",
        "            logit_list = []\n",
        "\n",
        "            for x, seg_head in zip(feats_head, self.seg_heads):\n",
        "                x = seg_head(x)\n",
        "                logit_list.append(x)\n",
        "\n",
        "            logit_list = [\n",
        "                F.interpolate(\n",
        "                    x, x_hw, mode='bilinear', align_corners=None)\n",
        "                for x in logit_list\n",
        "            ]\n",
        "        else:\n",
        "            x = self.seg_heads[0](feats_head[0])\n",
        "            # print(\"x:\",x.shape)\n",
        "            x = F.interpolate(x, x_hw, mode='bilinear', align_corners=None)\n",
        "            logit_list = [x]\n",
        "\n",
        "        return logit_list\n",
        "\n",
        "    # def init_weight(self):\n",
        "    #     if self.pretrained is not None:\n",
        "    #         utils.load_entire_model(self, self.pretrained)\n",
        "\n",
        "\n",
        "class PPLiteSegHead(nn.Module):\n",
        "    \"\"\"\n",
        "    The head of PPLiteSeg.\n",
        "\n",
        "    Args:\n",
        "        backbone_out_chs (List(Tensor)): The channels of output tensors in the backbone.\n",
        "        arm_out_chs (List(int)): The out channels of each arm module.\n",
        "        cm_bin_sizes (List(int)): The bin size of context module.\n",
        "        cm_out_ch (int): The output channel of the last context module.\n",
        "        arm_type (str): The type of attention refinement module.\n",
        "        resize_mode (str): The resize mode for the upsampling operation in decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, backbone_out_chs, arm_out_chs, cm_bin_sizes, cm_out_ch,\n",
        "                 arm_type, resize_mode):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cm = PPContextModule(backbone_out_chs[-1], cm_out_ch, cm_out_ch,\n",
        "                                  cm_bin_sizes)\n",
        "\n",
        "        # assert hasattr(layers, arm_type), \\\n",
        "        #     \"Not support arm_type ({})\".format(arm_type)\n",
        "        arm_class = eval(arm_type)\n",
        "\n",
        "        self.arm_list = nn.ModuleList()  # [..., arm8, arm16, arm32]\n",
        "        for i in range(len(backbone_out_chs)):\n",
        "            low_chs = backbone_out_chs[i]\n",
        "            high_ch = cm_out_ch if i == len(\n",
        "                backbone_out_chs) - 1 else arm_out_chs[i + 1]\n",
        "            out_ch = arm_out_chs[i]\n",
        "            arm = arm_class(\n",
        "                low_chs, high_ch, out_ch, ksize=3, resize_mode=resize_mode)\n",
        "            self.arm_list.append(arm)\n",
        "\n",
        "    def forward(self, in_feat_list):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_feat_list (List(Tensor)): Such as [x2, x4, x8, x16, x32].\n",
        "                x2, x4 and x8 are optional.\n",
        "        Returns:\n",
        "            out_feat_list (List(Tensor)): Such as [x2, x4, x8, x16, x32].\n",
        "                x2, x4 and x8 are optional.\n",
        "                The length of in_feat_list and out_feat_list are the same.\n",
        "        \"\"\"\n",
        "\n",
        "        high_feat = self.cm(in_feat_list[-1])\n",
        "        out_feat_list = []\n",
        "\n",
        "        for i in reversed(range(len(in_feat_list))):\n",
        "            low_feat = in_feat_list[i]\n",
        "            arm = self.arm_list[i]\n",
        "            high_feat = arm(low_feat, high_feat)\n",
        "            out_feat_list.insert(0, high_feat)\n",
        "\n",
        "        return out_feat_list\n",
        "\n",
        "\n",
        "class PPContextModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Context module.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): The number of input channels to pyramid pooling module.\n",
        "        inter_channels (int): The number of inter channels to pyramid pooling module.\n",
        "        out_channels (int): The number of output channels after pyramid pooling module.\n",
        "        bin_sizes (tuple, optional): The out size of pooled feature maps. Default: (1, 3).\n",
        "        align_corners (bool): An argument of F.interpolate. It should be set to False\n",
        "            when the output size of feature is even, e.g. 1024x512, otherwise it is True, e.g. 769x769.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 inter_channels,\n",
        "                 out_channels,\n",
        "                 bin_sizes,\n",
        "                 align_corners=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stages = nn.ModuleList([\n",
        "            self._make_stage(in_channels, inter_channels, size)\n",
        "            for size in bin_sizes\n",
        "        ])\n",
        "\n",
        "        self.conv_out = ConvBNReLU(\n",
        "            in_channels=inter_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            bias=True)\n",
        "\n",
        "        self.align_corners = align_corners\n",
        "\n",
        "    def _make_stage(self, in_channels, out_channels, size):\n",
        "        prior = nn.AdaptiveAvgPool2d(output_size=size)\n",
        "        conv = ConvBNReLU(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=True)\n",
        "        return nn.Sequential(prior, conv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = None\n",
        "        input_shape = input.shape[2:]\n",
        "\n",
        "        for stage in self.stages:\n",
        "            x = stage(input)\n",
        "            x = F.interpolate(\n",
        "                x,\n",
        "                input_shape,\n",
        "                mode='nearest',\n",
        "                align_corners=self.align_corners)\n",
        "            if out is None:\n",
        "                out = x\n",
        "            else:\n",
        "                out += x\n",
        "\n",
        "        out = self.conv_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SegHead(nn.Module):\n",
        "    def __init__(self, in_chan, mid_chan, n_classes):\n",
        "        super().__init__()\n",
        "        self.conv = ConvBNReLU(\n",
        "            in_chan,\n",
        "            mid_chan,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "        # print(\"=\"*100)\n",
        "        # print(\"out:\",mid_chan, \"n_classes:\",n_classes)\n",
        "        self.conv_out = nn.Conv2d(\n",
        "            mid_chan, n_classes, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "#\n",
        "# def get_seg_model(**kwargs):\n",
        "#     model = PPLiteSeg(pretrained=False)\n",
        "#     return model"
      ],
      "metadata": {
        "id": "qv82R-EY6Nwm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####pos_embdb"
      ],
      "metadata": {
        "id": "WcSltAs8RGCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "# --------------------------------------------------------\n",
        "# Position embedding utils\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2D sine-cosine position embedding\n",
        "# References:\n",
        "# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n",
        "# MoCo v3: https://github.com/facebookresearch/moco-v3\n",
        "# --------------------------------------------------------\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Interpolate position embeddings for high-resolution\n",
        "# References:\n",
        "# DeiT: https://github.com/facebookresearch/deit\n",
        "# --------------------------------------------------------\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed"
      ],
      "metadata": {
        "id": "s8L4BZZMn3UF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####UNetFamily"
      ],
      "metadata": {
        "id": "6qHgAG6vRVg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This part contains UNet series models,\n",
        "including UNet, R2UNet, Attention UNet, R2Attention UNet, DenseUNet\n",
        "\"\"\"\n",
        "from typing import Union, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from timm.models.vision_transformer import Block\n",
        "#from net.pp_lite_seg import  UAFM_SpAtten\n",
        "\n",
        "\n",
        "# ==========================Core Module================================\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class up_conv(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, scale_factor=2):\n",
        "        super(up_conv, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=scale_factor),\n",
        "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Recurrent_block(nn.Module):\n",
        "    def __init__(self, ch_out, t=2):\n",
        "        super(Recurrent_block, self).__init__()\n",
        "        self.t = t\n",
        "        self.ch_out = ch_out\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.t):\n",
        "\n",
        "            if i == 0:\n",
        "                x1 = self.conv(x)\n",
        "\n",
        "            x1 = self.conv(x + x1)\n",
        "        return x1\n",
        "\n",
        "\n",
        "class RRCNN_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, t=2):\n",
        "        super(RRCNN_block, self).__init__()\n",
        "        self.RCNN = nn.Sequential(\n",
        "            Recurrent_block(ch_out, t=t),\n",
        "            Recurrent_block(ch_out, t=t)\n",
        "        )\n",
        "        self.Conv_1x1 = nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.Conv_1x1(x)\n",
        "        x1 = self.RCNN(x)\n",
        "        return x + x1\n",
        "\n",
        "\n",
        "class single_conv(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(single_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention_block(nn.Module):  # attention Gate\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(Attention_block, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "\n",
        "        return x * psi\n",
        "\n",
        "# =====R2U增=====\n",
        "def autopad(k, p=None):\n",
        "    if p is None:\n",
        "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]\n",
        "    return p\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_channels: int,\n",
        "                 output_channels: int,\n",
        "                 kernel_size: Tuple[int, int],\n",
        "                 stride: Tuple[int, int],\n",
        "                 padding: Union[int, None] = None,\n",
        "                 activation: bool = True,\n",
        "                 **kwargs):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.activation = activation\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_channels,\n",
        "                              out_channels=self.output_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=stride,\n",
        "                              padding=autopad(k=kernel_size, p=padding), **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(num_features=output_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        if self.activation:\n",
        "            out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResPath(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_channels : int,\n",
        "                 output_channels: int,\n",
        "                 length: int,\n",
        "                 padding: Union[int, None] = None):\n",
        "        super(ResPath, self).__init__()\n",
        "        self.length = length\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.conv1 = ConvBlock(input_channels=self.input_channels,\n",
        "                               output_channels=self.output_channels,\n",
        "                               kernel_size=(1, 1),\n",
        "                               stride=(1, 1),\n",
        "                               padding=autopad(k=(1, 1), p=padding),\n",
        "                               activation=False)\n",
        "\n",
        "        self.conv2 = ConvBlock(input_channels=self.input_channels,\n",
        "                               output_channels=self.output_channels,\n",
        "                               kernel_size=(3, 3),\n",
        "                               stride=(1, 1),\n",
        "                               padding=autopad(k=(3, 3), p=padding))\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_features=self.conv2.output_channels)\n",
        "\n",
        "        self.module = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.length-1):\n",
        "            self.module.append(module=ConvBlock(input_channels=self.output_channels,\n",
        "                                                output_channels=self.output_channels,\n",
        "                                                kernel_size=(1, 1),\n",
        "                                                stride=(1, 1),\n",
        "                                                activation=False,\n",
        "                                                padding=autopad(k=(1, 1), p=padding)))\n",
        "            self.module.append(module=ConvBlock(input_channels=self.output_channels,\n",
        "                                                output_channels=self.output_channels,\n",
        "                                                kernel_size=(3, 3),\n",
        "                                                stride=(1, 1),\n",
        "                                                padding=autopad(k=(3, 3), p=padding)))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        shortcut = x\n",
        "        shortcut = self.conv1(shortcut)\n",
        "        out = self.conv2(x)\n",
        "        out = torch.add(shortcut, out)\n",
        "        out = F.relu(out)\n",
        "        out = self.bn(out)\n",
        "\n",
        "        for i in range(self.length-1):\n",
        "            shortcut = out\n",
        "            shortcut = self.module[i*2](shortcut)\n",
        "            out = self.module[i*2+1](out)\n",
        "            out = torch.add(shortcut, out)\n",
        "            out = F.relu(out)\n",
        "            out = self.bn(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# ==================================================================\n",
        "class U_Net(nn.Module):\n",
        "    def __init__(self, img_ch=3, output_ch=2, fea_channels=32):\n",
        "        super(U_Net, self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(ch_in=img_ch, ch_out=fea_channels)\n",
        "        self.Conv2 = conv_block(ch_in=fea_channels, ch_out=fea_channels*2)\n",
        "        self.Conv3 = conv_block(ch_in=fea_channels*2, ch_out=fea_channels*4)\n",
        "        self.Conv4 = conv_block(ch_in=fea_channels*4, ch_out=fea_channels*8)\n",
        "        self.Conv5 = conv_block(ch_in=fea_channels*8, ch_out=fea_channels*16)\n",
        "\n",
        "        self.Up5 = up_conv(ch_in=fea_channels*16, ch_out=fea_channels*8)\n",
        "        self.Up_conv5 = conv_block(ch_in=fea_channels*16, ch_out=fea_channels*8)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in=fea_channels*8, ch_out=fea_channels*4)\n",
        "        self.Up_conv4 = conv_block(ch_in=fea_channels*8, ch_out=fea_channels*4)\n",
        "\n",
        "        self.Up3 = up_conv(ch_in=fea_channels*4, ch_out=fea_channels*2)\n",
        "        self.Up_conv3 = conv_block(ch_in=fea_channels*4, ch_out=fea_channels*2)\n",
        "\n",
        "        self.Up2 = up_conv(ch_in=fea_channels*2, ch_out=fea_channels*1)\n",
        "        self.Up_conv2 = conv_block(ch_in=fea_channels*2, ch_out=fea_channels*1)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(fea_channels*1, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding path\n",
        "        x1 = self.Conv1(x)\n",
        "        # print('x1',x1.shape)\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.Conv2(x2)\n",
        "        # print('x2',x2.shape)\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.Conv3(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.Conv4(x4)\n",
        "        # print(x4.shape)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.Conv5(x5)\n",
        "\n",
        "        # decoding + concat path\n",
        "        d5 = self.Up5(x5)\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "\n",
        "        d5 = self.Up_conv5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "        # print(d4.shape)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "        d1 = F.softmax(d1,dim=1)  # mine\n",
        "\n",
        "        return [d1, d1,d1]\n",
        "\n",
        "from functools import partial\n",
        "#from net.pos_embdb import get_2d_sincos_pos_embed\n",
        "##########################################################################################\n",
        "class U_Nett(nn.Module):\n",
        "    def __init__(self, img_ch=3, output_ch=2, fea_channels=64):\n",
        "        super(U_Nett, self).__init__()\n",
        "\n",
        "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(ch_in=img_ch, ch_out=fea_channels)\n",
        "        self.Conv2 = conv_block(ch_in=fea_channels, ch_out=fea_channels*2)\n",
        "        self.Conv3 = conv_block(ch_in=fea_channels*2, ch_out=fea_channels*4)\n",
        "        self.Conv4 = conv_block(ch_in=fea_channels*4, ch_out=fea_channels*8)\n",
        "\n",
        "        self.sizem = 24\n",
        "        self.param = nn.Parameter(torch.zeros(1, self.sizem * self.sizem, 1),requires_grad=True)  # fixed sin-cos embedding\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.sizem*self.sizem, fea_channels*8), requires_grad=False)  # fixed sin-cos embedding\n",
        "        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
        "        self.transblock = nn.ModuleList([\n",
        "            Block(dim=fea_channels*8, num_heads=fea_channels*8//64, mlp_ratio=2, qkv_bias=True, norm_layer=norm_layer)\n",
        "            for i in range(4)])\n",
        "\n",
        "        # self.UaSp1 = UAFM_SpAtten(x_ch=fea_channels*4, y_ch=fea_channels*8, out_ch=fea_channels*4, ukernel_size=2, ksize=3)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in=fea_channels*8, ch_out=fea_channels*4)\n",
        "        self.Up_conv4 = conv_block(ch_in=fea_channels*8, ch_out=fea_channels*4)\n",
        "\n",
        "        # self.UaSp2 = UAFM_SpAtten(x_ch=fea_channels * 2, y_ch=fea_channels * 4, out_ch=fea_channels * 4, ukernel_size=2, ksize=3)\n",
        "        self.Up3 = up_conv(ch_in=fea_channels*4, ch_out=fea_channels*2)\n",
        "        self.Up_conv3 = conv_block(ch_in=fea_channels*4, ch_out=fea_channels*2)\n",
        "\n",
        "        # self.UaSp3 = UAFM_SpAtten(x_ch=fea_channels * 1, y_ch=fea_channels * 2, out_ch=fea_channels * 2, ukernel_size=3,ksize=3)\n",
        "\n",
        "        self.Up2 = up_conv(ch_in=fea_channels*2, ch_out=fea_channels*1, scale_factor=2)\n",
        "        self.Up_conv2 = conv_block(ch_in=fea_channels*2, ch_out=fea_channels*1)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(fea_channels*1, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "        self.initialize_weights()\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.sizem), cls_token=False)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        # encoding path\n",
        "        x1 = self.Conv1(x)\n",
        "        # print('x1',x1.shape)\n",
        "        x2 = self.Maxpool1(x1)\n",
        "        x2 = self.Conv2(x2)\n",
        "        # print('x2',x2.shape)\n",
        "        x3 = self.Maxpool2(x2)\n",
        "        x3 = self.Conv3(x3)\n",
        "\n",
        "        x4 = self.Maxpool3(x3)\n",
        "        x4 = self.Conv4(x4)\n",
        "\n",
        "        b, c, h, w = x4.shape\n",
        "        x4 = x4.view(b, c, h* w).permute(0, 2, 1)\n",
        "        xc = x4+ self.pos_embed\n",
        "        for cpb in self.transblock:\n",
        "            xc = cpb(xc)\n",
        "        x4 = (self.param*xc + x4).permute(0, 2, 1).view(b, c, h, w)\n",
        "\n",
        "\n",
        "        #d4 = self.UaSp1(x3, x4)\n",
        "        d4 = self.Up4(x4)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        #d3 = self.UaSp2(x2, d4)\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        #d2 = self.UaSp3(x1, d3)\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "        #d1 = F.softmax(d1,dim=1)  # mine\n",
        "\n",
        "        return [d1, d1,d1]\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     net = U_Net(1,2)\n",
        "#     in1 = torch.randn(1,1,64,64)\n",
        "#     out1 = net(in1)\n",
        "#     print(out1.shape)\n",
        "\n",
        "# # 计算参数量\n",
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     net = U_Net(1,2)\n",
        "#     in1 = torch.randn(1,1,64,64)\n",
        "#     out1 = net(in1)\n",
        "#     # print(net)\n",
        "#     print(\"Total number of parameters: \" + str(count_parameters(net)))\n",
        "#\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     net = U_Net(1,2)\n",
        "#     in1 = torch.randn(1,1,64,64)\n",
        "#\n",
        "# if __name__ == '__main__':\n",
        "#     import time\n",
        "#     batch_size = 1\n",
        "#     # batch = torch.zeros([batch_size, 1, 80, 80], dtype=torch.float32)\n",
        "#     batch = torch.randn(1,1,64,64)\n",
        "#     model = net\n",
        "#     print(\"Total params: {0:,}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
        "#     print('Forward pass (bs={:d}) when running in the cpu:'.format(batch_size))\n",
        "#     start_time = time.time()\n",
        "#     logits = model(batch)\n",
        "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "#\n",
        "#     # 计算FLOPs & Params\n",
        "#     from models.util import CalParams\n",
        "#     t = CalParams(net.cuda(), torch.rand(1,1,64,64).cuda())\n",
        "#     print(t)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "class R2U_Net(nn.Module):\n",
        "    def __init__(self, img_ch=1, output_ch=2, t=2):\n",
        "        super(R2U_Net, self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Upsample = nn.Upsample(scale_factor=2)\n",
        "\n",
        "        self.RRCNN1 = RRCNN_block(ch_in=img_ch, ch_out=64, t=t)\n",
        "        self.ResPath1 = ResPath(input_channels=img_ch, output_channels=64,length=5)\n",
        "\n",
        "        self.RRCNN2 = RRCNN_block(ch_in=64, ch_out=128, t=t)\n",
        "        self.ResPath2 = ResPath(input_channels=64,output_channels=128,length=4)\n",
        "\n",
        "        self.RRCNN3 = RRCNN_block(ch_in=128, ch_out=256, t=t)\n",
        "        self.ResPath3 = ResPath(input_channels=128, output_channels=256,length=3)\n",
        "\n",
        "        self.RRCNN4 = RRCNN_block(ch_in=256, ch_out=512, t=t)\n",
        "        self.ResPath4 = ResPath(input_channels=256, output_channels=512,length=2)\n",
        "\n",
        "        self.RRCNN5 = RRCNN_block(ch_in=512, ch_out=1024, t=t)\n",
        "        self.ResPath5 = ResPath(input_channels=512,output_channels=1024,length=1)\n",
        "\n",
        "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
        "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512, t=t)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
        "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256, t=t)\n",
        "\n",
        "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
        "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128, t=t)\n",
        "\n",
        "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
        "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64, t=t)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding path\n",
        "        x1 = self.RRCNN1(x)\n",
        "        # x1_1 = self.ResPath1(x1)\n",
        "        #print(x1.shape)\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.RRCNN2(x2)\n",
        "        # x2_1 = self.ResPath2(x2)\n",
        "\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.RRCNN3(x3)\n",
        "        # x3_1 = self.ResPath3(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.RRCNN4(x4)\n",
        "        # x4_1 = self.ResPath4(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.RRCNN5(x5)\n",
        "        # x5_1 = self.ResPath5(x5)\n",
        "\n",
        "        # decoding + concat path\n",
        "        d5 = self.Up5(x5)\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "        # d5 = torch.cat((x4_1, d5), dim=1)\n",
        "        d5 = self.Up_RRCNN5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        # d4 = torch.cat((x3_1, d4), dim=1)\n",
        "        d4 = self.Up_RRCNN4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        # d3 = torch.cat((x2_1, d3), dim=1)\n",
        "        d3 = self.Up_RRCNN3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        # d2 = torch.cat((x1_1, d2), dim=1)\n",
        "        d2 = self.Up_RRCNN2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "        d1 = F.softmax(d1,dim=1)\n",
        "\n",
        "        return d1\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     net = R2U_Net(1,2)\n",
        "#     print(net)\n",
        "#     in1 = torch.randn((64,1,48,48)).cuda()\n",
        "#     out1 = net(in1)\n",
        "#     print(out1.size())\n",
        "\n",
        "# ===========================================================\n",
        "class AttU_Net(nn.Module):\n",
        "    def __init__(self, img_ch=3, output_ch=1):\n",
        "        super(AttU_Net, self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(ch_in=img_ch, ch_out=64)\n",
        "        self.Conv2 = conv_block(ch_in=64, ch_out=128)\n",
        "        self.Conv3 = conv_block(ch_in=128, ch_out=256)\n",
        "        self.Conv4 = conv_block(ch_in=256, ch_out=512)\n",
        "        self.Conv5 = conv_block(ch_in=512, ch_out=1024)\n",
        "\n",
        "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
        "        self.Att5 = Attention_block(F_g=512, F_l=512, F_int=256)\n",
        "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
        "        self.Att4 = Attention_block(F_g=256, F_l=256, F_int=128)\n",
        "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
        "\n",
        "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
        "        self.Att3 = Attention_block(F_g=128, F_l=128, F_int=64)\n",
        "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
        "\n",
        "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
        "        self.Att2 = Attention_block(F_g=64, F_l=64, F_int=32)\n",
        "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding path\n",
        "        x1 = self.Conv1(x)\n",
        "\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.Conv2(x2)\n",
        "\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.Conv3(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.Conv4(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.Conv5(x5)\n",
        "\n",
        "        # decoding + concat path\n",
        "        d5 = self.Up5(x5)\n",
        "        x4 = self.Att5(g=d5, x=x4)\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "        d5 = self.Up_conv5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        x3 = self.Att4(g=d4, x=x3)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        x2 = self.Att3(g=d3, x=x2)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        x1 = self.Att2(g=d2, x=x1)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "        d1 = F.softmax(d1,dim=1)\n",
        "        return d1\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     net = AttU_Net(1,2)\n",
        "#     in1 = torch.randn(1,1,64,64)\n",
        "#\n",
        "# if __name__ == '__main__':\n",
        "#     import time\n",
        "#     batch_size = 1\n",
        "#     # batch = torch.zeros([batch_size, 1, 80, 80], dtype=torch.float32)\n",
        "#     batch = torch.randn(1,1,64,64)\n",
        "#     model = net\n",
        "#     print(\"Total params: {0:,}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
        "#     print('Forward pass (bs={:d}) when running in the cpu:'.format(batch_size))\n",
        "#     start_time = time.time()\n",
        "#     logits = model(batch)\n",
        "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "#\n",
        "#     # 计算FLOPs & Params\n",
        "#     from models.util import CalParams\n",
        "#     t = CalParams(net.cuda(), torch.rand(1,1,64,64).cuda())\n",
        "#     print(t)\n",
        "\n",
        "# ==============================================================\n",
        "class R2AttU_Net(nn.Module):\n",
        "    def __init__(self, img_ch=3, output_ch=1, t=2):\n",
        "        super(R2AttU_Net, self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Upsample = nn.Upsample(scale_factor=2)\n",
        "\n",
        "        self.RRCNN1 = RRCNN_block(ch_in=img_ch, ch_out=64, t=t)\n",
        "\n",
        "        self.RRCNN2 = RRCNN_block(ch_in=64, ch_out=128, t=t)\n",
        "\n",
        "        self.RRCNN3 = RRCNN_block(ch_in=128, ch_out=256, t=t)\n",
        "\n",
        "        self.RRCNN4 = RRCNN_block(ch_in=256, ch_out=512, t=t)\n",
        "\n",
        "        self.RRCNN5 = RRCNN_block(ch_in=512, ch_out=1024, t=t)\n",
        "\n",
        "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
        "        self.Att5 = Attention_block(F_g=512, F_l=512, F_int=256)\n",
        "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512, t=t)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
        "        self.Att4 = Attention_block(F_g=256, F_l=256, F_int=128)\n",
        "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256, t=t)\n",
        "\n",
        "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
        "        self.Att3 = Attention_block(F_g=128, F_l=128, F_int=64)\n",
        "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128, t=t)\n",
        "\n",
        "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
        "        self.Att2 = Attention_block(F_g=64, F_l=64, F_int=32)\n",
        "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64, t=t)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding path\n",
        "        x1 = self.RRCNN1(x)\n",
        "\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.RRCNN2(x2)\n",
        "\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.RRCNN3(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.RRCNN4(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.RRCNN5(x5)\n",
        "\n",
        "        # decoding + concat path\n",
        "        d5 = self.Up5(x5)\n",
        "        x4 = self.Att5(g=d5, x=x4)\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "        d5 = self.Up_RRCNN5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        x3 = self.Att4(g=d4, x=x3)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        d4 = self.Up_RRCNN4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        x2 = self.Att3(g=d3, x=x2)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        d3 = self.Up_RRCNN3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        x1 = self.Att2(g=d2, x=x1)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        d2 = self.Up_RRCNN2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "        d1 = F.softmax(d1, dim=1)\n",
        "\n",
        "        return d1\n",
        "\n",
        "#==================DenseUNet=====================================\n",
        "class Single_level_densenet(nn.Module):\n",
        "    def __init__(self, filters, num_conv=4):\n",
        "        super(Single_level_densenet, self).__init__()\n",
        "        self.num_conv = num_conv\n",
        "        self.conv_list = nn.ModuleList()\n",
        "        self.bn_list = nn.ModuleList()\n",
        "        for i in range(self.num_conv):\n",
        "            self.conv_list.append(nn.Conv2d(filters, filters, 3, padding=1))\n",
        "            self.bn_list.append(nn.BatchNorm2d(filters))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        outs.append(x)\n",
        "        for i in range(self.num_conv):\n",
        "            temp_out = self.conv_list[i](outs[i])\n",
        "            if i > 0:\n",
        "                for j in range(i):\n",
        "                    temp_out += outs[j]\n",
        "            outs.append(F.relu(self.bn_list[i](temp_out)))\n",
        "        out_final = outs[-1]\n",
        "        del outs\n",
        "        return out_final\n",
        "\n",
        "\n",
        "class Down_sample(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        super(Down_sample, self).__init__()\n",
        "        self.down_sample_layer = nn.MaxPool2d(kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.down_sample_layer(x)\n",
        "        return y, x\n",
        "\n",
        "\n",
        "class Upsample_n_Concat(nn.Module):\n",
        "    def __init__(self, filters):\n",
        "        super(Upsample_n_Concat, self).__init__()\n",
        "        self.upsample_layer = nn.ConvTranspose2d(filters, filters, 4, padding=1, stride=2)\n",
        "        self.conv = nn.Conv2d(2 * filters, filters, 3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(filters)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = self.upsample_layer(x)\n",
        "        x = torch.cat([x, y], dim=1)\n",
        "        x = F.relu(self.bn(self.conv(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Dense_Unet(nn.Module):\n",
        "    def __init__(self, in_chan=3,out_chan=2,filters=128, num_conv=4):\n",
        "\n",
        "        super(Dense_Unet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_chan, filters, 1)\n",
        "        self.d1 = Single_level_densenet(filters, num_conv)\n",
        "        self.down1 = Down_sample()\n",
        "        self.d2 = Single_level_densenet(filters, num_conv)\n",
        "        self.down2 = Down_sample()\n",
        "        self.d3 = Single_level_densenet(filters, num_conv)\n",
        "        self.down3 = Down_sample()\n",
        "        self.d4 = Single_level_densenet(filters, num_conv)\n",
        "        self.down4 = Down_sample()\n",
        "        self.bottom = Single_level_densenet(filters, num_conv)\n",
        "        self.up4 = Upsample_n_Concat(filters)\n",
        "        self.u4 = Single_level_densenet(filters, num_conv)\n",
        "        self.up3 = Upsample_n_Concat(filters)\n",
        "        self.u3 = Single_level_densenet(filters, num_conv)\n",
        "        self.up2 = Upsample_n_Concat(filters)\n",
        "        self.u2 = Single_level_densenet(filters, num_conv)\n",
        "        self.up1 = Upsample_n_Concat(filters)\n",
        "        self.u1 = Single_level_densenet(filters, num_conv)\n",
        "        self.outconv = nn.Conv2d(filters, out_chan, 1)\n",
        "\n",
        "    #         self.outconvp1 = nn.Conv2d(filters,out_chan, 1)\n",
        "    #         self.outconvm1 = nn.Conv2d(filters,out_chan, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x, y1 = self.down1(self.d1(x))\n",
        "        x, y2 = self.down1(self.d2(x))\n",
        "        x, y3 = self.down1(self.d3(x))\n",
        "        x, y4 = self.down1(self.d4(x))\n",
        "        x = self.bottom(x)\n",
        "        x = self.u4(self.up4(x, y4))\n",
        "        x = self.u3(self.up3(x, y3))\n",
        "        x = self.u2(self.up2(x, y2))\n",
        "        x = self.u1(self.up1(x, y1))\n",
        "        x1 = self.outconv(x)\n",
        "        #         xm1 = self.outconvm1(x)\n",
        "        #         xp1 = self.outconvp1(x)\n",
        "        x1 = F.softmax(x1,dim=1)\n",
        "        return x1\n",
        "# =========================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    net = U_Net(img_ch=3, output_ch=1, fea_channels=64)\n",
        "\n",
        "\n",
        "    from thop import profile\n",
        "    input = torch.randn(1, 3, 480, 384).cuda().float()\n",
        "    macs, params = profile(net.cuda(), inputs=(input,))\n",
        "    print(macs, params)\n",
        "\n",
        "    #print(net)\n",
        "    in1 = torch.randn(4,3,224,224).cuda()\n",
        "    out = net(in1)\n",
        "    #print(out.size())\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # test network forward\n",
        "#     net = AttU_Net(1,2).cuda()\n",
        "#     print(net)\n",
        "#     in1 = torch.randn((4,1,48,48)).cuda()\n",
        "#     out1 = net(in1)\n",
        "#     print(out1.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkyNLugj5gQF",
        "outputId": "bb073b6c-44d2-4beb-845d-41ca1e06736a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
            "[INFO] Register count_upsample() for <class 'torch.nn.modules.upsampling.Upsample'>.\n",
            "184281661440.0 34527041.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####dual_local_global_attention"
      ],
      "metadata": {
        "id": "5fJTmhslRrSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "# class LocalAttention(nn.Module):\n",
        "#     def __init__(self, input_size, embed_size=10, win_size=5, out_channels=32):\n",
        "#         super(LocalAttention, self).__init__()\n",
        "#\n",
        "#         self.input_size = input_size\n",
        "#         self.embed_size = embed_size\n",
        "#         self.win_size = win_size\n",
        "#         self.out_channels = out_channels\n",
        "#\n",
        "#         self.attention_layer = nn.Sequential(\n",
        "#             nn.Conv2d(1, 1, kernel_size=(self.win_size, self.embed_size)),\n",
        "#             nn.Sigmoid())\n",
        "#\n",
        "#         self.cnn = nn.Sequential(\n",
        "#             nn.Conv2d(1, self.out_channels, kernel_size=(1, self.embed_size)),\n",
        "#             nn.Tanh(),\n",
        "#             nn.MaxPool2d((self.input_size, 1)))\n",
        "#\n",
        "#     def forward(self, x):\n",
        "#         padding = Variable(torch.zeros(x.size(0), (self.win_size - 1) // 2, self.embed_size))\n",
        "#         padding = padding.cuda()\n",
        "#         x_pad = torch.cat((padding, x, padding), 1)\n",
        "#\n",
        "#         x_pad = x_pad.unsqueeze(1)\n",
        "#         scores = self.attention_layer(x_pad)\n",
        "#\n",
        "#         scores = scores.squeeze(1)\n",
        "#\n",
        "#         out = torch.mul(x, scores)\n",
        "#\n",
        "#         out = out.unsqueeze(1)\n",
        "#         out = self.cnn(out)\n",
        "#\n",
        "#         return out\n",
        "#\n",
        "#\n",
        "# class GlobalAttention(nn.Module):\n",
        "#     def __init__(self, input_size, embed_size, out_channels):\n",
        "#         super(GlobalAttention, self).__init__()\n",
        "#\n",
        "#         self.input_size = input_size\n",
        "#         self.embed_size = embed_size\n",
        "#         self.out_channels = out_channels\n",
        "#\n",
        "#         self.attention_layer = nn.Sequential(\n",
        "#             nn.Conv2d(1, 1, kernel_size=(self.input_size, self.embed_size)),\n",
        "#             nn.Sigmoid())\n",
        "#\n",
        "#         self.cnn_1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, self.out_channels, kernel_size=(2, self.embed_size)),\n",
        "#             nn.Tanh(),\n",
        "#             nn.MaxPool2d((self.input_size - 2 + 1, 1)))\n",
        "#\n",
        "#         self.cnn_2 = nn.Sequential(\n",
        "#             nn.Conv2d(1, self.out_channels, kernel_size=(3, self.embed_size)),\n",
        "#             nn.Tanh(),\n",
        "#             nn.MaxPool2d((self.input_size - 3 + 1, 1)))\n",
        "#\n",
        "#         self.cnn_3 = nn.Sequential(\n",
        "#             nn.Conv2d(1, self.out_channels, kernel_size=(4, self.embed_size)),\n",
        "#             nn.Tanh(),\n",
        "#             nn.MaxPool2d((self.input_size - 4 + 1, 1)))\n",
        "#\n",
        "#     def forward(self, x):\n",
        "#         x = x.unsqueeze(1)\n",
        "#         score = self.attention_layer(x)\n",
        "#         out = torch.mul(x, score)\n",
        "#         out_1 = self.cnn_1(out)\n",
        "#         out_2 = self.cnn_2(out)\n",
        "#         out_3 = self.cnn_3(out)\n",
        "#         return (out_1, out_2, out_3)\n",
        "#\n",
        "#\n",
        "# class CNNDLGA(nn.Module):\n",
        "#\n",
        "#     def __init__(self, input_size, embed_size=100, win_size=5, channels_local=200, channels_global=100,\n",
        "#                  fc_input_size=500, hidden_size=500, output_size=50):\n",
        "#         super(CNNDLGA, self).__init__()\n",
        "#\n",
        "#         self.localAttentionLayer_user = LocalAttention(input_size, embed_size, win_size, channels_local)\n",
        "#         self.globalAttentionLayer_user = GlobalAttention(input_size, embed_size, channels_global)\n",
        "#         self.localAttentionLayer_item = LocalAttention(input_size, embed_size, win_size, channels_local)\n",
        "#         self.globalAttentionLayer_item = GlobalAttention(input_size, embed_size, channels_global)\n",
        "#         self.fcLayer = nn.Sequential(\n",
        "#             nn.Linear(fc_input_size, hidden_size),\n",
        "#             nn.Dropout(0.5),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(hidden_size, output_size),\n",
        "#         )\n",
        "#\n",
        "#     def forward(self, x_user, x_item):\n",
        "#         # user\n",
        "#         local_user = self.localAttentionLayer_user(x_user)\n",
        "#         global1_user, global2_user, global3_user = self.globalAttentionLayer_user(x_user)\n",
        "#         out_user = torch.cat((local_user, global1_user, global2_user, global3_user), 1)\n",
        "#         out_user = out_user.view(out_user.size(0), -1)\n",
        "#         out_user = self.fcLayer(out_user)\n",
        "#\n",
        "#         # item\n",
        "#         local_item = self.localAttentionLayer_item(x_item)\n",
        "#         global1_item, global2_item, global3_item = self.globalAttentionLayer_item(x_item)\n",
        "#         out_item = torch.cat((local_item, global1_item, global2_item, global3_item), 1)\n",
        "#         out_item = out_item.view(out_item.size(0), -1)\n",
        "#         out_item = self.fcLayer(out_item)\n",
        "#\n",
        "#         out = torch.sum(torch.mul(out_user, out_item), 1)\n",
        "#\n",
        "#         return out\n",
        "\n",
        "\n",
        "class Patch_AttentionV2(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=2, pool_window=5, add_input=False):\n",
        "        super(Patch_AttentionV2, self).__init__()\n",
        "        self.pool_window = pool_window\n",
        "        self.add_input = add_input\n",
        "        self.SA = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=pool_window + 1, stride=1, padding=pool_window // 2),\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
        "            nn.BatchNorm2d(in_channels // reduction, momentum=0.95),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "        A = self.SA(x)\n",
        "\n",
        "        A = F.upsample(A, (h, w), mode='bilinear')\n",
        "        output = x * A\n",
        "        if self.add_input:\n",
        "            output += x\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "lhu8GnWY6AA1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####unetrpp_ff"
      ],
      "metadata": {
        "id": "vjSolgIQRy54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from typing import Tuple, Union\n",
        "#from network_architecture.neural_network import SegmentationNetwork\n",
        "#from network_architecture.dynunet_block import UnetOutBlock, UnetResBlock\n",
        "#from network_architecture.synapse.model_components import UnetrPPEncoder, UnetrUpBlock\n",
        "#from  net.UNetFamily import U_Nett\n",
        "#from net.dual_local_global_attention import Patch_AttentionV2\n",
        "class UpSampling(nn.Module):\n",
        "    def __init__(self, scale_factor =2):\n",
        "        super(UpSampling, self).__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor = scale_factor,  mode='bilinear', align_corners=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        return x\n",
        "\n",
        "class FeatureFusion(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, norm_name='batch'):\n",
        "        super(FeatureFusion, self).__init__()\n",
        "        self.urb1 = UnetResBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "        )\n",
        "        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=5, stride=3, padding=2, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_channels*2, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.urb2 = UnetResBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=out_channels*2,\n",
        "            out_channels=out_channels*1,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, multif):\n",
        "        b, c, h, w = x.shape\n",
        "        dec1, dec2 = multif\n",
        "        urb1 = self.urb1(x)\n",
        "\n",
        "        conv1 = self.conv1(urb1)\n",
        "        dec1 = self.conv2(dec1)\n",
        "        conv1 = torch.cat([conv1, dec1], dim=1)\n",
        "        urb2 = self.urb2(conv1)\n",
        "\n",
        "\n",
        "\n",
        "        urb2 = F.interpolate(urb2, (h, w ), mode='bilinear')\n",
        "\n",
        "        ff = torch.cat([urb1, urb2], dim=1)\n",
        "\n",
        "        return ff\n",
        "\n",
        "\n",
        "\n",
        "class RetinalVasularSegFF(SegmentationNetwork):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            img_size: [640, 640],\n",
        "            feature_size: int = 16,\n",
        "            hidden_size: int = 256,\n",
        "            num_heads: int = 4,\n",
        "            pos_embed: str = \"perceptron\",  # TODO: Remove the argument\n",
        "            norm_name: Union[Tuple, str] = \"instance\",\n",
        "            dropout_rate: float = 0.0,\n",
        "            depths=None,\n",
        "            dims=None,\n",
        "            conv_op=nn.Conv3d,\n",
        "            do_ds=True,\n",
        "\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if depths is None:\n",
        "            depths = [3, 3, 3, 3]\n",
        "        self.do_ds = do_ds\n",
        "        self.conv_op = conv_op\n",
        "        self.num_classes = out_channels\n",
        "        if not (0 <= dropout_rate <= 1):\n",
        "            raise AssertionError(\"dropout_rate should be between 0 and 1.\")\n",
        "\n",
        "        if pos_embed not in [\"conv\", \"perceptron\"]:\n",
        "            raise KeyError(f\"Position embedding layer of type {pos_embed} is not supported.\")\n",
        "\n",
        "        self.patch_size = (3, 3)\n",
        "        self.feat_size = (\n",
        "            img_size[0] // self.patch_size[0] // 8,  # 8 is the downsampling happened through the four encoders stages\n",
        "            img_size[1] // self.patch_size[1] // 8,  # 8 is the downsampling happened through the four encoders stages\n",
        "        )\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = [self.feat_size[0]*8*self.feat_size[1]*8, self.feat_size[0]*4*self.feat_size[1]*4,\n",
        "                           self.feat_size[0]*2*self.feat_size[1]*2, self.feat_size[0]*1*self.feat_size[1]*1]\n",
        "        self.unetr_pp_encoder = UnetrPPEncoder(input_size=self.input_size, patch_size=self.patch_size, dims=dims, depths=depths, num_heads=num_heads, in_channels=feature_size)\n",
        "\n",
        "        self.prarm = nn.Parameter(torch.tensor([0.2, 0.6, 0.2]),requires_grad=True).cuda().float()\n",
        "        norm_name ='batch'\n",
        "        self.unet = U_Nett(img_ch=feature_size, output_ch=feature_size, fea_channels=32)\n",
        "        self.encoder1 = UnetResBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=feature_size,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "        )\n",
        "\n",
        "        self.la1 = UnetResBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size*2,\n",
        "            out_channels=feature_size*2,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "        )\n",
        "        self.la2 = UnetResBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size*4,\n",
        "            out_channels=feature_size*4,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "        )\n",
        "        self.la3 = UnetResBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size*8,\n",
        "            out_channels=feature_size*8,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            norm_name=norm_name,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        self.decoder5 = UnetrUpBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size * 16,\n",
        "            out_channels=feature_size * 8,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            out_size=img_size[0]//(self.patch_size[0]*4) * img_size[1]//(self.patch_size[1]*4),\n",
        "        )\n",
        "        self.decoder4 = UnetrUpBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size * 8,\n",
        "            out_channels=feature_size * 4,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            out_size=img_size[0]//(self.patch_size[0]*2) * img_size[1]//(self.patch_size[1]*2),\n",
        "        )\n",
        "        self.decoder3 = UnetrUpBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size * 4,\n",
        "            out_channels=feature_size * 2,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=2,\n",
        "            norm_name=norm_name,\n",
        "            out_size=img_size[0]//self.patch_size[0] * img_size[1]//self.patch_size[1],\n",
        "        )\n",
        "        self.decoder2 = UnetrUpBlock(\n",
        "            spatial_dims=2,\n",
        "            in_channels=feature_size * 2,\n",
        "            out_channels=feature_size,\n",
        "            kernel_size=3,\n",
        "            upsample_kernel_size=self.patch_size,\n",
        "            norm_name=norm_name,\n",
        "            out_size=img_size[0] * img_size[1],\n",
        "            conv_decoder=True,\n",
        "        )\n",
        "        # self.FeaFus = FeatureFusion(feature_size, feature_size)\n",
        "        self.out1 = UnetOutBlock(spatial_dims=2, in_channels=feature_size, out_channels=out_channels)\n",
        "        # if self.do_ds:\n",
        "        #     self.out2 = UnetOutBlock(spatial_dims=2, in_channels=feature_size * 2, out_channels=out_channels)\n",
        "        #     self.out3 = UnetOutBlock(spatial_dims=2, in_channels=feature_size * 4, out_channels=out_channels)\n",
        "\n",
        "    def proj_feat(self, x, hidden_size, feat_size):\n",
        "        x = x.view(x.size(0), feat_size[0], feat_size[1], hidden_size)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        return x\n",
        "\n",
        "    def forward(self, x_in):\n",
        "\n",
        "        convBlock = self.encoder1(x_in)\n",
        "        x_output, hidden_states = self.unetr_pp_encoder(convBlock)\n",
        "\n",
        "\n",
        "        # convBlock = self.encoder11(convBlock)\n",
        "        convBlock1 = self.unet(convBlock)[0]\n",
        "\n",
        "        # Four encoders\n",
        "        enc1 = hidden_states[0]\n",
        "        enc2 = hidden_states[1]\n",
        "        enc3 = hidden_states[2]\n",
        "        enc4 = hidden_states[3]\n",
        "\n",
        "        enc1 = self.la1(enc1)\n",
        "        enc2 = self.la2(enc2)\n",
        "        enc3 = self.la3(enc3)\n",
        "\n",
        "        # Four decoders\n",
        "        dec4 = self.proj_feat(enc4, self.hidden_size, self.feat_size)\n",
        "        dec3 = self.decoder5(dec4, enc3)\n",
        "        dec2 = self.decoder4(dec3, enc2)\n",
        "        dec1 = self.decoder3(dec2, enc1)\n",
        "\n",
        "        decoder2 = self.decoder2(dec1, convBlock1)\n",
        "        out = decoder2#self.FeaFus(decoder2, [dec1, dec2])\n",
        "        # if self.do_ds:\n",
        "        #     logits = [self.out1(out), self.out2(dec1), self.out3(dec2)]\n",
        "        # else:\n",
        "        logits = self.out1(out)\n",
        "\n",
        "        return logits, logits, logits"
      ],
      "metadata": {
        "id": "xoCk0LbS5WRb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####loss"
      ],
      "metadata": {
        "id": "no88M30IR7wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "#import config.config as cfg\n",
        "\n",
        "def dice_loss(score, target):\n",
        "    target = target.float()\n",
        "    smooth = 1e-5\n",
        "    intersect = torch.sum(score * target)\n",
        "    y_sum = torch.sum(target * target)\n",
        "    z_sum = torch.sum(score * score)\n",
        "    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n",
        "    loss = 1 - loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def secondary_losses(pred, train_label):\n",
        "    train_label = torch.unsqueeze(train_label, dim=1)\n",
        "    depth, height = cfg.IMG_SIZE//3, cfg.IMG_SIZE//3\n",
        "    train_label = F.interpolate(train_label, (depth, height), mode='nearest')\n",
        "    train_label = torch.squeeze(train_label, dim=1)\n",
        "\n",
        "\n",
        "    loss_seg = F.cross_entropy(pred, train_label.long())\n",
        "    outputs_soft = F.softmax(pred, dim=1)\n",
        "    loss_dice_ = dice_loss(outputs_soft[:, 1, :, :], train_label[:, :, :])\n",
        "    loss = 1.0 * (loss_dice_ + loss_seg)\n",
        "    return  loss\n",
        "\n",
        "\n",
        "\n",
        "class FocalLoss2d(torch.nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, size_average=True, ignore_index=255):\n",
        "        super(FocalLoss2d, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ignore_index = ignore_index\n",
        "        self.size_average = size_average # 对batch里面的数据取均值/求和\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets.long(), reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        if self.size_average:\n",
        "            return focal_loss.mean()\n",
        "        else:\n",
        "            return focal_loss.sum()\n",
        "\n",
        "class WeightedFocalLoss(torch.nn.Module):\n",
        "    \"Non weighted version of Focal Loss\"\n",
        "    def __init__(self, alpha=.25, gamma=2):\n",
        "        super(WeightedFocalLoss, self).__init__()\n",
        "        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none').view(-1)#\n",
        "\n",
        "        targets = targets.type(torch.long)\n",
        "        at = self.alpha.gather(0, targets.data.view(-1))\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()\n",
        "\n",
        "\n",
        "\n",
        "def cal_sen(outputs_soft, train_label):\n",
        "\n",
        "    bn, h, w = outputs_soft.shape\n",
        "    Sen = 0\n",
        "    Acc = 0\n",
        "    Spec = 0\n",
        "    nm = 0\n",
        "    for i in range(bn):\n",
        "        b_data = torch.gt(outputs_soft[i], 0.5).float()#[ph: ph+height, pw:pw+width], 0.5).float()\n",
        "        b_label = train_label[i]#[ph: ph+height, pw:pw+width].float()\n",
        "        TP = torch.sum(b_data*b_label)\n",
        "        FN = torch.sum((1 - b_data) *b_label)\n",
        "\n",
        "        TN = torch.sum((1 - b_data) * (1- b_label))\n",
        "        FP = torch.sum((b_data) * (1- b_label))\n",
        "\n",
        "        if (TP + FN) ==0:\n",
        "            Sen = Sen + 0\n",
        "            nm = nm +1\n",
        "        else:\n",
        "            Sen = Sen + TP / (TP + FN)\n",
        "\n",
        "\n",
        "        Acc = Acc + (TP + TN)/(TP + FN + TN + FP)\n",
        "        Spec = Spec + (TN)/(TN + FP)\n",
        "\n",
        "    Sen = Sen / (bn-nm)\n",
        "    Acc = Acc / bn\n",
        "    Spec = Spec / bn\n",
        "\n",
        "    return Sen, Acc, Spec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zN53YaRg6tIQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##metrics"
      ],
      "metadata": {
        "id": "XOVIxvyDd4B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This part contains functions related to the calculation of performance indicators  介绍性能指标计算及相关功能\n",
        "\"\"\"\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import torch\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pylab as pylab\n",
        "import matplotlib.pyplot as plt\n",
        "params = {'legend.fontsize': 13,\n",
        "         'axes.labelsize': 15,\n",
        "         'axes.titlesize':15,\n",
        "         'xtick.labelsize':15,\n",
        "         'ytick.labelsize':15} # define pyplot parameters\n",
        "pylab.rcParams.update(params)\n",
        "#Area under the ROC curve\n",
        "\n",
        "class Evaluate():\n",
        "    def __init__(self,save_path=None):\n",
        "        self.target = None\n",
        "        self.output = None\n",
        "        self.save_path = save_path\n",
        "        if self.save_path is not None:\n",
        "            if not os.path.exists(self.save_path):\n",
        "                os.makedirs(self.save_path)\n",
        "        self.threshold_confusion = 0.5\n",
        "\n",
        "    # Add data pair (target and predicted value)\n",
        "    def add_batch(self,batch_tar,batch_out):\n",
        "        batch_tar = batch_tar.flatten()\n",
        "        batch_out = batch_out.flatten()\n",
        "\n",
        "        self.target = batch_tar if self.target is None else np.concatenate((self.target,batch_tar))\n",
        "        self.output = batch_out if self.output is None else np.concatenate((self.output,batch_out))\n",
        "\n",
        "    # Plot ROC and calculate AUC of ROC\n",
        "    def auc_roc(self,plot=False):\n",
        "        AUC_ROC = roc_auc_score(self.target, self.output)\n",
        "        # print(\"\\nAUC of ROC curve: \" + str(AUC_ROC))\n",
        "        if plot and self.save_path is not None:\n",
        "            fpr, tpr, thresholds = roc_curve(self.target, self.output)\n",
        "            # print(\"\\nArea under the ROC curve: \" + str(AUC_ROC))\n",
        "            plt.figure()\n",
        "            plt.plot(fpr, tpr, '-', label='Area Under the Curve (AUC = %0.4f)' % AUC_ROC)\n",
        "            plt.title('ROC curve')\n",
        "            plt.xlabel(\"FPR (False Positive Rate)\")\n",
        "            plt.ylabel(\"TPR (True Positive Rate)\")\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.savefig(join(self.save_path , \"ROC.png\"))\n",
        "        return AUC_ROC\n",
        "\n",
        "    # Plot PR curve and calculate AUC of PR curve\n",
        "    def auc_pr(self,plot=False):\n",
        "        precision, recall, thresholds = precision_recall_curve(self.target, self.output)\n",
        "        precision = np.fliplr([precision])[0]\n",
        "        recall = np.fliplr([recall])[0]\n",
        "        AUC_pr = np.trapz(precision, recall)\n",
        "        # print(\"\\nAUC of P-R curve: \" + str(AUC_pr))\n",
        "        if plot and self.save_path is not None:\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(recall, precision, '-', label='Area Under the Curve (AUC = %0.4f)' % AUC_pr)\n",
        "            plt.title('Precision - Recall curve')\n",
        "            plt.xlabel(\"Recall\")\n",
        "            plt.ylabel(\"Precision\")\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.savefig(join(self.save_path ,\"Precision_recall.png\"))\n",
        "        return AUC_pr\n",
        "\n",
        "    # Accuracy, specificity, sensitivity, precision can be obtained by calculating the confusion matrix\n",
        "    def confusion_matrix(self):\n",
        "        #Confusion matrix\n",
        "        y_pred = self.output>=self.threshold_confusion\n",
        "        confusion = confusion_matrix(self.target, y_pred)\n",
        "        # print(confusion)\n",
        "        accuracy = 0\n",
        "        if float(np.sum(confusion))!=0:\n",
        "            accuracy = float(confusion[0,0]+confusion[1,1])/float(np.sum(confusion))\n",
        "        # print(\"Global Accuracy: \" +str(accuracy))\n",
        "        specificity = 0\n",
        "        if float(confusion[0,0]+confusion[0,1])!=0:\n",
        "            specificity = float(confusion[0,0])/float(confusion[0,0]+confusion[0,1])\n",
        "        # print(\"Specificity: \" +str(specificity))\n",
        "        sensitivity = 0\n",
        "        if float(confusion[1,1]+confusion[1,0])!=0:\n",
        "            sensitivity = float(confusion[1,1])/float(confusion[1,1]+confusion[1,0])\n",
        "        # print(\"Sensitivity: \" +str(sensitivity))\n",
        "        precision = 0\n",
        "        if float(confusion[1,1]+confusion[0,1])!=0:\n",
        "            precision = float(confusion[1,1])/float(confusion[1,1]+confusion[0,1])\n",
        "        # print(\"Precision: \" +str(precision))\n",
        "        return confusion,accuracy,specificity,sensitivity,precision\n",
        "\n",
        "    # Jaccard similarity index\n",
        "    def jaccard_index(self):\n",
        "        pass\n",
        "        # jaccard_index = jaccard_similarity_score(y_true, y_pred, normalize=True)\n",
        "        # print(\"\\nJaccard similarity score: \" +str(jaccard_index))\n",
        "\n",
        "    # calculating f1_score\n",
        "    def f1_score(self):\n",
        "        pred = self.output>=self.threshold_confusion\n",
        "        F1_score = f1_score(self.target, pred, labels=None, average='binary', sample_weight=None)\n",
        "        # print(\"F1 score (F-measure): \" +str(F1_score))\n",
        "        return F1_score\n",
        "\n",
        "    # Save performance results to specified file\n",
        "    def save_all_result(self,plot_curve=True,save_name=None):\n",
        "        #Save the results\n",
        "        AUC_ROC = self.auc_roc(plot=plot_curve)\n",
        "        AUC_pr  = self.auc_pr(plot=plot_curve)\n",
        "        F1_score = self.f1_score()\n",
        "        confusion,accuracy, specificity, sensitivity, precision = self.confusion_matrix()\n",
        "        if save_name is not None:\n",
        "            file_perf = open(join(self.save_path, save_name), 'w')\n",
        "            file_perf.write(\"AUC ROC curve: \"+str(AUC_ROC)\n",
        "                            + \"\\nAUC PR curve: \" +str(AUC_pr)\n",
        "                            # + \"\\nJaccard similarity score: \" +str(jaccard_index)\n",
        "                            + \"\\nF1 score: \" +str(F1_score)\n",
        "                            +\"\\nAccuracy: \" +str(accuracy)\n",
        "                            +\"\\nSensitivity(SE): \" +str(sensitivity)\n",
        "                            +\"\\nSpecificity(SP): \" +str(specificity)\n",
        "                            +\"\\nPrecision: \" +str(precision)\n",
        "                            + \"\\n\\nConfusion matrix:\"\n",
        "                            + str(confusion)\n",
        "                            )\n",
        "            file_perf.close()\n",
        "        return OrderedDict([(\"AUC_ROC\",AUC_ROC),(\"AUC_PR\",AUC_pr),\n",
        "                            (\"f1-score\",F1_score),(\"Acc\",accuracy),\n",
        "                            (\"SE\",sensitivity),(\"SP\",specificity),\n",
        "                            (\"precision\",precision)\n",
        "                            ])"
      ],
      "metadata": {
        "id": "KMIpzMBtrkJY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##model forward"
      ],
      "metadata": {
        "id": "xkdsljTASN9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "#from data.load_train_data import TrainData\n",
        "#from data.load_test_data import TestData\n",
        "from torch.utils.data import DataLoader\n",
        "import sklearn.metrics as metrics\n",
        "#from net.retinal_vasuclar_net import RetinalVasularSeg\n",
        "#from net.unetrpp_ff import RetinalVasularSegFF\n",
        "#from net.UNetFamily import U_Net, AttU_Net, Dense_Unet\n",
        "#from net.loss import dice_loss, cal_sen\n",
        "#from metrics import Evaluate\n",
        "#import config.config as cfg\n",
        "\n",
        "def model_forward(model, test_data, patch_size, hh,ww, stride_y, stride_x):\n",
        "\n",
        "    sy = math.ceil((hh - patch_size[0]) / stride_y) + 1\n",
        "    sx = math.ceil((ww - patch_size[1]) / stride_x) + 1\n",
        "\n",
        "    score_map = np.zeros((1, hh, ww)).astype(np.float32)\n",
        "    cnt = np.zeros((1, hh, ww)).astype(np.int32)\n",
        "    test_datas = []\n",
        "    boxes = []\n",
        "    for y in range(0, sy):\n",
        "        ys = min(stride_y * y, hh - patch_size[0])\n",
        "        for x in range(0, sx):\n",
        "            xs = min(stride_x * x, ww - patch_size[1])\n",
        "\n",
        "            test_patch = test_data[:, :, ys:ys + patch_size[0], xs:xs + patch_size[1]]\n",
        "            test_datas.append(test_patch)\n",
        "            boxes.append([ys, ys + patch_size[0], xs, xs + patch_size[1]])\n",
        "\n",
        "    bsize = 8\n",
        "    batch_nums = math.ceil(len(test_datas)/bsize)\n",
        "    for i in range(batch_nums):\n",
        "        test_patch = torch.cat(test_datas[i*bsize:(i+1)*bsize], dim=0)\n",
        "        outputs_segb = model(test_patch)[0]\n",
        "        outputs_softb = F.sigmoid(outputs_segb)\n",
        "        # outputs_softb = torch.softmax(outputs_segb, dim=1)[:, 1, :, :]\n",
        "        predb = torch.squeeze(outputs_softb).detach().cpu().numpy()\n",
        "\n",
        "        for j in range(predb.shape[0]):\n",
        "            y1, y2, x1, x2 = boxes[i*bsize +j]\n",
        "            score_map[0, y1: y2, x1: x2] = score_map[0, y1: y2, x1: x2] + predb[j]\n",
        "\n",
        "            cnt[0, y1: y2, x1: x2] = cnt[0, y1: y2, x1: x2] + 1\n",
        "\n",
        "    # cv2.namedWindow(\"score_map\", cv2.WINDOW_NORMAL)\n",
        "    # cv2.imshow(\"score_map\", score_map[0, :, :])\n",
        "    #\n",
        "    # cv2.waitKey(0)\n",
        "    score_map = torch.tensor(score_map / cnt).cuda().float()\n",
        "\n",
        "    return  score_map\n"
      ],
      "metadata": {
        "id": "CfjKNTuFyh7e"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##main ##This part you uploaded after removing the extra parts and I was able to run it with a small change in \"args\". You can see the execution results and errors."
      ],
      "metadata": {
        "id": "ndPSC8VY4HDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "#from data.load_train_data import TrainData, data_crop\n",
        "#from data.load_test_data import TestData\n",
        "from torch.utils.data import DataLoader\n",
        "import sklearn.metrics as metrics\n",
        "#from net.retinal_vasuclar_net import RetinalVasularSeg\n",
        "#from net.unetrpp_ff import RetinalVasularSegFF\n",
        "#from net.UNetFamily import U_Net, AttU_Net, Dense_Unet\n",
        "# from net.Hessian import HessianNet\n",
        "#from net.loss import dice_loss, FocalLoss2d, WeightedFocalLoss, cal_sen\n",
        "#from test import model_forward\n",
        "#import config.config as cfg\n",
        "\n",
        "#def model_initial(model, model_name):\n",
        "#    # 加载预训练模型\n",
        "#    pretrained_dict = torch.load(model_name)[\"model\"]\n",
        "#    model_dict = model.state_dict()\n",
        "#    # 1. filter out unnecessary keys\n",
        "#    # pretrained_dictf = {k.replace('module.', \"\"): v for k, v in pretrained_dict.items() if k.replace('module.', \"\") in model_dict}\n",
        "#    pretrained_dictf = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "#    # 2. overwrite entries in the existing state dict\n",
        "#    model_dict.update(pretrained_dictf)\n",
        "#    # 3. load the new state dict\n",
        "#    model.load_state_dict(model_dict)\n",
        "\n",
        "#    print(\"over\")\n",
        "\n",
        "\n",
        "def _init_():\n",
        "    if not os.path.exists('outputs'):\n",
        "        os.makedirs('outputs')\n",
        "    if not os.path.exists('./outputs/' + args.exp_name):\n",
        "        os.makedirs('./outputs/' + args.exp_name)\n",
        "    if not os.path.exists('./outputs/' + args.exp_name + '/' + 'models'):\n",
        "        os.makedirs('./outputs/' + args.exp_name + '/' + 'models')\n",
        "    os.system('cp main_cls.py outputs' + '/' + args.exp_name + '/' + 'main_cls.py.backup')\n",
        "    os.system('cp model.py outputs' + '/' + args.exp_name + '/' + 'model.py.backup')\n",
        "    os.system('cp util.py outputs' + '/' + args.exp_name + '/' + 'util.py.backup')\n",
        "    os.system('cp data.py outputs' + '/' + args.exp_name + '/' + 'data.py.backup')\n",
        "\n",
        "class IOStream():\n",
        "    def __init__(self, path):\n",
        "        self.f = open(path, 'a')\n",
        "\n",
        "    def cprint(self, text):\n",
        "        print(text)\n",
        "        self.f.write(text+'\\n')\n",
        "        self.f.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.f.close()\n",
        "\n",
        "def train(args, io):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    file_path = \"/content/training/images/\"\n",
        "    label_path = \"/content/training/1st_manual/\"\n",
        "    #file_path = \"F:/DRIVE/training/images/\"\n",
        "    #label_path = \"F:/DRIVE/training/1st_manual/\"\n",
        "    # file_path = \"E:/vasular/CHASEDB1/images/\"\n",
        "    # label_path = \"E:/vasular/CHASEDB1/1st_label/\"\n",
        "    # file_path = \"G:/vasular/STAREdatabase/images/\"\n",
        "    # label_path = \"G:/vasular/STAREdatabase/labels-ah/\"\n",
        "    # file_path = \"G:/vasular/HRFdatas/images/\"\n",
        "    # label_path = \"G:/vasular/HRFdatas/images/\"\n",
        "    train_loader = DataLoader(TrainData(file_path, label_path, train_flag = True), num_workers=0,\n",
        "                              batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    file_path = \"/content/test/images/\"\n",
        "    label_path = \"/content/test/1st_manual/\"\n",
        "    #file_path = \"F:/DRIVE/test/images/\"\n",
        "    #label_path = \"F:/DRIVE/test/1st_manual/\"\n",
        "    # file_path = \"E:/vasular/CHASEDB1/test/\"\n",
        "    # label_path = \"E:/vasular/CHASEDB1/1st_label/\"\n",
        "    # file_path = \"G:/vasular/STAREdatabase/test/\"\n",
        "    # label_path = \"G:/vasular/STAREdatabase/labels-ah/\"\n",
        "    # file_path = \"G:/vasular/HRFdatas/test/\"\n",
        "    # label_path = \"G:/vasular/HRFdatas/test/\"\n",
        "    test_loader = DataLoader(TestData(file_path, label_path, train_flag = False), num_workers=0,\n",
        "                             batch_size=args.test_batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    # Try to load models\n",
        "    model = RetinalVasularSegFF(in_channels=3,\n",
        "                            out_channels=2,\n",
        "                            img_size=[192, 192],\n",
        "                            feature_size=16,\n",
        "                            num_heads=4,\n",
        "                            depths=[3, 3, 3, 3],\n",
        "                            dims=[32, 64, 128, 256],\n",
        "                              hidden_size=256,\n",
        "                            do_ds=True,\n",
        "                            )\n",
        "\n",
        "    #model_name = \"./outputs/teethseg_model_200.pth\"\n",
        "    # model_initial(model, model_name)\n",
        "\n",
        "    opt = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(opt, args.epochs, eta_min=1e-6, last_epoch = -1)\n",
        "\n",
        "    focalLoss = FocalLoss2d(gamma=2)#WeightedFocalLoss()#\n",
        "\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "    inter_nums = len(train_loader)\n",
        "    total_acc = 0\n",
        "    for epoch in range(0, args.epochs):\n",
        "        ####################\n",
        "        # Train\n",
        "        ####################\n",
        "\n",
        "        if args.scheduler == 'cos':\n",
        "            scheduler.step()\n",
        "        elif args.scheduler == 'step':\n",
        "            if opt.param_groups[0]['lr'] > 1e-5:\n",
        "                scheduler.step()\n",
        "            if opt.param_groups[0]['lr'] < 1e-5:\n",
        "                for param_group in opt.param_groups:\n",
        "                    param_group['lr'] = 1e-5\n",
        "\n",
        "        train_loss = 0.0\n",
        "        loss_dice = 0\n",
        "        sen_v = 0\n",
        "        acc_v = 0\n",
        "        spec_v = 0\n",
        "        # for data, edges, label in train_loader:\n",
        "        tic = time.time()\n",
        "        nums = 0\n",
        "        model.train()\n",
        "        for train_data, train_label, weight_ in train_loader:\n",
        "\n",
        "            train_data, train_label, train_weight_ = data_crop(train_data, train_label, weight_)\n",
        "\n",
        "            train_data = train_data.cuda().float()\n",
        "            train_label = train_label.cuda().float()\n",
        "            train_weight_ = train_weight_.cuda().float()\n",
        "\n",
        "            nums = nums +1\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                outputs_seg, out2, _ = model(train_data)\n",
        "\n",
        "                train_label[train_label >= 0.5] = 1\n",
        "                loss_seg = F.cross_entropy(outputs_seg, train_label.long())\n",
        "                # loss_seg = F.binary_cross_entropy_with_logits(outputs_seg[:,0, :, :], train_label)\n",
        "                sec_loss = focalLoss(outputs_seg, train_label[:, :, :].long())\n",
        "\n",
        "                outputs_soft = torch.softmax(outputs_seg, dim=1)[:, 1, :, :]#F.sigmoid(outputs_seg[:, 0, :, :])#\n",
        "                loss_dice_ = dice_loss(outputs_soft, train_label[:, :, :])\n",
        "\n",
        "                sen_v_, acc_v_, spec_v_ = cal_sen(outputs_soft, train_label[:, :, :].long())\n",
        "\n",
        "                loss = (loss_seg + 1*loss_dice_ + 2*sec_loss)*1  #\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            # Unscales gradients and calls\n",
        "            # or skips optimizer.step()\n",
        "            scaler.step(opt)\n",
        "            # Updates the scale for next iteration\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            loss_dice += loss_dice_.item()\n",
        "            sen_v += sen_v_.item()\n",
        "            acc_v += acc_v_.item()\n",
        "            spec_v += spec_v_.item()\n",
        "            VIEW_NUMS = 5\n",
        "            if nums % VIEW_NUMS == 0:\n",
        "                toc = time.time()\n",
        "                train_loss = train_loss/ (VIEW_NUMS)\n",
        "                loss_dice = loss_dice / (VIEW_NUMS)\n",
        "                sen_v = sen_v/(VIEW_NUMS)\n",
        "                acc_v = acc_v/(VIEW_NUMS)\n",
        "                spec_v = spec_v/(VIEW_NUMS)\n",
        "\n",
        "                print(\"lr = \", opt.param_groups[0]['lr'])\n",
        "                outstr = 'epoch %d /%d,epoch %d /%d, loss: %.6f, loss_dice: %.6f, sen_v: %.6f, acc_v: %.6f, spec_v: %.6f, const time: %.6f' % (\n",
        "                 epoch,args.epochs, nums, inter_nums, train_loss, loss_dice, sen_v, acc_v, spec_v, toc - tic)\n",
        "\n",
        "                io.cprint(outstr)\n",
        "                train_loss = 0.0\n",
        "                loss_dice = 0\n",
        "                sen_v = 0\n",
        "                acc_v = 0\n",
        "                spec_v = 0\n",
        "                tic = time.time()\n",
        "        if 0 == epoch % 10 and epoch>300:\n",
        "            test_nums = 0\n",
        "            loss_dice, sen_v, acc_v, spec_v =0, 0, 0, 0\n",
        "            model.eval()\n",
        "            CROP_SIZE = 192\n",
        "            patch_size = [CROP_SIZE, CROP_SIZE]\n",
        "            stride_y, stride_x = 128, 128\n",
        "            for test_data, test_label in test_loader:\n",
        "                test_data = test_data.cuda().float()\n",
        "                test_label = test_label.cuda().float()\n",
        "                nums = nums + 1\n",
        "                test_nums = test_nums + 1\n",
        "                hh, ww = test_label.shape[-2:]\n",
        "                with autocast():\n",
        "                    score_map = model_forward(model, test_data, patch_size, hh, ww, stride_y, stride_x)\n",
        "                    train_label[train_label > 0.5] = 1\n",
        "                    loss_dice_ = dice_loss(score_map, test_label.long())\n",
        "                    sen_v_, acc_v_, spec_v_ = cal_sen(score_map.detach(), test_label.detach().long())\n",
        "                loss = 1.0 * (loss_dice_).item()\n",
        "                sen_v = sen_v + sen_v_.item()\n",
        "                acc_v = acc_v + acc_v_.item()\n",
        "                spec_v = spec_v + spec_v_.item()\n",
        "                loss_dice = loss_dice + loss_dice_.item()\n",
        "            sen_v = sen_v/test_nums\n",
        "            acc_v = acc_v/test_nums\n",
        "            spec_v = spec_v/test_nums\n",
        "            loss_dice = loss_dice/test_nums\n",
        "            toc = time.time()\n",
        "            outstr = 'epoch %d, loss: %.6f, loss_dice: %.6f, sen_v: %.6f, acc_v: %.6f, spec_v: %.6f, const time: %.6f' % (\n",
        "                epoch, loss, loss_dice, sen_v, acc_v, spec_v, toc - tic)\n",
        "            io.cprint(\"test   \"+outstr)\n",
        "            if (total_acc < (sen_v*0.35+acc_v*0.35+spec_v*0.35)):\n",
        "                total_acc = (sen_v * 0.35 + acc_v * 0.35 + spec_v * 0.35)\n",
        "                torch.save({'model': model.state_dict(), 'epoch': epoch},\n",
        "                           'outputs/' + str(sen_v)+\"_\"+ str(acc_v)+ \"_\"+str(spec_v) + '.pth')\n",
        "\n",
        "        if (epoch) % 200 == 0:\n",
        "            torch.save({'model': model.state_dict(), 'epoch': epoch}, 'outputs/teethseg_model_' + str(epoch)+ '.pth')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description='retinal vascular segmentation')\n",
        "    parser.add_argument('--exp_name', type=str, default='retinal', metavar='N',\n",
        "                        help='Name of the experiment')\n",
        "    parser.add_argument('--batch_size', type=int, default=4, metavar='batch_size',\n",
        "                        help='Size of batch)')\n",
        "    parser.add_argument('--test_batch_size', type=int, default=1, metavar='batch_size',\n",
        "                        help='Size of batch)')\n",
        "    parser.add_argument('--epochs', type=int, default=1001, metavar='N',\n",
        "                        help='number of episode to train ')\n",
        "    parser.add_argument('--lr', type=float, default=2*1e-3, metavar='LR',\n",
        "                        help='learning rate (default: 0.001, 0.1 if using sgd)')\n",
        "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
        "                        help='SGD momentum (default: 0.9)')\n",
        "    parser.add_argument('--scheduler', type=str, default='cos', metavar='N',\n",
        "                        choices=['cos', 'step'],\n",
        "                        help='Scheduler to use, [cos, step]')\n",
        "    parser.add_argument('--no_cuda', type=bool, default=False,\n",
        "                        help='enables CUDA training')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    #args = parser.parse_args()\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    _init_()\n",
        "\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    io = IOStream('outputs/' + args.exp_name + '/run.log')\n",
        "    io.cprint(str(args))\n",
        "\n",
        "    train(args, io)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vXqjDhIGF8o0",
        "outputId": "51201ac6-3a20-4b85-81b2-a2e4f59176d2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(exp_name='retinal', batch_size=4, test_batch_size=1, epochs=1001, lr=0.002, momentum=0.9, scheduler='cos', no_cuda=False, seed=1)\n",
            "lr =  0.001999995077519133\n",
            "epoch 0 /1001,epoch 5 /5, loss: 1.491050, loss_dice: 0.709165, sen_v: 0.428682, acc_v: 0.570354, spec_v: 0.589697, const time: 3.778917\n",
            "lr =  0.001999980310125019\n",
            "epoch 1 /1001,epoch 5 /5, loss: 1.182309, loss_dice: 0.672420, sen_v: 0.000049, acc_v: 0.883301, spec_v: 0.999996, const time: 3.488863\n",
            "lr =  0.001999955697963115\n",
            "epoch 2 /1001,epoch 5 /5, loss: 1.054867, loss_dice: 0.599031, sen_v: 0.000031, acc_v: 0.876826, spec_v: 0.999996, const time: 3.763355\n",
            "lr =  0.0019999212412758474\n",
            "epoch 3 /1001,epoch 5 /5, loss: 0.863069, loss_dice: 0.501134, sen_v: 0.202521, acc_v: 0.900608, spec_v: 0.990257, const time: 3.689625\n",
            "lr =  0.001999876940402611\n",
            "epoch 4 /1001,epoch 5 /5, loss: 0.710403, loss_dice: 0.401850, sen_v: 0.493898, acc_v: 0.908211, spec_v: 0.960766, const time: 3.429961\n",
            "lr =  0.001999822795779764\n",
            "epoch 5 /1001,epoch 5 /5, loss: 0.592709, loss_dice: 0.311884, sen_v: 0.553609, acc_v: 0.913743, spec_v: 0.955946, const time: 3.512536\n",
            "lr =  0.0019997588079406258\n",
            "epoch 6 /1001,epoch 5 /5, loss: 0.550541, loss_dice: 0.276534, sen_v: 0.605361, acc_v: 0.917912, spec_v: 0.957642, const time: 3.783917\n",
            "lr =  0.0019996849775154686\n",
            "epoch 7 /1001,epoch 5 /5, loss: 0.520072, loss_dice: 0.266815, sen_v: 0.589205, acc_v: 0.925728, spec_v: 0.968093, const time: 3.628879\n",
            "lr =  0.001999601305231514\n",
            "epoch 8 /1001,epoch 5 /5, loss: 0.497069, loss_dice: 0.260029, sen_v: 0.614356, acc_v: 0.929969, spec_v: 0.967387, const time: 3.492806\n",
            "lr =  0.0019995077919129243\n",
            "epoch 9 /1001,epoch 5 /5, loss: 0.492265, loss_dice: 0.252492, sen_v: 0.612785, acc_v: 0.929929, spec_v: 0.969565, const time: 3.471155\n",
            "lr =  0.0019994044384807953\n",
            "epoch 10 /1001,epoch 5 /5, loss: 0.469209, loss_dice: 0.232315, sen_v: 0.666045, acc_v: 0.928588, spec_v: 0.964305, const time: 3.897796\n",
            "lr =  0.001999291245953146\n",
            "epoch 11 /1001,epoch 5 /5, loss: 0.473990, loss_dice: 0.245923, sen_v: 0.640802, acc_v: 0.932003, spec_v: 0.964603, const time: 3.471452\n",
            "lr =  0.001999168215444911\n",
            "epoch 12 /1001,epoch 5 /5, loss: 0.434193, loss_dice: 0.214195, sen_v: 0.669279, acc_v: 0.934289, spec_v: 0.969170, const time: 3.527974\n",
            "lr =  0.001999035348167926\n",
            "epoch 13 /1001,epoch 5 /5, loss: 0.445311, loss_dice: 0.226541, sen_v: 0.664826, acc_v: 0.933646, spec_v: 0.965029, const time: 3.447361\n",
            "lr =  0.0019988926454309187\n",
            "epoch 14 /1001,epoch 5 /5, loss: 0.435400, loss_dice: 0.215591, sen_v: 0.681858, acc_v: 0.934182, spec_v: 0.969134, const time: 4.011499\n",
            "lr =  0.0019987401086394956\n",
            "epoch 15 /1001,epoch 5 /5, loss: 0.440381, loss_dice: 0.225433, sen_v: 0.655121, acc_v: 0.934979, spec_v: 0.970097, const time: 3.578331\n",
            "lr =  0.001998577739296126\n",
            "epoch 16 /1001,epoch 5 /5, loss: 0.427282, loss_dice: 0.214752, sen_v: 0.680356, acc_v: 0.935997, spec_v: 0.965936, const time: 3.556939\n",
            "lr =  0.00199840553900013\n",
            "epoch 17 /1001,epoch 5 /5, loss: 0.422991, loss_dice: 0.214605, sen_v: 0.665375, acc_v: 0.936233, spec_v: 0.970452, const time: 3.471346\n",
            "lr =  0.0019982235094476606\n",
            "epoch 18 /1001,epoch 5 /5, loss: 0.411808, loss_dice: 0.205540, sen_v: 0.690578, acc_v: 0.937270, spec_v: 0.966038, const time: 3.922076\n",
            "lr =  0.001998031652431689\n",
            "epoch 19 /1001,epoch 5 /5, loss: 0.405497, loss_dice: 0.202175, sen_v: 0.673532, acc_v: 0.938690, spec_v: 0.972917, const time: 3.559720\n",
            "lr =  0.001997829969841984\n",
            "epoch 20 /1001,epoch 5 /5, loss: 0.394240, loss_dice: 0.196702, sen_v: 0.724485, acc_v: 0.939757, spec_v: 0.965769, const time: 3.569865\n",
            "lr =  0.0019976184636650973\n",
            "epoch 21 /1001,epoch 5 /5, loss: 0.382497, loss_dice: 0.192806, sen_v: 0.699134, acc_v: 0.942828, spec_v: 0.972889, const time: 3.619038\n",
            "lr =  0.00199739713598434\n",
            "epoch 22 /1001,epoch 5 /5, loss: 0.398106, loss_dice: 0.194576, sen_v: 0.710759, acc_v: 0.938305, spec_v: 0.967061, const time: 3.959823\n",
            "lr =  0.001997165988979765\n",
            "epoch 23 /1001,epoch 5 /5, loss: 0.384047, loss_dice: 0.194245, sen_v: 0.678770, acc_v: 0.942578, spec_v: 0.977385, const time: 3.599115\n",
            "lr =  0.001996925024928144\n",
            "epoch 24 /1001,epoch 5 /5, loss: 0.388059, loss_dice: 0.189699, sen_v: 0.743734, acc_v: 0.939534, spec_v: 0.963366, const time: 3.535189\n",
            "lr =  0.0019966742462029455\n",
            "epoch 25 /1001,epoch 5 /5, loss: 0.381266, loss_dice: 0.187886, sen_v: 0.700596, acc_v: 0.940566, spec_v: 0.972013, const time: 3.681661\n",
            "lr =  0.0019964136552743125\n",
            "epoch 26 /1001,epoch 5 /5, loss: 0.389828, loss_dice: 0.191398, sen_v: 0.700435, acc_v: 0.940295, spec_v: 0.972331, const time: 3.936272\n",
            "lr =  0.0019961432547090352\n",
            "epoch 27 /1001,epoch 5 /5, loss: 0.394376, loss_dice: 0.193204, sen_v: 0.730084, acc_v: 0.937474, spec_v: 0.963893, const time: 3.506294\n",
            "lr =  0.0019958630471705284\n",
            "epoch 28 /1001,epoch 5 /5, loss: 0.387103, loss_dice: 0.188897, sen_v: 0.702831, acc_v: 0.940497, spec_v: 0.973159, const time: 3.468290\n",
            "lr =  0.0019955730354188047\n",
            "epoch 29 /1001,epoch 5 /5, loss: 0.369881, loss_dice: 0.180613, sen_v: 0.722873, acc_v: 0.942234, spec_v: 0.970366, const time: 3.660633\n",
            "lr =  0.0019952732223104475\n",
            "epoch 30 /1001,epoch 5 /5, loss: 0.371220, loss_dice: 0.185279, sen_v: 0.710079, acc_v: 0.943673, spec_v: 0.971690, const time: 3.749693\n",
            "lr =  0.001994963610798581\n",
            "epoch 31 /1001,epoch 5 /5, loss: 0.380268, loss_dice: 0.185574, sen_v: 0.716906, acc_v: 0.940277, spec_v: 0.969240, const time: 3.441775\n",
            "lr =  0.001994644203932845\n",
            "epoch 32 /1001,epoch 5 /5, loss: 0.350498, loss_dice: 0.172656, sen_v: 0.722962, acc_v: 0.946360, spec_v: 0.971800, const time: 3.466975\n",
            "lr =  0.0019943150048593594\n",
            "epoch 33 /1001,epoch 5 /5, loss: 0.365100, loss_dice: 0.182455, sen_v: 0.702532, acc_v: 0.944973, spec_v: 0.974633, const time: 3.827008\n",
            "lr =  0.0019939760168206986\n",
            "epoch 34 /1001,epoch 5 /5, loss: 0.371669, loss_dice: 0.175053, sen_v: 0.748071, acc_v: 0.938384, spec_v: 0.964190, const time: 3.717773\n",
            "lr =  0.001993627243155856\n",
            "epoch 35 /1001,epoch 5 /5, loss: 0.358287, loss_dice: 0.177370, sen_v: 0.721545, acc_v: 0.944580, spec_v: 0.972008, const time: 3.513827\n",
            "lr =  0.001993268687300213\n",
            "epoch 36 /1001,epoch 5 /5, loss: 0.378980, loss_dice: 0.187978, sen_v: 0.730672, acc_v: 0.941279, spec_v: 0.967923, const time: 3.486547\n",
            "lr =  0.0019929003527855036\n",
            "epoch 37 /1001,epoch 5 /5, loss: 0.400700, loss_dice: 0.208245, sen_v: 0.687123, acc_v: 0.940480, spec_v: 0.971337, const time: 3.863576\n",
            "lr =  0.0019925222432397817\n",
            "epoch 38 /1001,epoch 5 /5, loss: 0.381174, loss_dice: 0.190133, sen_v: 0.729662, acc_v: 0.940029, spec_v: 0.967493, const time: 3.667348\n",
            "lr =  0.001992134362387383\n",
            "epoch 39 /1001,epoch 5 /5, loss: 0.376468, loss_dice: 0.185814, sen_v: 0.731549, acc_v: 0.941311, spec_v: 0.967583, const time: 3.428400\n",
            "lr =  0.0019917367140488904\n",
            "epoch 40 /1001,epoch 5 /5, loss: 0.361124, loss_dice: 0.177761, sen_v: 0.721732, acc_v: 0.942857, spec_v: 0.971388, const time: 3.409690\n",
            "lr =  0.0019913293021410947\n",
            "epoch 41 /1001,epoch 5 /5, loss: 0.351817, loss_dice: 0.168175, sen_v: 0.754682, acc_v: 0.943767, spec_v: 0.967406, const time: 3.918102\n",
            "lr =  0.0019909121306769567\n",
            "epoch 42 /1001,epoch 5 /5, loss: 0.356092, loss_dice: 0.178518, sen_v: 0.721577, acc_v: 0.946292, spec_v: 0.974561, const time: 3.735456\n",
            "lr =  0.001990485203765569\n",
            "epoch 43 /1001,epoch 5 /5, loss: 0.359991, loss_dice: 0.183230, sen_v: 0.745392, acc_v: 0.944885, spec_v: 0.967844, const time: 3.484759\n",
            "lr =  0.001990048525612112\n",
            "epoch 44 /1001,epoch 5 /5, loss: 0.357590, loss_dice: 0.180768, sen_v: 0.691267, acc_v: 0.945815, spec_v: 0.974662, const time: 3.462465\n",
            "lr =  0.0019896021005178168\n",
            "epoch 45 /1001,epoch 5 /5, loss: 0.335761, loss_dice: 0.160367, sen_v: 0.775714, acc_v: 0.944964, spec_v: 0.967003, const time: 3.901817\n",
            "lr =  0.0019891459328799203\n",
            "epoch 46 /1001,epoch 5 /5, loss: 0.347120, loss_dice: 0.167506, sen_v: 0.745081, acc_v: 0.944040, spec_v: 0.969411, const time: 3.559925\n",
            "lr =  0.0019886800271916214\n",
            "epoch 47 /1001,epoch 5 /5, loss: 0.334392, loss_dice: 0.162006, sen_v: 0.741760, acc_v: 0.946911, spec_v: 0.973996, const time: 3.482584\n",
            "lr =  0.0019882043880420385\n",
            "epoch 48 /1001,epoch 5 /5, loss: 0.342295, loss_dice: 0.165724, sen_v: 0.756882, acc_v: 0.944724, spec_v: 0.969153, const time: 3.460735\n",
            "lr =  0.0019877190201161633\n",
            "epoch 49 /1001,epoch 5 /5, loss: 0.325317, loss_dice: 0.157613, sen_v: 0.757139, acc_v: 0.947687, spec_v: 0.971594, const time: 4.023219\n",
            "lr =  0.001987223928194815\n",
            "epoch 50 /1001,epoch 5 /5, loss: 0.341283, loss_dice: 0.164936, sen_v: 0.732797, acc_v: 0.945605, spec_v: 0.973878, const time: 3.589775\n",
            "lr =  0.001986719117154593\n",
            "epoch 51 /1001,epoch 5 /5, loss: 0.330254, loss_dice: 0.153733, sen_v: 0.783229, acc_v: 0.943924, spec_v: 0.965832, const time: 3.474558\n",
            "lr =  0.0019862045919678286\n",
            "epoch 52 /1001,epoch 5 /5, loss: 0.336052, loss_dice: 0.166000, sen_v: 0.755961, acc_v: 0.946755, spec_v: 0.970203, const time: 3.451752\n",
            "lr =  0.001985680357702537\n",
            "epoch 53 /1001,epoch 5 /5, loss: 0.333731, loss_dice: 0.164970, sen_v: 0.732780, acc_v: 0.947269, spec_v: 0.973578, const time: 4.154281\n",
            "lr =  0.0019851464195223653\n",
            "epoch 54 /1001,epoch 5 /5, loss: 0.317093, loss_dice: 0.151669, sen_v: 0.762438, acc_v: 0.948784, spec_v: 0.973490, const time: 3.499106\n",
            "lr =  0.001984602782686545\n",
            "epoch 55 /1001,epoch 5 /5, loss: 0.340472, loss_dice: 0.165122, sen_v: 0.752520, acc_v: 0.945446, spec_v: 0.968546, const time: 3.433496\n",
            "lr =  0.0019840494525498373\n",
            "epoch 56 /1001,epoch 5 /5, loss: 0.321837, loss_dice: 0.153935, sen_v: 0.764412, acc_v: 0.946776, spec_v: 0.970328, const time: 3.514926\n",
            "lr =  0.001983486434562481\n",
            "epoch 57 /1001,epoch 5 /5, loss: 0.337069, loss_dice: 0.168624, sen_v: 0.728575, acc_v: 0.948035, spec_v: 0.975494, const time: 3.916248\n",
            "lr =  0.0019829137342701393\n",
            "epoch 58 /1001,epoch 5 /5, loss: 0.348446, loss_dice: 0.175768, sen_v: 0.739972, acc_v: 0.945718, spec_v: 0.969626, const time: 3.455480\n",
            "lr =  0.001982331357313846\n",
            "epoch 59 /1001,epoch 5 /5, loss: 0.330236, loss_dice: 0.159321, sen_v: 0.767752, acc_v: 0.946073, spec_v: 0.969401, const time: 3.503087\n",
            "lr =  0.001981739309429947\n",
            "epoch 60 /1001,epoch 5 /5, loss: 0.336219, loss_dice: 0.168264, sen_v: 0.746615, acc_v: 0.947843, spec_v: 0.973006, const time: 3.662658\n",
            "lr =  0.0019811375964500478\n",
            "epoch 61 /1001,epoch 5 /5, loss: 0.322294, loss_dice: 0.161059, sen_v: 0.742934, acc_v: 0.950107, spec_v: 0.974107, const time: 3.838943\n",
            "lr =  0.0019805262243009526\n",
            "epoch 62 /1001,epoch 5 /5, loss: 0.325505, loss_dice: 0.155256, sen_v: 0.772697, acc_v: 0.946142, spec_v: 0.969605, const time: 3.458177\n",
            "lr =  0.001979905199004608\n",
            "epoch 63 /1001,epoch 5 /5, loss: 0.335697, loss_dice: 0.159127, sen_v: 0.765876, acc_v: 0.944806, spec_v: 0.968783, const time: 3.432459\n",
            "lr =  0.001979274526678043\n",
            "epoch 64 /1001,epoch 5 /5, loss: 0.329873, loss_dice: 0.157718, sen_v: 0.772499, acc_v: 0.945670, spec_v: 0.970294, const time: 3.595803\n",
            "lr =  0.0019786342135333077\n",
            "epoch 65 /1001,epoch 5 /5, loss: 0.317679, loss_dice: 0.155757, sen_v: 0.759860, acc_v: 0.949361, spec_v: 0.973000, const time: 3.867410\n",
            "lr =  0.001977984265877415\n",
            "epoch 66 /1001,epoch 5 /5, loss: 0.324358, loss_dice: 0.155537, sen_v: 0.750004, acc_v: 0.947045, spec_v: 0.973234, const time: 3.484134\n",
            "lr =  0.001977324690112275\n",
            "epoch 67 /1001,epoch 5 /5, loss: 0.332897, loss_dice: 0.161805, sen_v: 0.781637, acc_v: 0.945586, spec_v: 0.965967, const time: 3.539226\n",
            "lr =  0.0019766554927346342\n",
            "epoch 68 /1001,epoch 5 /5, loss: 0.319791, loss_dice: 0.157204, sen_v: 0.739081, acc_v: 0.949746, spec_v: 0.975665, const time: 3.844551\n",
            "lr =  0.0019759766803360114\n",
            "epoch 69 /1001,epoch 5 /5, loss: 0.322488, loss_dice: 0.163108, sen_v: 0.744484, acc_v: 0.950189, spec_v: 0.973875, const time: 3.731992\n",
            "lr =  0.0019752882596026314\n",
            "epoch 70 /1001,epoch 5 /5, loss: 0.326669, loss_dice: 0.155349, sen_v: 0.762505, acc_v: 0.945783, spec_v: 0.970513, const time: 3.506180\n",
            "lr =  0.001974590237315361\n",
            "epoch 71 /1001,epoch 5 /5, loss: 0.311788, loss_dice: 0.147287, sen_v: 0.784552, acc_v: 0.948023, spec_v: 0.967256, const time: 3.499449\n",
            "lr =  0.00197388262034964\n",
            "epoch 72 /1001,epoch 5 /5, loss: 0.325561, loss_dice: 0.163743, sen_v: 0.731362, acc_v: 0.949309, spec_v: 0.976717, const time: 3.941768\n",
            "lr =  0.0019731654156754153\n",
            "epoch 73 /1001,epoch 5 /5, loss: 0.312806, loss_dice: 0.146850, sen_v: 0.786828, acc_v: 0.946966, spec_v: 0.968925, const time: 3.609376\n",
            "lr =  0.001972438630357072\n",
            "epoch 74 /1001,epoch 5 /5, loss: 0.310784, loss_dice: 0.148504, sen_v: 0.793783, acc_v: 0.947998, spec_v: 0.967038, const time: 3.506034\n",
            "lr =  0.0019717022715533632\n",
            "epoch 75 /1001,epoch 5 /5, loss: 0.326865, loss_dice: 0.163956, sen_v: 0.722328, acc_v: 0.949417, spec_v: 0.978323, const time: 3.556051\n",
            "lr =  0.0019709563465173394\n",
            "epoch 76 /1001,epoch 5 /5, loss: 0.324478, loss_dice: 0.150608, sen_v: 0.804963, acc_v: 0.943582, spec_v: 0.962832, const time: 3.977212\n",
            "lr =  0.001970200862596278\n",
            "epoch 77 /1001,epoch 5 /5, loss: 0.327143, loss_dice: 0.154810, sen_v: 0.780132, acc_v: 0.945987, spec_v: 0.968252, const time: 3.529299\n",
            "lr =  0.0019694358272316095\n",
            "epoch 78 /1001,epoch 5 /5, loss: 0.322058, loss_dice: 0.161540, sen_v: 0.753667, acc_v: 0.948839, spec_v: 0.972513, const time: 3.516132\n",
            "lr =  0.0019686612479588458\n",
            "epoch 79 /1001,epoch 5 /5, loss: 0.305248, loss_dice: 0.147303, sen_v: 0.772693, acc_v: 0.949996, spec_v: 0.972539, const time: 3.598281\n",
            "lr =  0.0019678771324075054\n",
            "epoch 80 /1001,epoch 5 /5, loss: 0.325065, loss_dice: 0.161446, sen_v: 0.751656, acc_v: 0.948544, spec_v: 0.972729, const time: 3.991022\n",
            "lr =  0.001967083488301037\n",
            "epoch 81 /1001,epoch 5 /5, loss: 0.311156, loss_dice: 0.154412, sen_v: 0.768720, acc_v: 0.950757, spec_v: 0.973343, const time: 3.513546\n",
            "lr =  0.0019662803234567453\n",
            "epoch 82 /1001,epoch 5 /5, loss: 0.304092, loss_dice: 0.146108, sen_v: 0.783526, acc_v: 0.949224, spec_v: 0.968956, const time: 3.573920\n",
            "lr =  0.001965467645785713\n",
            "epoch 83 /1001,epoch 5 /5, loss: 0.305643, loss_dice: 0.152504, sen_v: 0.759344, acc_v: 0.952004, spec_v: 0.973841, const time: 3.715961\n",
            "lr =  0.001964645463292723\n",
            "epoch 84 /1001,epoch 5 /5, loss: 0.310891, loss_dice: 0.149671, sen_v: 0.752641, acc_v: 0.949103, spec_v: 0.975966, const time: 3.830262\n",
            "lr =  0.00196381378407618\n",
            "epoch 85 /1001,epoch 5 /5, loss: 0.318576, loss_dice: 0.150957, sen_v: 0.809244, acc_v: 0.946360, spec_v: 0.963981, const time: 3.497303\n",
            "lr =  0.0019629726163280297\n",
            "epoch 86 /1001,epoch 5 /5, loss: 0.300805, loss_dice: 0.147276, sen_v: 0.756091, acc_v: 0.951317, spec_v: 0.974623, const time: 3.524519\n",
            "lr =  0.001962121968333679\n",
            "epoch 87 /1001,epoch 5 /5, loss: 0.295847, loss_dice: 0.143402, sen_v: 0.772264, acc_v: 0.952005, spec_v: 0.975660, const time: 3.761272\n",
            "lr =  0.001961261848471915\n",
            "epoch 88 /1001,epoch 5 /5, loss: 0.310627, loss_dice: 0.147599, sen_v: 0.799621, acc_v: 0.947492, spec_v: 0.965528, const time: 3.650216\n",
            "lr =  0.00196039226521482\n",
            "epoch 89 /1001,epoch 5 /5, loss: 0.299178, loss_dice: 0.144852, sen_v: 0.762706, acc_v: 0.950695, spec_v: 0.975319, const time: 3.508833\n",
            "lr =  0.001959513227127691\n",
            "epoch 90 /1001,epoch 5 /5, loss: 0.323141, loss_dice: 0.148896, sen_v: 0.770757, acc_v: 0.944883, spec_v: 0.971041, const time: 3.562167\n",
            "lr =  0.001958624742868953\n",
            "epoch 91 /1001,epoch 5 /5, loss: 0.305519, loss_dice: 0.145116, sen_v: 0.801155, acc_v: 0.948556, spec_v: 0.966331, const time: 3.908006\n",
            "lr =  0.001957726821190076\n",
            "epoch 92 /1001,epoch 5 /5, loss: 0.303149, loss_dice: 0.149008, sen_v: 0.756437, acc_v: 0.950755, spec_v: 0.975223, const time: 3.489500\n",
            "lr =  0.001956819470935486\n",
            "epoch 93 /1001,epoch 5 /5, loss: 0.287793, loss_dice: 0.135401, sen_v: 0.789344, acc_v: 0.951304, spec_v: 0.973764, const time: 3.549932\n",
            "lr =  0.0019559027010424813\n",
            "epoch 94 /1001,epoch 5 /5, loss: 0.319079, loss_dice: 0.154151, sen_v: 0.787616, acc_v: 0.947660, spec_v: 0.966799, const time: 3.599282\n",
            "lr =  0.0019549765205411403\n",
            "epoch 95 /1001,epoch 5 /5, loss: 0.320814, loss_dice: 0.156059, sen_v: 0.759547, acc_v: 0.948360, spec_v: 0.972718, const time: 3.950360\n",
            "lr =  0.0019540409385542367\n",
            "epoch 96 /1001,epoch 5 /5, loss: 0.316158, loss_dice: 0.152668, sen_v: 0.759374, acc_v: 0.947711, spec_v: 0.974437, const time: 3.547353\n",
            "lr =  0.0019530959642971464\n",
            "epoch 97 /1001,epoch 5 /5, loss: 0.288685, loss_dice: 0.134107, sen_v: 0.803530, acc_v: 0.950801, spec_v: 0.971323, const time: 3.512280\n",
            "lr =  0.0019521416070777593\n",
            "epoch 98 /1001,epoch 5 /5, loss: 0.309298, loss_dice: 0.148392, sen_v: 0.774943, acc_v: 0.948926, spec_v: 0.970804, const time: 3.585227\n",
            "lr =  0.0019511778762963858\n",
            "epoch 99 /1001,epoch 5 /5, loss: 0.297047, loss_dice: 0.146840, sen_v: 0.765231, acc_v: 0.952139, spec_v: 0.975019, const time: 3.899754\n",
            "lr =  0.001950204781445665\n",
            "epoch 100 /1001,epoch 5 /5, loss: 0.300595, loss_dice: 0.142973, sen_v: 0.787100, acc_v: 0.949091, spec_v: 0.970049, const time: 3.517157\n",
            "lr =  0.0019492223321104707\n",
            "epoch 101 /1001,epoch 5 /5, loss: 0.326970, loss_dice: 0.165664, sen_v: 0.749848, acc_v: 0.949370, spec_v: 0.971580, const time: 3.477049\n",
            "lr =  0.0019482305379678172\n",
            "epoch 102 /1001,epoch 5 /5, loss: 0.321153, loss_dice: 0.158290, sen_v: 0.750484, acc_v: 0.948334, spec_v: 0.974215, const time: 3.639976\n",
            "lr =  0.0019472294087867648\n",
            "epoch 103 /1001,epoch 5 /5, loss: 0.308443, loss_dice: 0.147843, sen_v: 0.778255, acc_v: 0.948720, spec_v: 0.969273, const time: 3.798866\n",
            "lr =  0.0019462189544283226\n",
            "epoch 104 /1001,epoch 5 /5, loss: 0.328426, loss_dice: 0.160145, sen_v: 0.755623, acc_v: 0.946909, spec_v: 0.970726, const time: 3.524867\n",
            "lr =  0.0019451991848453514\n",
            "epoch 105 /1001,epoch 5 /5, loss: 0.306950, loss_dice: 0.151016, sen_v: 0.766021, acc_v: 0.950389, spec_v: 0.973444, const time: 3.560304\n",
            "lr =  0.0019441701100824656\n",
            "epoch 106 /1001,epoch 5 /5, loss: 0.313217, loss_dice: 0.151964, sen_v: 0.774595, acc_v: 0.948876, spec_v: 0.970567, const time: 3.783861\n",
            "lr =  0.0019431317402759355\n",
            "epoch 107 /1001,epoch 5 /5, loss: 0.303502, loss_dice: 0.148010, sen_v: 0.777732, acc_v: 0.949615, spec_v: 0.971227, const time: 3.761772\n",
            "lr =  0.0019420840856535857\n",
            "epoch 108 /1001,epoch 5 /5, loss: 0.306055, loss_dice: 0.149796, sen_v: 0.771374, acc_v: 0.949954, spec_v: 0.971779, const time: 3.473738\n",
            "lr =  0.001941027156534696\n",
            "epoch 109 /1001,epoch 5 /5, loss: 0.293890, loss_dice: 0.140416, sen_v: 0.772044, acc_v: 0.951381, spec_v: 0.972975, const time: 3.549707\n",
            "lr =  0.001939960963329898\n",
            "epoch 110 /1001,epoch 5 /5, loss: 0.294052, loss_dice: 0.140760, sen_v: 0.797069, acc_v: 0.950138, spec_v: 0.970492, const time: 3.767100\n",
            "lr =  0.001938885516541074\n",
            "epoch 111 /1001,epoch 5 /5, loss: 0.293896, loss_dice: 0.141018, sen_v: 0.776990, acc_v: 0.951476, spec_v: 0.974682, const time: 3.549757\n",
            "lr =  0.0019378008267612534\n",
            "epoch 112 /1001,epoch 5 /5, loss: 0.299866, loss_dice: 0.146024, sen_v: 0.783378, acc_v: 0.951013, spec_v: 0.972293, const time: 3.500924\n",
            "lr =  0.001936706904674507\n",
            "epoch 113 /1001,epoch 5 /5, loss: 0.299486, loss_dice: 0.145893, sen_v: 0.783246, acc_v: 0.950465, spec_v: 0.971104, const time: 3.501521\n",
            "lr =  0.0019356037610558437\n",
            "epoch 114 /1001,epoch 5 /5, loss: 0.298106, loss_dice: 0.145270, sen_v: 0.776514, acc_v: 0.951962, spec_v: 0.974393, const time: 3.976318\n",
            "lr =  0.0019344914067711033\n",
            "epoch 115 /1001,epoch 5 /5, loss: 0.288640, loss_dice: 0.134658, sen_v: 0.791831, acc_v: 0.950393, spec_v: 0.971927, const time: 3.584294\n",
            "lr =  0.001933369852776849\n",
            "epoch 116 /1001,epoch 5 /5, loss: 0.306535, loss_dice: 0.145854, sen_v: 0.784599, acc_v: 0.949078, spec_v: 0.970882, const time: 3.473932\n",
            "lr =  0.0019322391101202609\n",
            "epoch 117 /1001,epoch 5 /5, loss: 0.301134, loss_dice: 0.140645, sen_v: 0.773496, acc_v: 0.948508, spec_v: 0.971885, const time: 3.484436\n",
            "lr =  0.0019310991899390258\n",
            "epoch 118 /1001,epoch 5 /5, loss: 0.289792, loss_dice: 0.138607, sen_v: 0.786247, acc_v: 0.951055, spec_v: 0.971985, const time: 3.957398\n",
            "lr =  0.0019299501034612287\n",
            "epoch 119 /1001,epoch 5 /5, loss: 0.300477, loss_dice: 0.140754, sen_v: 0.781560, acc_v: 0.948655, spec_v: 0.970345, const time: 3.493484\n",
            "lr =  0.0019287918620052406\n",
            "epoch 120 /1001,epoch 5 /5, loss: 0.297993, loss_dice: 0.145572, sen_v: 0.778037, acc_v: 0.951753, spec_v: 0.972407, const time: 3.509641\n",
            "lr =  0.0019276244769796092\n",
            "epoch 121 /1001,epoch 5 /5, loss: 0.302842, loss_dice: 0.147643, sen_v: 0.752643, acc_v: 0.950842, spec_v: 0.975216, const time: 3.545074\n",
            "lr =  0.0019264479598829444\n",
            "epoch 122 /1001,epoch 5 /5, loss: 0.297618, loss_dice: 0.145722, sen_v: 0.781310, acc_v: 0.951531, spec_v: 0.971274, const time: 3.929921\n",
            "lr =  0.0019252623223038059\n",
            "epoch 123 /1001,epoch 5 /5, loss: 0.295702, loss_dice: 0.145422, sen_v: 0.774630, acc_v: 0.951893, spec_v: 0.972090, const time: 3.533057\n",
            "lr =  0.0019240675759205903\n",
            "epoch 124 /1001,epoch 5 /5, loss: 0.298624, loss_dice: 0.143379, sen_v: 0.755821, acc_v: 0.950537, spec_v: 0.974445, const time: 3.554694\n",
            "lr =  0.0019228637325014134\n",
            "epoch 125 /1001,epoch 5 /5, loss: 0.305927, loss_dice: 0.144010, sen_v: 0.800847, acc_v: 0.947825, spec_v: 0.966424, const time: 3.712094\n",
            "lr =  0.0019216508039039965\n",
            "epoch 126 /1001,epoch 5 /5, loss: 0.300204, loss_dice: 0.146765, sen_v: 0.759890, acc_v: 0.950626, spec_v: 0.973103, const time: 3.736419\n",
            "lr =  0.0019204288020755495\n",
            "epoch 127 /1001,epoch 5 /5, loss: 0.305334, loss_dice: 0.147506, sen_v: 0.766167, acc_v: 0.949249, spec_v: 0.973688, const time: 3.521682\n",
            "lr =  0.0019191977390526514\n",
            "epoch 128 /1001,epoch 5 /5, loss: 0.311120, loss_dice: 0.146427, sen_v: 0.793516, acc_v: 0.946601, spec_v: 0.965003, const time: 3.492336\n",
            "lr =  0.0019179576269611335\n",
            "epoch 129 /1001,epoch 5 /5, loss: 0.305139, loss_dice: 0.150100, sen_v: 0.766100, acc_v: 0.950782, spec_v: 0.973805, const time: 3.826376\n",
            "lr =  0.0019167084780159593\n",
            "epoch 130 /1001,epoch 5 /5, loss: 0.299273, loss_dice: 0.143642, sen_v: 0.783070, acc_v: 0.949858, spec_v: 0.971802, const time: 3.741973\n",
            "lr =  0.0019154503045211047\n",
            "epoch 131 /1001,epoch 5 /5, loss: 0.299489, loss_dice: 0.143690, sen_v: 0.781707, acc_v: 0.949522, spec_v: 0.971588, const time: 3.550423\n",
            "lr =  0.0019141831188694356\n",
            "epoch 132 /1001,epoch 5 /5, loss: 0.294414, loss_dice: 0.141466, sen_v: 0.793320, acc_v: 0.949535, spec_v: 0.969263, const time: 3.470342\n",
            "lr =  0.0019129069335425874\n",
            "epoch 133 /1001,epoch 5 /5, loss: 0.295952, loss_dice: 0.145177, sen_v: 0.770435, acc_v: 0.951655, spec_v: 0.973705, const time: 3.834579\n",
            "lr =  0.0019116217611108404\n",
            "epoch 134 /1001,epoch 5 /5, loss: 0.289812, loss_dice: 0.140278, sen_v: 0.779333, acc_v: 0.951600, spec_v: 0.971530, const time: 3.556057\n",
            "lr =  0.001910327614232998\n",
            "epoch 135 /1001,epoch 5 /5, loss: 0.296902, loss_dice: 0.142869, sen_v: 0.773878, acc_v: 0.950231, spec_v: 0.972432, const time: 3.527893\n",
            "lr =  0.0019090245056562606\n",
            "epoch 136 /1001,epoch 5 /5, loss: 0.288545, loss_dice: 0.140197, sen_v: 0.777361, acc_v: 0.952276, spec_v: 0.973517, const time: 3.490317\n",
            "lr =  0.0019077124482160992\n",
            "epoch 137 /1001,epoch 5 /5, loss: 0.301231, loss_dice: 0.143938, sen_v: 0.789107, acc_v: 0.949396, spec_v: 0.969661, const time: 4.018237\n",
            "lr =  0.0019063914548361316\n",
            "epoch 138 /1001,epoch 5 /5, loss: 0.308199, loss_dice: 0.151058, sen_v: 0.761502, acc_v: 0.949950, spec_v: 0.972988, const time: 3.493046\n",
            "lr =  0.001905061538527992\n",
            "epoch 139 /1001,epoch 5 /5, loss: 0.306391, loss_dice: 0.147790, sen_v: 0.777523, acc_v: 0.948660, spec_v: 0.970722, const time: 3.575390\n",
            "lr =  0.0019037227123912062\n",
            "epoch 140 /1001,epoch 5 /5, loss: 0.294209, loss_dice: 0.141713, sen_v: 0.786123, acc_v: 0.950897, spec_v: 0.972454, const time: 3.609414\n",
            "lr =  0.0019023749896130595\n",
            "epoch 141 /1001,epoch 5 /5, loss: 0.304948, loss_dice: 0.149197, sen_v: 0.774370, acc_v: 0.950987, spec_v: 0.972110, const time: 3.973983\n",
            "lr =  0.0019010183834684685\n",
            "epoch 142 /1001,epoch 5 /5, loss: 0.288833, loss_dice: 0.138891, sen_v: 0.798126, acc_v: 0.952038, spec_v: 0.971837, const time: 3.503322\n",
            "lr =  0.0018996529073198503\n",
            "epoch 143 /1001,epoch 5 /5, loss: 0.292675, loss_dice: 0.140544, sen_v: 0.787964, acc_v: 0.951127, spec_v: 0.971577, const time: 3.469617\n",
            "lr =  0.00189827857461699\n",
            "epoch 144 /1001,epoch 5 /5, loss: 0.281201, loss_dice: 0.134075, sen_v: 0.778415, acc_v: 0.952914, spec_v: 0.975187, const time: 3.599766\n",
            "lr =  0.001896895398896909\n",
            "epoch 145 /1001,epoch 5 /5, loss: 0.284564, loss_dice: 0.137169, sen_v: 0.795792, acc_v: 0.952484, spec_v: 0.971152, const time: 3.949242\n",
            "lr =  0.0018955033937837313\n",
            "epoch 146 /1001,epoch 5 /5, loss: 0.299760, loss_dice: 0.147726, sen_v: 0.773036, acc_v: 0.951750, spec_v: 0.973894, const time: 3.529564\n",
            "lr =  0.0018941025729885496\n",
            "epoch 147 /1001,epoch 5 /5, loss: 0.302986, loss_dice: 0.147027, sen_v: 0.762181, acc_v: 0.949647, spec_v: 0.973372, const time: 3.471671\n",
            "lr =  0.0018926929503092903\n",
            "epoch 148 /1001,epoch 5 /5, loss: 0.294012, loss_dice: 0.136810, sen_v: 0.798432, acc_v: 0.949117, spec_v: 0.968751, const time: 3.647490\n",
            "lr =  0.0018912745396305764\n",
            "epoch 149 /1001,epoch 5 /5, loss: 0.277934, loss_dice: 0.136349, sen_v: 0.792366, acc_v: 0.954424, spec_v: 0.972645, const time: 3.899127\n",
            "lr =  0.001889847354923593\n",
            "epoch 150 /1001,epoch 5 /5, loss: 0.284141, loss_dice: 0.137902, sen_v: 0.776962, acc_v: 0.952658, spec_v: 0.976203, const time: 3.516415\n",
            "lr =  0.0018884114102459472\n",
            "epoch 151 /1001,epoch 5 /5, loss: 0.286638, loss_dice: 0.132794, sen_v: 0.803403, acc_v: 0.950352, spec_v: 0.969939, const time: 3.558396\n",
            "lr =  0.001886966719741532\n",
            "epoch 152 /1001,epoch 5 /5, loss: 0.289993, loss_dice: 0.138066, sen_v: 0.786122, acc_v: 0.951532, spec_v: 0.972485, const time: 3.603299\n",
            "lr =  0.001885513297640384\n",
            "epoch 153 /1001,epoch 5 /5, loss: 0.286762, loss_dice: 0.134709, sen_v: 0.788748, acc_v: 0.950971, spec_v: 0.972881, const time: 3.816750\n",
            "lr =  0.0018840511582585474\n",
            "epoch 154 /1001,epoch 5 /5, loss: 0.284962, loss_dice: 0.135175, sen_v: 0.788660, acc_v: 0.951531, spec_v: 0.971962, const time: 3.530869\n",
            "lr =  0.0018825803159979288\n",
            "epoch 155 /1001,epoch 5 /5, loss: 0.289111, loss_dice: 0.138644, sen_v: 0.793117, acc_v: 0.951359, spec_v: 0.971389, const time: 3.457799\n",
            "lr =  0.001881100785346158\n",
            "epoch 156 /1001,epoch 5 /5, loss: 0.284558, loss_dice: 0.136979, sen_v: 0.779571, acc_v: 0.952088, spec_v: 0.973785, const time: 3.819773\n",
            "lr =  0.001879612580876444\n",
            "epoch 157 /1001,epoch 5 /5, loss: 0.288617, loss_dice: 0.139813, sen_v: 0.787616, acc_v: 0.951771, spec_v: 0.972203, const time: 3.670017\n",
            "lr =  0.0018781157172474328\n",
            "epoch 158 /1001,epoch 5 /5, loss: 0.298086, loss_dice: 0.147505, sen_v: 0.777438, acc_v: 0.951684, spec_v: 0.971094, const time: 3.539196\n",
            "lr =  0.0018766102092030608\n",
            "epoch 159 /1001,epoch 5 /5, loss: 0.279004, loss_dice: 0.133759, sen_v: 0.779069, acc_v: 0.952947, spec_v: 0.976558, const time: 3.425230\n",
            "lr =  0.001875096071572412\n",
            "epoch 160 /1001,epoch 5 /5, loss: 0.273546, loss_dice: 0.129044, sen_v: 0.809478, acc_v: 0.952604, spec_v: 0.969643, const time: 3.690723\n",
            "lr =  0.0018735733192695703\n",
            "epoch 161 /1001,epoch 5 /5, loss: 0.271470, loss_dice: 0.129840, sen_v: 0.802310, acc_v: 0.953748, spec_v: 0.973449, const time: 3.783885\n",
            "lr =  0.0018720419672934735\n",
            "epoch 162 /1001,epoch 5 /5, loss: 0.287416, loss_dice: 0.140905, sen_v: 0.787034, acc_v: 0.952754, spec_v: 0.973623, const time: 3.478571\n",
            "lr =  0.0018705020307277645\n",
            "epoch 163 /1001,epoch 5 /5, loss: 0.289225, loss_dice: 0.137838, sen_v: 0.791447, acc_v: 0.950785, spec_v: 0.971458, const time: 3.546275\n",
            "lr =  0.001868953524740645\n",
            "epoch 164 /1001,epoch 5 /5, loss: 0.286733, loss_dice: 0.136613, sen_v: 0.803446, acc_v: 0.950600, spec_v: 0.969407, const time: 3.859165\n",
            "lr =  0.0018673964645847226\n",
            "epoch 165 /1001,epoch 5 /5, loss: 0.286055, loss_dice: 0.143695, sen_v: 0.768367, acc_v: 0.955541, spec_v: 0.977504, const time: 3.552007\n",
            "lr =  0.0018658308655968637\n",
            "epoch 166 /1001,epoch 5 /5, loss: 0.281688, loss_dice: 0.136785, sen_v: 0.785733, acc_v: 0.952940, spec_v: 0.974485, const time: 3.505212\n",
            "lr =  0.001864256743198041\n",
            "epoch 167 /1001,epoch 5 /5, loss: 0.281151, loss_dice: 0.130403, sen_v: 0.803251, acc_v: 0.951130, spec_v: 0.970274, const time: 3.560207\n",
            "lr =  0.001862674112893181\n",
            "epoch 168 /1001,epoch 5 /5, loss: 0.286308, loss_dice: 0.138744, sen_v: 0.790881, acc_v: 0.952654, spec_v: 0.972168, const time: 3.963637\n",
            "lr =  0.001861082990271014\n",
            "epoch 169 /1001,epoch 5 /5, loss: 0.285169, loss_dice: 0.138768, sen_v: 0.781053, acc_v: 0.953127, spec_v: 0.976492, const time: 3.607638\n",
            "lr =  0.0018594833910039168\n",
            "epoch 170 /1001,epoch 5 /5, loss: 0.297361, loss_dice: 0.141929, sen_v: 0.807869, acc_v: 0.949273, spec_v: 0.966862, const time: 3.457334\n",
            "lr =  0.0018578753308477612\n",
            "epoch 171 /1001,epoch 5 /5, loss: 0.297949, loss_dice: 0.147437, sen_v: 0.784411, acc_v: 0.950663, spec_v: 0.969094, const time: 3.516712\n",
            "lr =  0.0018562588256417572\n",
            "epoch 172 /1001,epoch 5 /5, loss: 0.283511, loss_dice: 0.139702, sen_v: 0.755646, acc_v: 0.954024, spec_v: 0.978822, const time: 4.012002\n",
            "lr =  0.0018546338913082985\n",
            "epoch 173 /1001,epoch 5 /5, loss: 0.307352, loss_dice: 0.151596, sen_v: 0.799275, acc_v: 0.950673, spec_v: 0.968758, const time: 3.638952\n",
            "lr =  0.0018530005438528035\n",
            "epoch 174 /1001,epoch 5 /5, loss: 0.290287, loss_dice: 0.142107, sen_v: 0.759269, acc_v: 0.951849, spec_v: 0.977212, const time: 3.469305\n",
            "lr =  0.00185135879936356\n",
            "epoch 175 /1001,epoch 5 /5, loss: 0.290557, loss_dice: 0.144171, sen_v: 0.778450, acc_v: 0.952722, spec_v: 0.972811, const time: 3.512296\n",
            "lr =  0.0018497086740115647\n",
            "epoch 176 /1001,epoch 5 /5, loss: 0.292263, loss_dice: 0.142967, sen_v: 0.796288, acc_v: 0.951543, spec_v: 0.971177, const time: 3.853649\n",
            "lr =  0.0018480501840503656\n",
            "epoch 177 /1001,epoch 5 /5, loss: 0.275224, loss_dice: 0.134624, sen_v: 0.782145, acc_v: 0.954205, spec_v: 0.974008, const time: 3.480936\n",
            "lr =  0.0018463833458159007\n",
            "epoch 178 /1001,epoch 5 /5, loss: 0.289029, loss_dice: 0.139728, sen_v: 0.776954, acc_v: 0.951584, spec_v: 0.972490, const time: 3.471914\n",
            "lr =  0.0018447081757263378\n",
            "epoch 179 /1001,epoch 5 /5, loss: 0.285968, loss_dice: 0.138451, sen_v: 0.779393, acc_v: 0.952274, spec_v: 0.972824, const time: 3.573452\n",
            "lr =  0.0018430246902819128\n",
            "epoch 180 /1001,epoch 5 /5, loss: 0.286425, loss_dice: 0.134121, sen_v: 0.800809, acc_v: 0.950357, spec_v: 0.970357, const time: 3.882028\n",
            "lr =  0.0018413329060647664\n",
            "epoch 181 /1001,epoch 5 /5, loss: 0.281705, loss_dice: 0.136839, sen_v: 0.787049, acc_v: 0.952981, spec_v: 0.973153, const time: 3.535368\n",
            "lr =  0.001839632839738781\n",
            "epoch 182 /1001,epoch 5 /5, loss: 0.288261, loss_dice: 0.137409, sen_v: 0.784667, acc_v: 0.951809, spec_v: 0.973427, const time: 3.530949\n",
            "lr =  0.0018379245080494175\n",
            "epoch 183 /1001,epoch 5 /5, loss: 0.289754, loss_dice: 0.142784, sen_v: 0.784108, acc_v: 0.952125, spec_v: 0.973544, const time: 3.758507\n",
            "lr =  0.0018362079278235496\n",
            "epoch 184 /1001,epoch 5 /5, loss: 0.275794, loss_dice: 0.127732, sen_v: 0.815535, acc_v: 0.951197, spec_v: 0.969608, const time: 3.829064\n",
            "lr =  0.0018344831159692978\n",
            "epoch 185 /1001,epoch 5 /5, loss: 0.279312, loss_dice: 0.130913, sen_v: 0.811142, acc_v: 0.951216, spec_v: 0.969219, const time: 3.534496\n",
            "lr =  0.0018327500894758633\n",
            "epoch 186 /1001,epoch 5 /5, loss: 0.287131, loss_dice: 0.140580, sen_v: 0.760766, acc_v: 0.953618, spec_v: 0.976797, const time: 3.478539\n",
            "lr =  0.001831008865413361\n",
            "epoch 187 /1001,epoch 5 /5, loss: 0.279784, loss_dice: 0.134857, sen_v: 0.800882, acc_v: 0.952511, spec_v: 0.971301, const time: 3.671923\n",
            "lr =  0.0018292594609326507\n",
            "epoch 188 /1001,epoch 5 /5, loss: 0.295866, loss_dice: 0.144084, sen_v: 0.785038, acc_v: 0.950578, spec_v: 0.970080, const time: 3.815807\n",
            "lr =  0.0018275018932651678\n",
            "epoch 189 /1001,epoch 5 /5, loss: 0.288354, loss_dice: 0.143194, sen_v: 0.775050, acc_v: 0.953400, spec_v: 0.973980, const time: 3.501694\n",
            "lr =  0.001825736179722755\n",
            "epoch 190 /1001,epoch 5 /5, loss: 0.283639, loss_dice: 0.143366, sen_v: 0.786944, acc_v: 0.954771, spec_v: 0.972667, const time: 3.516849\n",
            "lr =  0.0018239623376974901\n",
            "epoch 191 /1001,epoch 5 /5, loss: 0.285711, loss_dice: 0.141306, sen_v: 0.768657, acc_v: 0.953949, spec_v: 0.976430, const time: 3.713429\n",
            "lr =  0.001822180384661517\n",
            "epoch 192 /1001,epoch 5 /5, loss: 0.281671, loss_dice: 0.136609, sen_v: 0.791733, acc_v: 0.952595, spec_v: 0.972158, const time: 3.733856\n",
            "lr =  0.00182039033816687\n",
            "epoch 193 /1001,epoch 5 /5, loss: 0.295857, loss_dice: 0.141454, sen_v: 0.778934, acc_v: 0.950127, spec_v: 0.972984, const time: 3.529237\n",
            "lr =  0.0018185922158453051\n",
            "epoch 194 /1001,epoch 5 /5, loss: 0.298810, loss_dice: 0.144380, sen_v: 0.792216, acc_v: 0.949372, spec_v: 0.968726, const time: 3.537381\n",
            "lr =  0.0018167860354081232\n",
            "epoch 195 /1001,epoch 5 /5, loss: 0.306928, loss_dice: 0.146443, sen_v: 0.776461, acc_v: 0.948007, spec_v: 0.970474, const time: 3.890100\n",
            "lr =  0.0018149718146459964\n",
            "epoch 196 /1001,epoch 5 /5, loss: 0.294257, loss_dice: 0.144725, sen_v: 0.763282, acc_v: 0.951967, spec_v: 0.975425, const time: 3.697578\n",
            "lr =  0.001813149571428794\n",
            "epoch 197 /1001,epoch 5 /5, loss: 0.282156, loss_dice: 0.134796, sen_v: 0.789379, acc_v: 0.952446, spec_v: 0.973691, const time: 3.577618\n",
            "lr =  0.0018113193237054054\n",
            "epoch 198 /1001,epoch 5 /5, loss: 0.279150, loss_dice: 0.130149, sen_v: 0.803786, acc_v: 0.951257, spec_v: 0.969544, const time: 3.533007\n",
            "lr =  0.0018094810895035631\n",
            "epoch 199 /1001,epoch 5 /5, loss: 0.289462, loss_dice: 0.139110, sen_v: 0.781310, acc_v: 0.951547, spec_v: 0.972764, const time: 3.904286\n",
            "lr =  0.0018076348869296656\n",
            "epoch 200 /1001,epoch 5 /5, loss: 0.296344, loss_dice: 0.146004, sen_v: 0.770171, acc_v: 0.951798, spec_v: 0.973745, const time: 3.627325\n",
            "lr =  0.0018057807341685993\n",
            "epoch 201 /1001,epoch 5 /5, loss: 0.282109, loss_dice: 0.136789, sen_v: 0.794272, acc_v: 0.952571, spec_v: 0.971580, const time: 3.488964\n",
            "lr =  0.0018039186494835584\n",
            "epoch 202 /1001,epoch 5 /5, loss: 0.286738, loss_dice: 0.137709, sen_v: 0.786688, acc_v: 0.951543, spec_v: 0.972401, const time: 3.456856\n",
            "lr =  0.001802048651215866\n",
            "epoch 203 /1001,epoch 5 /5, loss: 0.273624, loss_dice: 0.128738, sen_v: 0.802226, acc_v: 0.953418, spec_v: 0.973819, const time: 3.924939\n",
            "lr =  0.0018001707577847935\n",
            "epoch 204 /1001,epoch 5 /5, loss: 0.289470, loss_dice: 0.138031, sen_v: 0.794469, acc_v: 0.950675, spec_v: 0.971771, const time: 3.500440\n",
            "lr =  0.001798284987687378\n",
            "epoch 205 /1001,epoch 5 /5, loss: 0.280035, loss_dice: 0.134320, sen_v: 0.811954, acc_v: 0.952045, spec_v: 0.970362, const time: 3.513564\n",
            "lr =  0.0017963913594982418\n",
            "epoch 206 /1001,epoch 5 /5, loss: 0.288148, loss_dice: 0.140989, sen_v: 0.771488, acc_v: 0.953107, spec_v: 0.975195, const time: 3.599292\n",
            "lr =  0.0017944898918694071\n",
            "epoch 207 /1001,epoch 5 /5, loss: 0.283787, loss_dice: 0.134119, sen_v: 0.791338, acc_v: 0.950641, spec_v: 0.971096, const time: 4.016807\n",
            "lr =  0.0017925806035301156\n",
            "epoch 208 /1001,epoch 5 /5, loss: 0.292281, loss_dice: 0.138747, sen_v: 0.791780, acc_v: 0.950135, spec_v: 0.970018, const time: 3.517404\n",
            "lr =  0.0017906635132866407\n",
            "epoch 209 /1001,epoch 5 /5, loss: 0.291230, loss_dice: 0.142483, sen_v: 0.785563, acc_v: 0.952114, spec_v: 0.972443, const time: 3.413081\n",
            "lr =  0.0017887386400221035\n",
            "epoch 210 /1001,epoch 5 /5, loss: 0.285240, loss_dice: 0.140878, sen_v: 0.785244, acc_v: 0.952757, spec_v: 0.972267, const time: 3.597142\n",
            "lr =  0.0017868060026962878\n",
            "epoch 211 /1001,epoch 5 /5, loss: 0.274680, loss_dice: 0.130319, sen_v: 0.772689, acc_v: 0.953520, spec_v: 0.978159, const time: 3.818403\n",
            "lr =  0.0017848656203454526\n",
            "epoch 212 /1001,epoch 5 /5, loss: 0.295649, loss_dice: 0.136449, sen_v: 0.818889, acc_v: 0.947868, spec_v: 0.964833, const time: 3.513007\n",
            "lr =  0.0017829175120821438\n",
            "epoch 213 /1001,epoch 5 /5, loss: 0.305436, loss_dice: 0.152202, sen_v: 0.759634, acc_v: 0.950508, spec_v: 0.973187, const time: 3.471370\n",
            "lr =  0.0017809616970950075\n",
            "epoch 214 /1001,epoch 5 /5, loss: 0.287576, loss_dice: 0.142908, sen_v: 0.767408, acc_v: 0.953062, spec_v: 0.975209, const time: 3.700750\n",
            "lr =  0.0017789981946485993\n",
            "epoch 215 /1001,epoch 5 /5, loss: 0.283400, loss_dice: 0.131817, sen_v: 0.815256, acc_v: 0.949677, spec_v: 0.967188, const time: 3.875378\n",
            "lr =  0.0017770270240831958\n",
            "epoch 216 /1001,epoch 5 /5, loss: 0.282168, loss_dice: 0.137134, sen_v: 0.788364, acc_v: 0.953138, spec_v: 0.972466, const time: 3.481184\n",
            "lr =  0.0017750482048146036\n",
            "epoch 217 /1001,epoch 5 /5, loss: 0.296188, loss_dice: 0.143902, sen_v: 0.771369, acc_v: 0.951275, spec_v: 0.973292, const time: 3.488558\n",
            "lr =  0.0017730617563339688\n",
            "epoch 218 /1001,epoch 5 /5, loss: 0.276313, loss_dice: 0.137596, sen_v: 0.792210, acc_v: 0.954116, spec_v: 0.972225, const time: 3.661759\n",
            "lr =  0.001771067698207583\n",
            "epoch 219 /1001,epoch 5 /5, loss: 0.277123, loss_dice: 0.134233, sen_v: 0.778658, acc_v: 0.953257, spec_v: 0.974794, const time: 3.833636\n",
            "lr =  0.0017690660500766934\n",
            "epoch 220 /1001,epoch 5 /5, loss: 0.286757, loss_dice: 0.134105, sen_v: 0.812138, acc_v: 0.949671, spec_v: 0.967968, const time: 3.457113\n",
            "lr =  0.0017670568316573067\n",
            "epoch 221 /1001,epoch 5 /5, loss: 0.290920, loss_dice: 0.144182, sen_v: 0.770150, acc_v: 0.952840, spec_v: 0.974869, const time: 3.591068\n",
            "lr =  0.001765040062739997\n",
            "epoch 222 /1001,epoch 5 /5, loss: 0.281557, loss_dice: 0.139118, sen_v: 0.782674, acc_v: 0.953456, spec_v: 0.973389, const time: 3.817625\n",
            "lr =  0.00176301576318971\n",
            "epoch 223 /1001,epoch 5 /5, loss: 0.296394, loss_dice: 0.144025, sen_v: 0.790821, acc_v: 0.949708, spec_v: 0.968813, const time: 3.727120\n",
            "lr =  0.0017609839529455662\n",
            "epoch 224 /1001,epoch 5 /5, loss: 0.280702, loss_dice: 0.135008, sen_v: 0.785713, acc_v: 0.952631, spec_v: 0.973863, const time: 3.498263\n",
            "lr =  0.0017589446520206663\n",
            "epoch 225 /1001,epoch 5 /5, loss: 0.274411, loss_dice: 0.131112, sen_v: 0.796266, acc_v: 0.953005, spec_v: 0.973046, const time: 3.485982\n",
            "lr =  0.0017568978805018937\n",
            "epoch 226 /1001,epoch 5 /5, loss: 0.269995, loss_dice: 0.127720, sen_v: 0.804509, acc_v: 0.953586, spec_v: 0.972334, const time: 3.789236\n",
            "lr =  0.0017548436585497157\n",
            "epoch 227 /1001,epoch 5 /5, loss: 0.283239, loss_dice: 0.138797, sen_v: 0.778712, acc_v: 0.953788, spec_v: 0.975037, const time: 3.694844\n",
            "lr =  0.0017527820063979857\n",
            "epoch 228 /1001,epoch 5 /5, loss: 0.287131, loss_dice: 0.139055, sen_v: 0.791483, acc_v: 0.951516, spec_v: 0.971400, const time: 3.481498\n",
            "lr =  0.0017507129443537438\n",
            "epoch 229 /1001,epoch 5 /5, loss: 0.270372, loss_dice: 0.132966, sen_v: 0.792492, acc_v: 0.955103, spec_v: 0.973962, const time: 3.450693\n",
            "lr =  0.0017486364927970168\n",
            "epoch 230 /1001,epoch 5 /5, loss: 0.282458, loss_dice: 0.137637, sen_v: 0.789335, acc_v: 0.952286, spec_v: 0.971492, const time: 3.807636\n",
            "lr =  0.001746552672180617\n",
            "epoch 231 /1001,epoch 5 /5, loss: 0.282883, loss_dice: 0.134550, sen_v: 0.800533, acc_v: 0.951267, spec_v: 0.971851, const time: 3.646882\n",
            "lr =  0.0017444615030299414\n",
            "epoch 232 /1001,epoch 5 /5, loss: 0.277946, loss_dice: 0.134686, sen_v: 0.790435, acc_v: 0.953881, spec_v: 0.974781, const time: 3.451439\n",
            "lr =  0.0017423630059427689\n",
            "epoch 233 /1001,epoch 5 /5, loss: 0.289800, loss_dice: 0.141440, sen_v: 0.772983, acc_v: 0.952306, spec_v: 0.973835, const time: 3.442181\n",
            "lr =  0.0017402572015890584\n",
            "epoch 234 /1001,epoch 5 /5, loss: 0.270234, loss_dice: 0.132231, sen_v: 0.809643, acc_v: 0.954887, spec_v: 0.972462, const time: 3.808971\n",
            "lr =  0.001738144110710744\n",
            "epoch 235 /1001,epoch 5 /5, loss: 0.271862, loss_dice: 0.130139, sen_v: 0.794951, acc_v: 0.953698, spec_v: 0.973450, const time: 3.618224\n",
            "lr =  0.0017360237541215314\n",
            "epoch 236 /1001,epoch 5 /5, loss: 0.283080, loss_dice: 0.137772, sen_v: 0.775190, acc_v: 0.952544, spec_v: 0.974762, const time: 3.544806\n",
            "lr =  0.0017338961527066927\n",
            "epoch 237 /1001,epoch 5 /5, loss: 0.279943, loss_dice: 0.134677, sen_v: 0.798543, acc_v: 0.952651, spec_v: 0.971935, const time: 3.495159\n",
            "lr =  0.001731761327422861\n",
            "epoch 238 /1001,epoch 5 /5, loss: 0.276641, loss_dice: 0.133176, sen_v: 0.797826, acc_v: 0.952998, spec_v: 0.971769, const time: 3.875504\n",
            "lr =  0.0017296192992978232\n",
            "epoch 239 /1001,epoch 5 /5, loss: 0.285149, loss_dice: 0.138475, sen_v: 0.772906, acc_v: 0.952865, spec_v: 0.975237, const time: 3.602510\n",
            "lr =  0.0017274700894303132\n",
            "epoch 240 /1001,epoch 5 /5, loss: 0.273654, loss_dice: 0.131535, sen_v: 0.814830, acc_v: 0.953101, spec_v: 0.970257, const time: 3.469444\n",
            "lr =  0.0017253137189898054\n",
            "epoch 241 /1001,epoch 5 /5, loss: 0.274466, loss_dice: 0.133081, sen_v: 0.789160, acc_v: 0.954167, spec_v: 0.973901, const time: 3.429340\n",
            "lr =  0.0017231502092163041\n",
            "epoch 242 /1001,epoch 5 /5, loss: 0.268696, loss_dice: 0.125661, sen_v: 0.800406, acc_v: 0.953567, spec_v: 0.973508, const time: 3.943149\n",
            "lr =  0.0017209795814201348\n",
            "epoch 243 /1001,epoch 5 /5, loss: 0.271122, loss_dice: 0.128471, sen_v: 0.806035, acc_v: 0.953083, spec_v: 0.970336, const time: 3.424449\n",
            "lr =  0.0017188018569817358\n",
            "epoch 244 /1001,epoch 5 /5, loss: 0.272857, loss_dice: 0.132882, sen_v: 0.789258, acc_v: 0.953830, spec_v: 0.973816, const time: 3.449566\n",
            "lr =  0.0017166170573514458\n",
            "epoch 245 /1001,epoch 5 /5, loss: 0.283603, loss_dice: 0.137697, sen_v: 0.784503, acc_v: 0.952306, spec_v: 0.973694, const time: 3.429353\n",
            "lr =  0.0017144252040492934\n",
            "epoch 246 /1001,epoch 5 /5, loss: 0.280140, loss_dice: 0.134069, sen_v: 0.804487, acc_v: 0.953227, spec_v: 0.971959, const time: 3.925681\n",
            "lr =  0.0017122263186647858\n",
            "epoch 247 /1001,epoch 5 /5, loss: 0.283797, loss_dice: 0.138954, sen_v: 0.780090, acc_v: 0.952871, spec_v: 0.973475, const time: 3.567163\n",
            "lr =  0.0017100204228566946\n",
            "epoch 248 /1001,epoch 5 /5, loss: 0.273719, loss_dice: 0.136409, sen_v: 0.773020, acc_v: 0.955345, spec_v: 0.975820, const time: 3.465686\n",
            "lr =  0.0017078075383528434\n",
            "epoch 249 /1001,epoch 5 /5, loss: 0.277243, loss_dice: 0.132860, sen_v: 0.793503, acc_v: 0.952813, spec_v: 0.972141, const time: 3.438634\n",
            "lr =  0.0017055876869498938\n",
            "epoch 250 /1001,epoch 5 /5, loss: 0.276690, loss_dice: 0.132212, sen_v: 0.812403, acc_v: 0.952262, spec_v: 0.970351, const time: 3.996798\n",
            "lr =  0.001703360890513131\n",
            "epoch 251 /1001,epoch 5 /5, loss: 0.283567, loss_dice: 0.140485, sen_v: 0.782469, acc_v: 0.952822, spec_v: 0.972938, const time: 3.450792\n",
            "lr =  0.001701127170976247\n",
            "epoch 252 /1001,epoch 5 /5, loss: 0.281779, loss_dice: 0.139192, sen_v: 0.774641, acc_v: 0.953861, spec_v: 0.975019, const time: 3.510310\n",
            "lr =  0.0016988865503411263\n",
            "epoch 253 /1001,epoch 5 /5, loss: 0.277358, loss_dice: 0.135336, sen_v: 0.795487, acc_v: 0.952773, spec_v: 0.971942, const time: 3.530592\n",
            "lr =  0.0016966390506776283\n",
            "epoch 254 /1001,epoch 5 /5, loss: 0.284682, loss_dice: 0.134928, sen_v: 0.794275, acc_v: 0.951120, spec_v: 0.971480, const time: 3.815017\n",
            "lr =  0.0016943846941233698\n",
            "epoch 255 /1001,epoch 5 /5, loss: 0.275974, loss_dice: 0.132277, sen_v: 0.796398, acc_v: 0.953202, spec_v: 0.973001, const time: 3.506570\n",
            "lr =  0.0016921235028835076\n",
            "epoch 256 /1001,epoch 5 /5, loss: 0.292684, loss_dice: 0.140958, sen_v: 0.786475, acc_v: 0.950224, spec_v: 0.972236, const time: 3.467556\n",
            "lr =  0.0016898554992305192\n",
            "epoch 257 /1001,epoch 5 /5, loss: 0.277281, loss_dice: 0.131879, sen_v: 0.805562, acc_v: 0.952720, spec_v: 0.971981, const time: 3.565897\n",
            "lr =  0.0016875807055039835\n",
            "epoch 258 /1001,epoch 5 /5, loss: 0.278814, loss_dice: 0.131506, sen_v: 0.790601, acc_v: 0.951554, spec_v: 0.971949, const time: 3.839348\n",
            "lr =  0.0016852991441103607\n",
            "epoch 259 /1001,epoch 5 /5, loss: 0.270571, loss_dice: 0.128494, sen_v: 0.805643, acc_v: 0.953906, spec_v: 0.972657, const time: 3.498425\n",
            "lr =  0.001683010837522772\n",
            "epoch 260 /1001,epoch 5 /5, loss: 0.277153, loss_dice: 0.132786, sen_v: 0.798321, acc_v: 0.953121, spec_v: 0.972368, const time: 3.426634\n",
            "lr =  0.0016807158082807784\n",
            "epoch 261 /1001,epoch 5 /5, loss: 0.278174, loss_dice: 0.135665, sen_v: 0.776922, acc_v: 0.952904, spec_v: 0.975265, const time: 3.674641\n",
            "lr =  0.0016784140789901578\n",
            "epoch 262 /1001,epoch 5 /5, loss: 0.279147, loss_dice: 0.135828, sen_v: 0.807074, acc_v: 0.952970, spec_v: 0.970122, const time: 3.740562\n",
            "lr =  0.0016761056723226823\n",
            "epoch 263 /1001,epoch 5 /5, loss: 0.270011, loss_dice: 0.130428, sen_v: 0.795967, acc_v: 0.954587, spec_v: 0.974457, const time: 3.469087\n",
            "lr =  0.0016737906110158964\n",
            "epoch 264 /1001,epoch 5 /5, loss: 0.280655, loss_dice: 0.136151, sen_v: 0.787107, acc_v: 0.952735, spec_v: 0.972868, const time: 3.475475\n",
            "lr =  0.0016714689178728914\n",
            "epoch 265 /1001,epoch 5 /5, loss: 0.274183, loss_dice: 0.130956, sen_v: 0.800899, acc_v: 0.952618, spec_v: 0.972432, const time: 3.743954\n",
            "lr =  0.0016691406157620821\n",
            "epoch 266 /1001,epoch 5 /5, loss: 0.280805, loss_dice: 0.134490, sen_v: 0.799622, acc_v: 0.952051, spec_v: 0.970686, const time: 3.691563\n",
            "lr =  0.00166680572761698\n",
            "epoch 267 /1001,epoch 5 /5, loss: 0.268540, loss_dice: 0.131134, sen_v: 0.788876, acc_v: 0.955168, spec_v: 0.976024, const time: 3.414860\n",
            "lr =  0.001664464276435969\n",
            "epoch 268 /1001,epoch 5 /5, loss: 0.276878, loss_dice: 0.132766, sen_v: 0.795134, acc_v: 0.952165, spec_v: 0.973160, const time: 3.488384\n",
            "lr =  0.0016621162852820776\n",
            "epoch 269 /1001,epoch 5 /5, loss: 0.278131, loss_dice: 0.132752, sen_v: 0.806406, acc_v: 0.952370, spec_v: 0.969905, const time: 3.860766\n",
            "lr =  0.001659761777282753\n",
            "epoch 270 /1001,epoch 5 /5, loss: 0.273601, loss_dice: 0.132573, sen_v: 0.780522, acc_v: 0.954192, spec_v: 0.974408, const time: 3.516582\n",
            "lr =  0.0016574007756296318\n",
            "epoch 271 /1001,epoch 5 /5, loss: 0.281949, loss_dice: 0.133362, sen_v: 0.788896, acc_v: 0.951645, spec_v: 0.973044, const time: 3.469717\n",
            "lr =  0.0016550333035783126\n",
            "epoch 272 /1001,epoch 5 /5, loss: 0.270469, loss_dice: 0.130099, sen_v: 0.811994, acc_v: 0.953324, spec_v: 0.970819, const time: 3.408262\n",
            "lr =  0.0016526593844481275\n",
            "epoch 273 /1001,epoch 5 /5, loss: 0.277558, loss_dice: 0.133184, sen_v: 0.792534, acc_v: 0.952643, spec_v: 0.973117, const time: 3.912429\n",
            "lr =  0.00165027904162191\n",
            "epoch 274 /1001,epoch 5 /5, loss: 0.279560, loss_dice: 0.135562, sen_v: 0.800769, acc_v: 0.952881, spec_v: 0.971585, const time: 3.666389\n",
            "lr =  0.0016478922985457682\n",
            "epoch 275 /1001,epoch 5 /5, loss: 0.278204, loss_dice: 0.135295, sen_v: 0.796923, acc_v: 0.952699, spec_v: 0.971972, const time: 3.449539\n",
            "lr =  0.0016454991787288502\n",
            "epoch 276 /1001,epoch 5 /5, loss: 0.266015, loss_dice: 0.128379, sen_v: 0.786474, acc_v: 0.955002, spec_v: 0.976619, const time: 3.463718\n",
            "lr =  0.0016430997057431153\n",
            "epoch 277 /1001,epoch 5 /5, loss: 0.287192, loss_dice: 0.139670, sen_v: 0.795493, acc_v: 0.952405, spec_v: 0.971349, const time: 4.001307\n",
            "lr =  0.0016406939032231\n",
            "epoch 278 /1001,epoch 5 /5, loss: 0.266210, loss_dice: 0.128190, sen_v: 0.800597, acc_v: 0.954435, spec_v: 0.973337, const time: 3.461135\n",
            "lr =  0.0016382817948656873\n",
            "epoch 279 /1001,epoch 5 /5, loss: 0.271851, loss_dice: 0.131507, sen_v: 0.794490, acc_v: 0.954060, spec_v: 0.972035, const time: 3.483888\n",
            "lr =  0.0016358634044298704\n",
            "epoch 280 /1001,epoch 5 /5, loss: 0.279539, loss_dice: 0.134475, sen_v: 0.791754, acc_v: 0.952202, spec_v: 0.972689, const time: 3.416940\n",
            "lr =  0.0016334387557365214\n",
            "epoch 281 /1001,epoch 5 /5, loss: 0.274983, loss_dice: 0.135875, sen_v: 0.776730, acc_v: 0.954433, spec_v: 0.975688, const time: 3.921632\n",
            "lr =  0.001631007872668155\n",
            "epoch 282 /1001,epoch 5 /5, loss: 0.270625, loss_dice: 0.128349, sen_v: 0.808966, acc_v: 0.953335, spec_v: 0.972063, const time: 3.438358\n",
            "lr =  0.0016285707791686946\n",
            "epoch 283 /1001,epoch 5 /5, loss: 0.272669, loss_dice: 0.129156, sen_v: 0.817747, acc_v: 0.952287, spec_v: 0.970382, const time: 3.493343\n",
            "lr =  0.0016261274992432342\n",
            "epoch 284 /1001,epoch 5 /5, loss: 0.262319, loss_dice: 0.123494, sen_v: 0.809955, acc_v: 0.954091, spec_v: 0.973158, const time: 3.424550\n",
            "lr =  0.0016236780569578045\n",
            "epoch 285 /1001,epoch 5 /5, loss: 0.270894, loss_dice: 0.130566, sen_v: 0.795824, acc_v: 0.953839, spec_v: 0.974054, const time: 4.002211\n",
            "lr =  0.0016212224764391343\n",
            "epoch 286 /1001,epoch 5 /5, loss: 0.269987, loss_dice: 0.127878, sen_v: 0.811838, acc_v: 0.953296, spec_v: 0.971322, const time: 3.461276\n",
            "lr =  0.0016187607818744136\n",
            "epoch 287 /1001,epoch 5 /5, loss: 0.276612, loss_dice: 0.134877, sen_v: 0.800247, acc_v: 0.953256, spec_v: 0.971119, const time: 3.490950\n",
            "lr =  0.0016162929975110552\n",
            "epoch 288 /1001,epoch 5 /5, loss: 0.280510, loss_dice: 0.143040, sen_v: 0.766124, acc_v: 0.955014, spec_v: 0.976438, const time: 3.482481\n",
            "lr =  0.0016138191476564546\n",
            "epoch 289 /1001,epoch 5 /5, loss: 0.262982, loss_dice: 0.123819, sen_v: 0.801761, acc_v: 0.954311, spec_v: 0.974299, const time: 3.988417\n",
            "lr =  0.0016113392566777529\n",
            "epoch 290 /1001,epoch 5 /5, loss: 0.270413, loss_dice: 0.126722, sen_v: 0.824485, acc_v: 0.952858, spec_v: 0.969081, const time: 3.417467\n",
            "lr =  0.0016088533490015958\n",
            "epoch 291 /1001,epoch 5 /5, loss: 0.279410, loss_dice: 0.135429, sen_v: 0.783922, acc_v: 0.953342, spec_v: 0.974060, const time: 3.430923\n",
            "lr =  0.001606361449113891\n",
            "epoch 292 /1001,epoch 5 /5, loss: 0.280510, loss_dice: 0.135397, sen_v: 0.773920, acc_v: 0.952495, spec_v: 0.973952, const time: 3.512979\n",
            "lr =  0.0016038635815595712\n",
            "epoch 293 /1001,epoch 5 /5, loss: 0.295609, loss_dice: 0.141901, sen_v: 0.803399, acc_v: 0.949352, spec_v: 0.967448, const time: 3.889169\n",
            "lr =  0.0016013597709423478\n",
            "epoch 294 /1001,epoch 5 /5, loss: 0.306770, loss_dice: 0.151139, sen_v: 0.759417, acc_v: 0.950499, spec_v: 0.974388, const time: 3.443971\n",
            "lr =  0.0015988500419244725\n",
            "epoch 295 /1001,epoch 5 /5, loss: 0.283598, loss_dice: 0.137588, sen_v: 0.785425, acc_v: 0.952137, spec_v: 0.973778, const time: 3.435391\n",
            "lr =  0.001596334419226491\n",
            "epoch 296 /1001,epoch 5 /5, loss: 0.282888, loss_dice: 0.132792, sen_v: 0.807139, acc_v: 0.950716, spec_v: 0.968751, const time: 3.661581\n",
            "lr =  0.0015938129276270021\n",
            "epoch 297 /1001,epoch 5 /5, loss: 0.273523, loss_dice: 0.130897, sen_v: 0.799764, acc_v: 0.953756, spec_v: 0.972751, const time: 3.747273\n",
            "lr =  0.0015912855919624123\n",
            "epoch 298 /1001,epoch 5 /5, loss: 0.275318, loss_dice: 0.134267, sen_v: 0.790680, acc_v: 0.953795, spec_v: 0.973975, const time: 3.405434\n",
            "lr =  0.0015887524371266915\n",
            "epoch 299 /1001,epoch 5 /5, loss: 0.278953, loss_dice: 0.134018, sen_v: 0.792095, acc_v: 0.952405, spec_v: 0.972598, const time: 3.448546\n",
            "lr =  0.001586213488071128\n",
            "epoch 300 /1001,epoch 5 /5, loss: 0.280152, loss_dice: 0.135052, sen_v: 0.795905, acc_v: 0.952457, spec_v: 0.972687, const time: 3.605044\n",
            "lr =  0.0015836687698040817\n",
            "epoch 301 /1001,epoch 5 /5, loss: 0.275693, loss_dice: 0.131162, sen_v: 0.792501, acc_v: 0.952467, spec_v: 0.972650, const time: 3.822111\n",
            "lr =  0.0015811183073907394\n",
            "epoch 302 /1001,epoch 5 /5, loss: 0.275603, loss_dice: 0.129394, sen_v: 0.798684, acc_v: 0.952096, spec_v: 0.972771, const time: 3.452691\n",
            "lr =  0.0015785621259528674\n",
            "epoch 303 /1001,epoch 5 /5, loss: 0.278049, loss_dice: 0.136068, sen_v: 0.811673, acc_v: 0.953123, spec_v: 0.969183, const time: 3.450769\n",
            "lr =  0.0015760002506685622\n",
            "epoch 304 /1001,epoch 5 /5, loss: 0.279718, loss_dice: 0.135044, sen_v: 0.769032, acc_v: 0.953376, spec_v: 0.976823, const time: 3.636173\n",
            "lr =  0.0015734327067720058\n",
            "epoch 305 /1001,epoch 5 /5, loss: 0.271928, loss_dice: 0.128636, sen_v: 0.799744, acc_v: 0.952887, spec_v: 0.972935, const time: 3.758430\n",
            "lr =  0.001570859519553214\n",
            "epoch 306 /1001,epoch 5 /5, loss: 0.273607, loss_dice: 0.124670, sen_v: 0.823642, acc_v: 0.950765, spec_v: 0.968052, const time: 3.457504\n",
            "lr =  0.00156828071435779\n",
            "epoch 307 /1001,epoch 5 /5, loss: 0.282873, loss_dice: 0.135545, sen_v: 0.797642, acc_v: 0.952267, spec_v: 0.971232, const time: 3.441421\n",
            "lr =  0.0015656963165866726\n",
            "epoch 308 /1001,epoch 5 /5, loss: 0.269663, loss_dice: 0.130891, sen_v: 0.774021, acc_v: 0.954896, spec_v: 0.977030, const time: 3.721541\n",
            "lr =  0.0015631063516958863\n",
            "epoch 309 /1001,epoch 5 /5, loss: 0.274422, loss_dice: 0.129666, sen_v: 0.815801, acc_v: 0.952092, spec_v: 0.968777, const time: 3.674191\n",
            "lr =  0.0015605108451962925\n",
            "epoch 310 /1001,epoch 5 /5, loss: 0.274703, loss_dice: 0.132282, sen_v: 0.806070, acc_v: 0.953236, spec_v: 0.972113, const time: 3.459509\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (2,192,192) into shape (192,192)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f9d0f0d7a2ae>\u001b[0m in \u001b[0;36m<cell line: 239>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-f9d0f0d7a2ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, io)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0mscore_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mww\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                     \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_label\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mloss_dice_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-0729cdb69a76>\u001b[0m in \u001b[0;36mmodel_forward\u001b[0;34m(model, test_data, patch_size, hh, ww, stride_y, stride_x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbsize\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mscore_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mcnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,192,192) into shape (192,192)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##main# This part is the last series of code in the \"main\" part"
      ],
      "metadata": {
        "id": "S2EUpxSVs9Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "#from data.load_train_data import TrainData, data_crop\n",
        "#from data.load_test_data import TestData\n",
        "from torch.utils.data import DataLoader\n",
        "import sklearn.metrics as metrics\n",
        "#from net.retinal_vasuclar_net import RetinalVasularSeg\n",
        "#from net.unetrpp_ff import RetinalVasularSegFF\n",
        "#from net.UNetFamily import U_Net, AttU_Net, Dense_Unet\n",
        "# #from net.Hessian import HessianNet\n",
        "#from net.loss import dice_loss, FocalLoss2d, WeightedFocalLoss, cal_sen\n",
        "#from test import model_forward\n",
        "#import config.config as cfg\n",
        "\n",
        "def model_initial(model, model_name):\n",
        "    # 加载预训练模型\n",
        "    pretrained_dict = torch.load(model_name)[\"model\"]\n",
        "    model_dict = model.state_dict()\n",
        "    # 1. filter out unnecessary keys\n",
        "    # pretrained_dictf = {k.replace('module.', \"\"): v for k, v in pretrained_dict.items() if k.replace('module.', \"\") in model_dict}\n",
        "    pretrained_dictf = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    # 2. overwrite entries in the existing state dict\n",
        "    model_dict.update(pretrained_dictf)\n",
        "    # 3. load the new state dict\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    print(\"over\")\n",
        "\n",
        "\n",
        "def _init_(exp_name):\n",
        "    if not os.path.exists('outputs'):\n",
        "        os.makedirs('outputs')\n",
        "    if not os.path.exists('./outputs/' + exp_name):\n",
        "        os.makedirs('./outputs/' + exp_name)\n",
        "    if not os.path.exists('./outputs/' + exp_name + '/' + 'models'):\n",
        "        os.makedirs('./outputs/' + exp_name + '/' + 'models')\n",
        "    os.system('cp main_cls.py outputs' + '/' + exp_name + '/' + 'main_cls.py.backup')\n",
        "    os.system('cp model.py outputs' + '/' + exp_name + '/' + 'model.py.backup')\n",
        "    os.system('cp util.py outputs' + '/' + exp_name + '/' + 'util.py.backup')\n",
        "    os.system('cp data.py outputs' + '/' + exp_name + '/' + 'data.py.backup')\n",
        "\n",
        "class IOStream():\n",
        "    def __init__(self, path):\n",
        "        self.f = open(path, 'a')\n",
        "\n",
        "    def cprint(self, text):\n",
        "        print(text)\n",
        "        self.f.write(text+'\\n')\n",
        "        self.f.flush()\n",
        "\n",
        "    def close(self):\n",
        "        self.f.close()\n",
        "\n",
        "def train(io):\n",
        "\n",
        "\n",
        "    batch_size = 4\n",
        "    test_batch_size = 1\n",
        "    epochs = 1001\n",
        "    lr = 2 * 1e-3\n",
        "    momentum = 0.9\n",
        "    scheduler = 'cos'\n",
        "    no_cuda = False\n",
        "\n",
        "    file_path = \"/content/training/images/\"\n",
        "    label_path = \"/content/training/1st_manual/\"\n",
        "    #file_path = \"/\"\n",
        "    #label_path = \"F:/DRIVE/training/1st_manual/\"\n",
        "    # file_path = \"E:/vasular/CHASEDB1/images/\"\n",
        "    # label_path = \"E:/vasular/CHASEDB1/1st_label/\"\n",
        "    # file_path = \"G:/vasular/STAREdatabase/images/\"\n",
        "    # label_path = \"G:/vasular/STAREdatabase/labels-ah/\"\n",
        "    # file_path = \"G:/vasular/HRFdatas/images/\"\n",
        "    # label_path = \"G:/vasular/HRFdatas/images/\"\n",
        "    train_loader = DataLoader(TrainData(file_path, label_path, train_flag = True), num_workers=0,\n",
        "                              batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    file_path = \"/content/test/images/\"\n",
        "    label_path = \"/content/test/1st_manual/\"\n",
        "    #file_path = \"F:/DRIVE/test/images/\"\n",
        "    #label_path = \"F:/DRIVE/test/1st_manual/\"\n",
        "    # file_path = \"E:/vasular/CHASEDB1/test/\"\n",
        "    # label_path = \"E:/vasular/CHASEDB1/1st_label/\"\n",
        "    # file_path = \"G:/vasular/STAREdatabase/test/\"\n",
        "    # label_path = \"G:/vasular/STAREdatabase/labels-ah/\"\n",
        "    # file_path = \"G:/vasular/HRFdatas/test/\"\n",
        "    # label_path = \"G:/vasular/HRFdatas/test/\"\n",
        "    test_loader = DataLoader(TestData(file_path, label_path, train_flag = False), num_workers=0,\n",
        "                             batch_size=test_batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    # Try to load models\n",
        "    model = RetinalVasularSegFF(in_channels=3,\n",
        "                            out_channels=2,\n",
        "                            img_size=[192, 192],\n",
        "                            feature_size=16,\n",
        "                            num_heads=4,\n",
        "                            depths=[3, 3, 3, 3],\n",
        "                            dims=[32, 64, 128, 256],\n",
        "                              hidden_size=256,\n",
        "                            do_ds=True,\n",
        "                            )\n",
        "\n",
        "    model_name = \"./outputs/teethseg_model_200.pth\"\n",
        "    # model_initial(model, model_name)\n",
        "\n",
        "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(opt, epochs, eta_min=1e-6, last_epoch = -1)\n",
        "\n",
        "    focalLoss = FocalLoss2d(gamma=2)#WeightedFocalLoss()#\n",
        "\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "    inter_nums = len(train_loader)\n",
        "    total_acc = 0\n",
        "    for epoch in range(0, epochs):\n",
        "        ####################\n",
        "        # Train\n",
        "        ####################\n",
        "\n",
        "        if scheduler == 'cos':\n",
        "            scheduler.step()\n",
        "        elif scheduler == 'step':\n",
        "            if opt.param_groups[0]['lr'] > 1e-5:\n",
        "                scheduler.step()\n",
        "            if opt.param_groups[0]['lr'] < 1e-5:\n",
        "                for param_group in opt.param_groups:\n",
        "                    param_group['lr'] = 1e-5\n",
        "\n",
        "        train_loss = 0.0\n",
        "        loss_dice = 0\n",
        "        sen_v = 0\n",
        "        acc_v = 0\n",
        "        spec_v = 0\n",
        "        # for data, edges, label in train_loader:\n",
        "        tic = time.time()\n",
        "        nums = 0\n",
        "        model.train()\n",
        "        for train_data, train_label, weight_ in train_loader:\n",
        "\n",
        "            train_data, train_label, train_weight_ = data_crop(train_data, train_label, weight_)\n",
        "\n",
        "            train_data = train_data.cuda().float()\n",
        "            train_label = train_label.cuda().float()\n",
        "            train_weight_ = train_weight_.cuda().float()\n",
        "\n",
        "            nums = nums +1\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                outputs_seg, out2, _ = model(train_data)\n",
        "\n",
        "                train_label[train_label >= 0.5] = 1\n",
        "                loss_seg = F.cross_entropy(outputs_seg, train_label.long())\n",
        "                # loss_seg = F.binary_cross_entropy_with_logits(outputs_seg[:,0, :, :], train_label)\n",
        "                sec_loss = focalLoss(outputs_seg, train_label[:, :, :].long())\n",
        "\n",
        "                outputs_soft = torch.softmax(outputs_seg, dim=1)[:, 1, :, :]#F.sigmoid(outputs_seg[:, 0, :, :])#\n",
        "                loss_dice_ = dice_loss(outputs_soft, train_label[:, :, :])\n",
        "\n",
        "                sen_v_, acc_v_, spec_v_ = cal_sen(outputs_soft, train_label[:, :, :].long())\n",
        "\n",
        "                loss = (loss_seg + 1*loss_dice_ + 2*sec_loss)*1  #\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            # Unscales gradients and calls\n",
        "            # or skips optimizer.step()\n",
        "            scaler.step(opt)\n",
        "            # Updates the scale for next iteration\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            loss_dice += loss_dice_.item()\n",
        "            sen_v += sen_v_.item()\n",
        "            acc_v += acc_v_.item()\n",
        "            spec_v += spec_v_.item()\n",
        "            VIEW_NUMS = 5\n",
        "            if nums % VIEW_NUMS == 0:\n",
        "                toc = time.time()\n",
        "                train_loss = train_loss/ (VIEW_NUMS)\n",
        "                loss_dice = loss_dice / (VIEW_NUMS)\n",
        "                sen_v = sen_v/(VIEW_NUMS)\n",
        "                acc_v = acc_v/(VIEW_NUMS)\n",
        "                spec_v = spec_v/(VIEW_NUMS)\n",
        "\n",
        "                print(\"lr = \", opt.param_groups[0]['lr'])\n",
        "                outstr = 'epoch %d /%d,epoch %d /%d, loss: %.6f, loss_dice: %.6f, sen_v: %.6f, acc_v: %.6f, spec_v: %.6f, const time: %.6f' % (\n",
        "                 epoch,epochs, nums, inter_nums, train_loss, loss_dice, sen_v, acc_v, spec_v, toc - tic)\n",
        "\n",
        "                io.cprint(outstr)\n",
        "                train_loss = 0.0\n",
        "                loss_dice = 0\n",
        "                sen_v = 0\n",
        "                acc_v = 0\n",
        "                spec_v = 0\n",
        "                tic = time.time()\n",
        "        if 0 == epoch % 10 and epoch>10:\n",
        "            test_nums = 0\n",
        "            CROP_SIZE = 192\n",
        "            loss_dice, sen_v, acc_v, spec_v =0, 0, 0, 0\n",
        "            model.eval()\n",
        "            patch_size = [CROP_SIZE, CROP_SIZE]\n",
        "            stride_y, stride_x = 128, 128\n",
        "            for test_data, test_label in test_loader:\n",
        "                test_data = test_data.cuda().float()\n",
        "                test_label = test_label.cuda().float()\n",
        "                nums = nums + 1\n",
        "                test_nums = test_nums + 1\n",
        "                hh, ww = test_label.shape[-2:]\n",
        "                with autocast():\n",
        "                    score_map = model_forward(model, test_data, patch_size, hh, ww, stride_y, stride_x)\n",
        "                    train_label[train_label > 0.5] = 1\n",
        "                    loss_dice_ = dice_loss(score_map, test_label.long())\n",
        "                    sen_v_, acc_v_, spec_v_ = cal_sen(score_map.detach(), test_label.detach().long())\n",
        "                loss = 1.0 * (loss_dice_).item()\n",
        "                sen_v = sen_v + sen_v_.item()\n",
        "                acc_v = acc_v + acc_v_.item()\n",
        "                spec_v = spec_v + spec_v_.item()\n",
        "                loss_dice = loss_dice + loss_dice_.item()\n",
        "            sen_v = sen_v/test_nums\n",
        "            acc_v = acc_v/test_nums\n",
        "            spec_v = spec_v/test_nums\n",
        "            loss_dice = loss_dice/test_nums\n",
        "            toc = time.time()\n",
        "            outstr = 'epoch %d, loss: %.6f, loss_dice: %.6f, sen_v: %.6f, acc_v: %.6f, spec_v: %.6f, const time: %.6f' % (\n",
        "                epoch, loss, loss_dice, sen_v, acc_v, spec_v, toc - tic)\n",
        "            io.cprint(\"test   \"+outstr)\n",
        "            if (total_acc < (sen_v*0.35+acc_v*0.35+spec_v*0.35)):\n",
        "                total_acc = (sen_v * 0.35 + acc_v * 0.35 + spec_v * 0.35)\n",
        "                torch.save({'model': model.state_dict(), 'epoch': epoch},\n",
        "                           'outputs/' + str(sen_v)+\"_\"+ str(acc_v)+ \"_\"+str(spec_v) + '.pth')\n",
        "        if (epoch) % 200 == 0:\n",
        "            torch.save({'model': model.state_dict(), 'epoch': epoch}, 'outputs/teethseg_model_' + str(epoch)+ '.pth')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    # Training settings\n",
        "\n",
        "    exp_name = 'retinal'\n",
        "    seed = 1\n",
        "\n",
        "    _init_(exp_name)\n",
        "\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    io = IOStream('outputs/' +exp_name + '/run.log')\n",
        "\n",
        "    train(io)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yJDeBSjWYTAP",
        "outputId": "5521a8a7-2368-4ecb-8804-ccb3f71a4d1a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr =  0.002\n",
            "epoch 0 /1001,epoch 5 /5, loss: 1.171121, loss_dice: 0.646048, sen_v: 0.079011, acc_v: 0.831452, spec_v: 0.926893, const time: 6.246006\n",
            "lr =  0.002\n",
            "epoch 1 /1001,epoch 5 /5, loss: 0.955764, loss_dice: 0.554631, sen_v: 0.152047, acc_v: 0.879853, spec_v: 0.973801, const time: 3.913092\n",
            "lr =  0.002\n",
            "epoch 2 /1001,epoch 5 /5, loss: 0.685662, loss_dice: 0.377291, sen_v: 0.482829, acc_v: 0.907636, spec_v: 0.960470, const time: 3.618625\n",
            "lr =  0.002\n",
            "epoch 3 /1001,epoch 5 /5, loss: 0.616338, loss_dice: 0.321664, sen_v: 0.521745, acc_v: 0.915105, spec_v: 0.965293, const time: 3.662418\n",
            "lr =  0.002\n",
            "epoch 4 /1001,epoch 5 /5, loss: 0.595184, loss_dice: 0.307318, sen_v: 0.601379, acc_v: 0.913610, spec_v: 0.952901, const time: 3.904668\n",
            "lr =  0.002\n",
            "epoch 5 /1001,epoch 5 /5, loss: 0.510428, loss_dice: 0.264044, sen_v: 0.624022, acc_v: 0.927222, spec_v: 0.966460, const time: 3.753670\n",
            "lr =  0.002\n",
            "epoch 6 /1001,epoch 5 /5, loss: 0.485185, loss_dice: 0.248519, sen_v: 0.608940, acc_v: 0.929753, spec_v: 0.966259, const time: 3.587254\n",
            "lr =  0.002\n",
            "epoch 7 /1001,epoch 5 /5, loss: 0.465457, loss_dice: 0.238348, sen_v: 0.657665, acc_v: 0.932782, spec_v: 0.967181, const time: 3.577070\n",
            "lr =  0.002\n",
            "epoch 8 /1001,epoch 5 /5, loss: 0.441168, loss_dice: 0.218447, sen_v: 0.681963, acc_v: 0.933763, spec_v: 0.966792, const time: 3.982472\n",
            "lr =  0.002\n",
            "epoch 9 /1001,epoch 5 /5, loss: 0.446055, loss_dice: 0.225820, sen_v: 0.676393, acc_v: 0.934656, spec_v: 0.968080, const time: 3.539919\n",
            "lr =  0.002\n",
            "epoch 10 /1001,epoch 5 /5, loss: 0.414892, loss_dice: 0.204527, sen_v: 0.707562, acc_v: 0.935992, spec_v: 0.964252, const time: 3.540059\n",
            "lr =  0.002\n",
            "epoch 11 /1001,epoch 5 /5, loss: 0.421728, loss_dice: 0.209532, sen_v: 0.687036, acc_v: 0.936986, spec_v: 0.970719, const time: 3.641605\n",
            "lr =  0.002\n",
            "epoch 12 /1001,epoch 5 /5, loss: 0.412484, loss_dice: 0.197988, sen_v: 0.725945, acc_v: 0.933464, spec_v: 0.963750, const time: 4.014156\n",
            "lr =  0.002\n",
            "epoch 13 /1001,epoch 5 /5, loss: 0.406571, loss_dice: 0.195524, sen_v: 0.710235, acc_v: 0.934931, spec_v: 0.963351, const time: 3.579869\n",
            "lr =  0.002\n",
            "epoch 14 /1001,epoch 5 /5, loss: 0.416807, loss_dice: 0.210102, sen_v: 0.658612, acc_v: 0.939734, spec_v: 0.974005, const time: 3.580219\n",
            "lr =  0.002\n",
            "epoch 15 /1001,epoch 5 /5, loss: 0.394436, loss_dice: 0.192661, sen_v: 0.735794, acc_v: 0.937123, spec_v: 0.961920, const time: 3.692027\n",
            "lr =  0.002\n",
            "epoch 16 /1001,epoch 5 /5, loss: 0.381084, loss_dice: 0.189794, sen_v: 0.717708, acc_v: 0.941736, spec_v: 0.969318, const time: 3.765566\n",
            "lr =  0.002\n",
            "epoch 17 /1001,epoch 5 /5, loss: 0.376294, loss_dice: 0.187679, sen_v: 0.712503, acc_v: 0.942448, spec_v: 0.970129, const time: 3.512701\n",
            "lr =  0.002\n",
            "epoch 18 /1001,epoch 5 /5, loss: 0.373475, loss_dice: 0.182991, sen_v: 0.715249, acc_v: 0.940841, spec_v: 0.970735, const time: 3.508607\n",
            "lr =  0.002\n",
            "epoch 19 /1001,epoch 5 /5, loss: 0.372930, loss_dice: 0.186144, sen_v: 0.716022, acc_v: 0.943471, spec_v: 0.969613, const time: 3.876757\n",
            "lr =  0.002\n",
            "epoch 20 /1001,epoch 5 /5, loss: 0.382295, loss_dice: 0.186881, sen_v: 0.725236, acc_v: 0.940163, spec_v: 0.968205, const time: 3.718740\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (2,192,192) into shape (192,192)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-eb99f5439547>\u001b[0m in \u001b[0;36m<cell line: 247>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIOStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mexp_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/run.log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-eb99f5439547>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(io)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     \u001b[0mscore_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mww\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                     \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_label\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mloss_dice_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0729cdb69a76>\u001b[0m in \u001b[0;36mmodel_forward\u001b[0;34m(model, test_data, patch_size, hh, ww, stride_y, stride_x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbsize\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mscore_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mcnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,192,192) into shape (192,192)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test# This is the result of the first run when I asked why the training was not done by changing \"main\" and the \"main\" part was run quickly and the test gave low results. #issue ۳#please help"
      ],
      "metadata": {
        "id": "oxYoprAMy4hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "#from data.load_train_data import TrainData\n",
        "#from data.load_test_data import TestData\n",
        "from torch.utils.data import DataLoader\n",
        "import sklearn.metrics as metrics\n",
        "#from net.retinal_vasuclar_net import RetinalVasularSeg\n",
        "#from net.unetrpp_ff import RetinalVasularSegFF\n",
        "#from net.UNetFamily import U_Net, AttU_Net, Dense_Unet\n",
        "#from net.loss import dice_loss, cal_sen\n",
        "#from metrics import Evaluate\n",
        "#import config.config as cfg\n",
        "\n",
        "#def model_initial(model, model_name):\n",
        "#    # 加载预训练模型\n",
        "#    pretrained_dict = torch.load(model_name)[\"model\"]\n",
        "#    model_dict = model.state_dict()\n",
        "#    # 1. filter out unnecessary keys\n",
        "#    # pretrained_dictf = {k.replace('module.', \"\"): v for k, v in pretrained_dict.items() if k.replace('module.', \"\") in model_dict}\n",
        "#    pretrained_dictf = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "#   # 2. overwrite entries in the existing state dict\n",
        "#    model_dict.update(pretrained_dictf)\n",
        "#    # 3. load the new state dict\n",
        "#    model.load_state_dict(model_dict)\n",
        "\n",
        "#    print(\"over\")\n",
        "\n",
        "\n",
        "\n",
        "def model_forward(model, test_data, patch_size, hh,ww, stride_y, stride_x):\n",
        "\n",
        "    sy = math.ceil((hh - patch_size[0]) / stride_y) + 1\n",
        "    sx = math.ceil((ww - patch_size[1]) / stride_x) + 1\n",
        "\n",
        "    score_map = np.zeros((1, hh, ww)).astype(np.float32)\n",
        "    cnt = np.zeros((1, hh, ww)).astype(np.int32)\n",
        "    test_datas = []\n",
        "    boxes = []\n",
        "    for y in range(0, sy):\n",
        "        ys = min(stride_y * y, hh - patch_size[0])\n",
        "        for x in range(0, sx):\n",
        "            xs = min(stride_x * x, ww - patch_size[1])\n",
        "\n",
        "            test_patch = test_data[:, :, ys:ys + patch_size[0], xs:xs + patch_size[1]]\n",
        "            test_datas.append(test_patch)\n",
        "            boxes.append([ys, ys + patch_size[0], xs, xs + patch_size[1]])\n",
        "\n",
        "    bsize = 8\n",
        "    batch_nums = math.ceil(len(test_datas)/bsize)\n",
        "    for i in range(batch_nums):\n",
        "        test_patch = torch.cat(test_datas[i*bsize:(i+1)*bsize], dim=0)\n",
        "        outputs_segb = model(test_patch)[0]\n",
        "        outputs_softb = F.sigmoid(outputs_segb)\n",
        "        # outputs_softb = torch.softmax(outputs_segb, dim=1)[:, 1, :, :]\n",
        "        predb = torch.squeeze(outputs_softb).detach().cpu().numpy()\n",
        "\n",
        "        for j in range(predb.shape[0]):\n",
        "            y1, y2, x1, x2 = boxes[i*bsize +j]\n",
        "            score_map[0, y1: y2, x1: x2] = score_map[0, y1: y2, x1: x2] + predb[j]\n",
        "\n",
        "            cnt[0, y1: y2, x1: x2] = cnt[0, y1: y2, x1: x2] + 1\n",
        "\n",
        "    # cv2.namedWindow(\"score_map\", cv2.WINDOW_NORMAL)\n",
        "    # cv2.imshow(\"score_map\", score_map[0, :, :])\n",
        "    #\n",
        "    # cv2.waitKey(0)\n",
        "    score_map = torch.tensor(score_map / cnt).cuda().float()\n",
        "\n",
        "    return  score_map\n",
        "\n",
        "\n",
        "def test():\n",
        "\n",
        "    file_path = \"/content/test/images/\"\n",
        "    label_path = \"/content/test/1st_manual/\"\n",
        "    #file_path = \"G:/vasular/DRIVE/test/images/\"\n",
        "    #label_path = \"G:/vasular/DRIVE/test/1st_manual/\"\n",
        "    # file_path = \"G:/vasular/CHASEDB1/test/\"\n",
        "    # label_path = \"G:/vasular/CHASEDB1/1st_label/\"\n",
        "    # file_path = \"G:/vasular/STAREdatabase/test/\"\n",
        "    # label_path = \"G:/vasular/STAREdatabase/labels-ah/\"\n",
        "    test_loader = DataLoader(TestData(file_path, label_path, train_flag = False), num_workers=0,\n",
        "                             batch_size=1, shuffle=True, drop_last=False)\n",
        "    device = torch.device(\"cuda\")\n",
        "    CROP_SIZE = 192\n",
        "    # Try to load models\n",
        "    model = RetinalVasularSegFF(in_channels=3,\n",
        "                            out_channels=1,\n",
        "                            img_size=[CROP_SIZE, CROP_SIZE],\n",
        "                            feature_size=16,\n",
        "                            num_heads=4,\n",
        "                            depths=[3, 3, 3, 3],\n",
        "                            dims=[32, 64, 128, 256],\n",
        "                              hidden_size=256,\n",
        "                            do_ds=True,\n",
        "                            )\n",
        "\n",
        "    # model = U_Net(img_ch=3, output_ch=2)\n",
        "    #model_name = \"./outputs/teethseg_model_1000.pth\"\n",
        "    #model_name = '/content/outputs/cls_1024/models'\n",
        "    #model_initial(model, model_name)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    evaluate = Evaluate()\n",
        "\n",
        "    tic = time.time()\n",
        "    nums = 0\n",
        "    CROP_SIZE = 192\n",
        "    sen_v, acc_v, spec_v =0, 0, 0\n",
        "    patch_size = [CROP_SIZE, CROP_SIZE]\n",
        "    stride_y, stride_x = 32, 32\n",
        "    score_maps, test_labels =[], []\n",
        "    for test_data, test_label in test_loader:\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_data = test_data.cuda().float()\n",
        "            test_label = test_label.cuda().float()\n",
        "\n",
        "            hh, ww = test_label.shape[-2:]\n",
        "            nums = nums +1\n",
        "\n",
        "            test_label[test_label > 0.5] = 1\n",
        "            score_map = model_forward(model, test_data, patch_size, hh, ww, stride_y, stride_x)\n",
        "            #cal Acc\n",
        "            sen_v_, acc_v_, spec_v_ = cal_sen(score_map, test_label.long())\n",
        "\n",
        "            # pred = torch.squeeze(outputs_soft[:, 1, :, :]).cpu().numpy() *255\n",
        "            # img = torch.squeeze(test_data).permute(1, 2, 0).cpu().numpy()*255\n",
        "\n",
        "            # cv2.imwrite(\"./outputs/\"+ str(nums) + \"_\"+str(acc_v_.item()) + \"p.png\", pred)\n",
        "            # cv2.imwrite(\"./outputs/\" + str(nums) + \"_\" + str(acc_v_.item()) + \"m.png\", img)\n",
        "            print(nums)\n",
        "            sen_v = sen_v + sen_v_.item()\n",
        "            acc_v = acc_v + acc_v_.item()\n",
        "            spec_v = spec_v + spec_v_.item()\n",
        "            score_maps.append(torch.squeeze(score_map).cpu().numpy())\n",
        "            test_labels.append(torch.squeeze(test_label.long()).cpu().numpy())\n",
        "    score_maps = np.array(score_maps)\n",
        "    test_labels = np.array(test_labels)\n",
        "    evaluate.add_batch(test_labels, score_maps)\n",
        "    result = evaluate.save_all_result()\n",
        "    print(result)\n",
        "    print(\"sen_v = \", sen_v/nums, \"  \", \"acc_v = \", acc_v/nums, \"  \", \"spec_v = \", spec_v/nums, \"  \")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "   test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpPDO0Z9y6Nd",
        "outputId": "e51643c1-d670-4267-a953-76abd34e7788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "OrderedDict([('AUC_ROC', 0.455738808872651), ('AUC_PR', 0.07543721945004993), ('f1-score', 0.16105150446312566), ('Acc', 0.08757803976239545), ('SE', 1.0), ('SP', 0.0), ('precision', 0.08757803976239545)])\n",
            "sen_v =  1.0    acc_v =  0.08757804036140442    spec_v =  0.0   \n"
          ]
        }
      ]
    }
  ]
}